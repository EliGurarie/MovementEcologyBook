[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Techniques and Concepts in Movement Ecology",
    "section": "",
    "text": "Course description\n\n\n\nMovement is a ubiquitous and fundamental property of all biological organisms. In this course, we will learn to model and analyze spatial patterns and animal movement data. Starting with a strong statistical foundation on time-series and spatial processes, we will cover topics such as point patterns, migration, dispersal, home-ranging, behavioral change point analysis, multi-state modeling, continuous-time movement models, and perhaps touch on cutting edge topics like cognitive movement ecology and collective movements. The overarching goal will be to match mathematical models, statistical techniques and computational tools to ecologically meaningful questions. Conceptual and theoretical lectures will be combined with practical work in R, including developing novel Bayesian MCMC tools in STAN. The course will culminate with student-led research projects involving some non-trivial analysis of spatial data.\nThe structure of the class will be somewhat improvised. But there will be a combination of lectures and labs, and perhaps some guest lecturers on special topics. There will be weekly homework assignments and a final project that involves a novel, non-trivial analysis to be worked on and presented at the end of the semester.\nAll materials will be on this course website and integrated into the document-style lecture materials.\n\n\nEadweard Muybridge demonstrated that bison move"
  },
  {
    "objectID": "whyspecialstats.html#spatial-data",
    "href": "whyspecialstats.html#spatial-data",
    "title": "\n2  Why do we need special statistics?\n",
    "section": "\n2.1 Spatial data",
    "text": "2.1 Spatial data\nSpatial data is simply data that contains spatial coordinates - in practice, we are almost always only concerned with two dimensions (\\({\\bf X, Y}\\)) with perhaps a set of additional observations \\(A\\). footnote: We use boldfacing to indicate vectors (e.g. of length \\(n\\)), and capitals to indicate matrices such that \\(A\\) is a \\(n \\times k\\) matrix for \\(k\\) observations. Spatial data may represent a complete set of observations, for example it can represent the locations of all trees of a certain size in a given study plot. Often in such cases there are questions that can be asked about the generating process which determines the distribution of those locations. When describing a process, the coordinates themselves and alone can be considered a “response”. For example, a typical question might be: are the trees clustered, inhibiting each other, or randomly distributed? This is referred to as analyzing a point process (see Chapter XX).\n\n\n\n\nCoordinates of American beech (Fagus grandifolia) in a research site at Adirondack Station, SUNY-ESF, New York State. The sizes reflect diameter at breast height in 2009. (Gienke et al. 2014, data: M. Dovciak)\n\n\n\n\nAlternatively, spatial data can be considered a sampling of a continuous spatial structure. For example, we might measure a set of soil properties (pH, saility, moisture, thickness) over a study area, and have some questions about why the soil is structured the way it is. In those cases, the spatial coordinates are not of intrinsic interest (they were established by the reserach), but more similar to a “nuisance” parameter. To make sure that any inference we make on those data is “correct”, i.e. not unduly influenced by that autocorrelation, we have to take that autocorrelation into account. On the other hand, the scale of spatial autocorrelation of a process can itself be a variable of interest. Proper ways to do this is very much a central topic in spatial ecology as well (Chapter XX)."
  },
  {
    "objectID": "whyspecialstats.html#movement-data",
    "href": "whyspecialstats.html#movement-data",
    "title": "\n2  Why do we need special statistics?\n",
    "section": "\n2.2 Movement data",
    "text": "2.2 Movement data\nMovement data - representing a measured sequence of locations for an individual organism - can be thought of as spatial data that is indexed in time, or - equivalently - as a two-dimensional time-series.\n\n one-dimensional: Multi-Dimensional! (X,Y,Time) > Solution: get comfortable with complex numbers.\n independent: Highly dependent! > Solution: get comfortable with  correlated / dependent data .\n identically distributed!: Complex and heterogeneous! > Solution: behavioral change point analysis / segmentation, hidden markov models.\n often normal: Circular and skewed distributions! > Solution: Flexible Likelihood-based modelling,\n randomly/uniformly sampled: Gappy / irregular / oddly duty-cycled data > Solution: Continuous movement modeling."
  },
  {
    "objectID": "complex_numbers.html#components-of-complex-number",
    "href": "complex_numbers.html#components-of-complex-number",
    "title": "\n3  Complex Numbers are Simple\n",
    "section": "\n3.1 Components of complex number",
    "text": "3.1 Components of complex number\nComplex numbers are a handy bookkeeping tool to package together two dimensions in a single quantity. They expand the one-dimensional (“Real”) number line into a second dimension (“Imaginary”).\nWe write a complex number \\(Z = X + iY\\)\n\n\n\\(X\\) is the real part.\n\n\\(Y\\) is the imaginary part.\n\nThe same number can be written in terms of distances and angles (which are a more “natural” language for spatial data):\n\\[ Z = R \\, \\exp(i \\theta) \\]\n\n\n\\(R\\) is the length of the vector from the origin, aka modulus.\n\n\\(\\theta\\) is the orientation of the vector, aka argument.\n\nThis angle, \\(\\theta\\) is defined in radians that turn counter-clockwise from the x-axis, \\(\\pi/2\\) is on the y-axis, \\(\\pi\\) is on the negative x-axis, etc. You can see that by simply putting in some numbers:\n\\[Z = 1 = \\exp{0}\\] \\[Z = 0 + 1i = \\exp{(i \\pi/2)}\\]\nAll of these have modulus 1.\nMore that the algebraic / trigonometric way to get distances (moduli) and angles (arguments) from a vector is a bit tedious. Thus, for \\(Z = X + iY\\):\n\\[R = \\sqrt{X^2 + Y^2}\\] Not terrible. But check out how to compute the angle from x and y:\n\\[\n\\theta(x,y) =\n\\begin{cases}\n\\arctan\\left(\\frac y x\\right) &\\text{if } x > 0, \\\\[5mu]\n\\arctan\\left(\\frac y x\\right) + \\pi &\\text{if } x < 0 \\text{ and } y \\ge 0, \\\\[5mu]\n\\arctan\\left(\\frac y x\\right) - \\pi &\\text{if } x < 0 \\text{ and } y < 0, \\\\[5mu]\n+\\frac{\\pi}{2} &\\text{if } x = 0 \\text{ and } y > 0, \\\\[5mu]\n-\\frac{\\pi}{2} &\\text{if } x = 0 \\text{ and } y < 0, \\\\[5mu]\n\\text{undefined} &\\text{if } x = 0 \\text{ and } y = 0.\n\\end{cases}\n\\]\nYikes!\nOn the other hand, these quantities are obtained instantly and easily in R.\n\n3.1.1 In R\n\nX <- c(3,4,-2)\nY <- c(0,3,2)\nZ <- X + 1i*Y\nZ\n\n[1]  3+0i  4+3i -2+2i\n\n\nAlternatively:\n\nZ <- complex(re = X, im=Y)\n\n\nplot(Z, pch=19, col=1:3, asp=1)\narrows(rep(0,length(Z)), rep(0,length(Z)), Re(Z), Im(Z), lwd=2, col=1:3)\n\n\n\n\nNote: ALWAYS use asp=1 - “aspect ratio = 1:1” - when plotting (properly projected) movement data!\nObtaining summary statistics is nearly instant. Obtain lengths of vectors:\n\nMod(Z)\n\n[1] 3.000000 5.000000 2.828427\n\n\nObtain orientation of vectors:\n\nArg(Z)\n\n[1] 0.0000000 0.6435011 2.3561945\n\n\nNote, the orientations are in radians, i.e. range from \\(0\\) to \\(2\\pi\\) going counter-clockwise from the \\(x\\)-axis. Compass directions go from 0 to 360 clockwise, so, to convert:\n\n90-(Arg(Z)*180)/pi\n\n[1]  90.0000  53.1301 -45.0000"
  },
  {
    "objectID": "complex_numbers.html#quickly-simulating-a-path",
    "href": "complex_numbers.html#quickly-simulating-a-path",
    "title": "\n3  Complex Numbers are Simple\n",
    "section": "\n3.2 Quickly simulating a path",
    "text": "3.2 Quickly simulating a path\nQuick code for a correlated random walk:\n\nX <- cumsum(arima.sim(n=100, model=list(ar=.7)))\nY <- cumsum(arima.sim(n=100, model=list(ar=.7)))\nZ <- X + 1i*Y\nplot(Z, type=\"o\", asp=1)\n\n\n\n\nInstant summary statistics of a trajectory:\nThe average location\n\nmean(Z)\n\n[1] 17.91147-41.70098i\n\n\nThe step vectors:\n\ndZ <- diff(Z)\nplot(dZ, asp=1, type=\"n\")\narrows(rep(0, length(dZ)), rep(0, length(dZ)), Re(dZ), Im(dZ), col=rgb(0,0,0,.5), lwd=2, length=0.1)\n\n\n\n\nDistribution of step lengths:\n\nS <- Mod(dZ)\nsummary(S)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.1983  0.9115  1.5089  1.5806  2.0551  4.0748 \n\nhist(S, col=\"grey\", bor=\"darkgrey\", freq=FALSE)\nlines(density(S), col=2, lwd=2)\n\n\n\n\nWhat about angles?\n\nThe absolute orientations:\n\n\nPhi <- Arg(dZ)\nhist(Phi, col=\"grey\", bor=\"darkgrey\", freq=FALSE, breaks=seq(-pi,pi,pi/3))\n\n\n\n\n\nTurning angles\n\n\nTheta <- diff(Phi)\nhist(Theta, col=\"grey\", bor=\"darkgrey\", freq=FALSE)\n\n\n\n\n QUESTION: What is a problem with this histogram?\nCircular statistics:\nAngles are a wrapped continuous variable, i.e. \\(180^o > 0^o = 360^o < 180^o\\). The best way to visualize the distribution of wrapped variables is with Rose-Diagrams. An R package that deals with circular data is circular.\n\nrequire(circular)\nTheta <- as.circular(Theta)\nPhi <- as.circular(Phi)\nrose.diag(Phi, bins=16, col=\"grey\", prop=2, main=expression(Phi))\nrose.diag(Theta, bins=16, col=\"grey\", prop=2, main=expression(Theta))\n\n\n\n\nLAB EXERCISE\n\n\nLoad movement data of choice!\nConvert the locations to a complex variable Z.\nObtain a vector of time stamps T, draw a histogram of the time intervals. Then, ignore those differences.\nObtain, summarize and illustrate:\n\n\nthe step lengths\nthe absolute orientation\nthe turning angles"
  },
  {
    "objectID": "complex_numbers.html#complex-manipulations-are-easy",
    "href": "complex_numbers.html#complex-manipulations-are-easy",
    "title": "\n3  Complex Numbers are Simple\n",
    "section": "\n3.3 Complex manipulations (are easy)",
    "text": "3.3 Complex manipulations (are easy)\n\n3.3.1 Addition & substration of vectors\nAddition and subtraction of vectors:\n\\[ Z_1 = X_1 + i Y_1; Z_2 = X_2 + i Y_2\\] \\[ Z_1 + Z_2 = (X_1 + X_2) + i(Y_1 + Y_2)\\]\nUseful, e.g., for shifting locations:\n\nplot(Z, asp=1, type=\"l\", col=\"darkgrey\", xlim=c(-2,2)*max(Mod(Z)))\nlines(Z - mean(Z), col=2, lwd=2)\nlines(Z + Z[length(Z)], col=3, lwd=2)\n\n\n\n\n\n3.3.2 Rotation of vectors\nMultiplication of complex vectors \\[Z_1 = R_1 \\exp(i \\theta_1); Z_2 = R_2 \\exp(i \\theta_2)\\] \\[ Z_1 Z_2 = R_1 R_2 \\exp(i (\\theta_1 + \\theta_2))\\] Note the magic of the rotation summing! If \\(\\text{Mod}(Z_2) = 1\\), multiplications rotates by \\(\\text{Arg}(Z_2)\\)\n\ntheta1 <- complex(mod=1, arg=pi/4)\ntheta2 <- complex(mod=1, arg=-pi/4)\n\nplot(Z, asp=1, type=\"l\", col=\"darkgrey\", lwd=3)\nlines(Z*theta1, col=2, lwd=2)\nlines(Z*theta2, col=3, lwd=2)\n\n\n\n\nA colorful loop:\n\nrequire(gplots)\nplot(Z, asp=1, type=\"n\", xlim=c(-1,1)*max(Mod(Z)), ylim=c(-1,1)*max(Mod(Z)))\ncols <- rich.colors(1000,alpha=0.1)\nthetas <- seq(0,2*pi,length=100)\nfor(i in 1:1000)\n  lines(Z*complex(mod=1, arg=thetas[i]), col=cols[i], lwd=4)\n\n\n\n\nI know you’re probably thinking …\n\n“Thanks for teaching me how to make a weird swirling rainbow thing … but why in the world would I want to shift and rotate my precious, precious data, which was just perfect the way it was?\n\nMy response:  Null Sets for Pseudo Absences!"
  },
  {
    "objectID": "complex_numbers.html#example-with-finnish-wolves",
    "href": "complex_numbers.html#example-with-finnish-wolves",
    "title": "\n3  Complex Numbers are Simple\n",
    "section": "\n3.4 Example with Finnish Wolves",
    "text": "3.4 Example with Finnish Wolves\n\nIn-depth summer predation study, questions related to habitat use, landscape type - forest/bog/field - linear elements - roads/rivers/power lines, etc.\n\n\n3.4.1 Defining Null Sets\n\nObtain all the steps and turning angles\nRotate them by the orientation of the last step (\\(Arg(Z_1-Z_0)\\))\nAdd the rotated steps to the last step (\\(Z_1\\))\n\n\n\n\nObtain all the steps and turning angles\nRotate them by the orientation of the last step (\\(Arg(Z_1-Z_0)\\))\n\n\nn <- length(Z)\nS <- Mod(diff(Z))\nPhi <- Arg(diff(Z))\nTheta <- diff(Phi)\nRelSteps <- complex(mod = S[-1], arg=Theta)\n\nZ0 <- Z[-((n-1):n)]\nZ1 <- Z[-c(1,n)]\nZ2 <- Z[-(1:2)]\n\nRotate <- complex(mod = 1, arg=Arg(Z1-Z0))\n\n\nplot(c(0, RelSteps), asp=1, xlab=\"x\", ylab=\"y\", pch=19)\narrows(rep(0,n-2), rep(0, n-2), Re(RelSteps), Im(RelSteps), col=\"darkgrey\")\n\n\n\n\n\nNote: in practice (i.e. with tons of data), it is sufficient to randomly sample some smaller number (e.g. 30) null steps at each location.\n\n\nAdd the rotated steps to the last step\n\n\nZ.null <- matrix(0, ncol=n-2, nrow=n-2)\nfor(i in 1:length(Z1))\n  Z.null[i,] <- Z1[i] + RelSteps * Rotate[i]\n\n\nMake the fuzzy catterpillar plot\n\n\nplot(Z, type=\"o\", col=1:length(Z), pch=19, asp=1)\nfor(i in 1:nrow(Z.null))\n  segments(rep(Re(Z1[i]), n-2), rep(Im(Z1[i]), n-2), \n           Re(Z.null[i,]), Im(Z.null[i,]), col=i+1)\n\n\n\n\n\n3.4.2 Null set\nThe use of the null set is a way to test a narrower null hypothesis that accounts for auto correlation in the data.\nThe places the animal COULD HAVE but DID NOT go to are pseudo-absences, against which you can fit, e.g., logistic regression models (aka Step-selection functions).\nOr just be simple/lazy (like us) and compare observed locations with Chi-squared tests:\n\n EXERCISE: Create a fuzzy-catterpillar plot!\nUse (a portion) of the data you analyzed before.\n\n# get pieces\nn <- length(Z)\nSteps <- diff(Z)\nS <- Mod(Steps)\nPhi <- Arg(Steps)\nTheta <- diff(Phi)\nRelSteps <- complex(mod = S[-1], arg = Theta)\n\n# calculate null set\nZ0 <- Z[1:(n-2)]\nZ1 <- Z[2:(n-1)]\nZ2 <- Z[3:n]\nRotate <- complex(mod = 1, arg = Arg(Z1-Z0))\n\nZ.null <- matrix(NA, ncol=n-2, nrow=n-2)\nfor(i in 1:length(Z1))\n  Z.null[i,] <- Z1[i] + sample(RelSteps) * Rotate[i]\n\n# plot\nplot(Z, type=\"o\", col=1:10, pch=19, asp=1)\nfor(i in 1:nrow(Z.null))\n  segments(rep(Re(Z1[i]), n-2), rep(Im(Z1[i]), n-2), \n           Re(Z.null[i,]), Im(Z.null[i,]), col=i+1)\n\n\n Fuzzy Polar Bear Catterpillar!"
  },
  {
    "objectID": "likelihoods.html#likelihoods-are",
    "href": "likelihoods.html#likelihoods-are",
    "title": "\n6  Likelihood Theory\n",
    "section": "\n6.1 Likelihoods are:",
    "text": "6.1 Likelihoods are:\n\nthe link between: Data -> Probability Models -> Inference\nthe most important tool, in practice, for fitting models … i.e. when we estimate parameters we like to Maximize the Likelihood,\nNote: there are some theoretical reasons for this - MLE’s are proven to be CONSISTENT and have the LOWEST VARIANCE of any other estimator*\nprovide a flexible / general framework for comparing / selecting models, via Likelihood Ratio Tests, Information Criteria (AIC, BIC, etc.)\n\nAnd, very strangely:\n\nThey are almost NEVER TAUGHT in undergraduate statistics classes!\nThough they are EASY TO UNDERSTAND.\n\n\nWe are basically peeking under the hood of a lot of statistical machinery.\n\nIn short:\n\nLikelihoods turn Probabilities on their heads!\n\n6.1.1 Probability statements\nLet’s say the average height of a human male is \\[X \\sim {\\cal N}(\\mu_0 = 6, \\sigma_0 = 0.5)\\] This is a probability model, i.e. it tells us that: \\[P(x < X < x+dx) = f(x|\\mu, \\sigma)dx\\] where \\(f(x|\\mu, \\sigma)\\), is the density function of the normal distribution with \\(\\mu\\) and \\(\\sigma\\).\nThis probability statement means: If you take (for example) a single 7 foot tall person, you can say with certainty that:\n\nthe probability that a person is exactly 7 feet tall is exactly 0\nbut the per-foot “probability” that he’s around 7 feet tall is\n\n\\[f(7|\\mu=6, \\sigma=0.5) = 0.11  1/foot\\]\n\ndnorm(7,6,.5)\n\n[1] 0.1079819\n\n\nBut the per-foot “probability” that he’s around 7 feet tall is\n\\[f(7|\\mu=6, \\sigma=0.5) = 0.11  1/foot\\]\n\ndnorm(7,6,.5)\n\n[1] 0.1079819\n\n\nWhats a “1/foot”? Convert it to “per inch”:\n\ndnorm(7,6,.5)/12\n\n[1] 0.008998494\n\n\nJust under 1% probability that a person is within one inch of 7 feet.\n\n6.1.2 Defining a Likelihood\nSay, you don’t know what those parameters are, but you are curious! So you collect one data point: a single man, and he is 7 feet tall.\n\\[ X_1 = \\{7\\}\\]\nTo “flip this on its head”“, we ask a new question: What is the likelihood that the mean and standard deviation of human heights are 6 and 0.5, GIVEN that we observed a man who is \\(X_1 = 7\\) feet tall?\nWe write this as: \\[ {\\cal L}(\\mu, \\sigma | x).\\]\nIt is “flipped on its head” because it as a function of the parameters given an observation, rather than as the probability of an observation given some parameters. But, by definition, the likelihood is numerically equal to the probability:\n\\[L_0 = {\\cal L}(\\mu_0, \\sigma_0 | X_1) = f(X_1|\\mu_0, \\sigma_0) = 0.11\\]z\nIn colloquial language, a “likelihood” is sort of similar to a “probability” - i.e. the statement: “Event A is likely.” seems to have similar meaning to the statement: “Event A has high probability.”\nIn statistics, the “likelihood” and the “probability” are, in fact EQUAL - but there is an inversion of what is considered “known” and “unknown: - A probability tells you something about a random event given parameter values.\n- The likelihood tells you something about parameter values given observations.\n In practice - statistical inference is about having the data and guessing the parameters. Thus the concept of Likelihoods is extremely useful and natural. \nIn contrast to Probabilities, the actual raw value of the likelihood is almost NEVER of interest - We ONLY care about its value when compared with likelihoods of different models.\nfor example, we can compare the likelihood of the parameters \\(\\mu=6\\) and \\(\\sigma=0.5\\), GIVEN the observation \\(X_1 = 7\\), with an alternative probability model … say, \\(\\mu_1 = 7\\) and \\(\\sigma_1 = 0.001\\). That likelihood is given by:\n\\[ L_1 = {\\cal L}(\\mu_1, \\sigma_1 | X_1 = 7) = f(7 | 7, .001)\\]\n\n(L1 <- dnorm(7,7,.001))\n\n[1] 398.9423\n\n\n\\(L_1\\) is, clearly, much much greater than our original likelihood, i.e. this set of parameters is much likelier than the original set of parameters. Indeed, the ratio \\(L_1 / L_0 = 3693\\) ….\n\nso we can say that model 2 is more than 3000x more likely than Model 1!\n\n(Of course, the likelihood ratio test for these two likelihoods has zero power, because we only have one data point. But we can still quantify their relative likelihoods.)\n\n6.1.3 Joint Likelihoods\n\nLet’s sample 5 individuals from a professional basketball arena, and record their heights: 6.6, 6.8, 7.0, 7.2, 7.4 feet.\nAt this point, I might have reasonable suspicion that our null-model might not be appropriate for this subset of humans.\nVisualize these data-points against our null-model:\n\nmu0 <- 6\nsigma0 <- 0.5\nX <- c(6.6,6.8,7.0,7.2,7.4)\ncurve(dnorm(x, mu0, sigma0), xlim=c(4,8))\npoints(X, dnorm(X,mu0,sigma0), pch=19, col=2)\npoints(X, dnorm(X,mu0,sigma0), type=\"h\", col=2)\n\n\n\n\nNow, we can compute the likelihood of the null parameters given all of these observations.The likelihood (a joint likelihood) is just the product of the likelihood for each of these points, because it is equal to the joint density distribution … this is because the Probability of \\(n\\) independent events is the product of the probabilities:\n\\[{\\cal L}(\\mu_0, \\sigma_0 | {\\bf X}) = \\prod_{i=1}^n f(X_i|\\mu_0, \\sigma_0) \\]\n\n(L0 <- prod(dnorm(X, mu0, sigma0)))\n\n[1] 6.596596e-06\n\n\nThis is a very small number, but again - it is meaningless without having a comparison. Let’s compute the joint likelihood of our alternative model:\n\nmu1 <- 7\nsigma1 <- 0.001\n(L1 <- prod(dnorm(X, mu1, sigma1)))\n\n[1] 0\n\n\nTo machine power, the second model is MUCH LESS likely than the first model! To illustrate this:\n\ncurve(dnorm(x, mu1, sigma1), xlim=c(6.2,7.8), n=1000)\npoints(X, dnorm(X,mu1,sigma1), pch=19, col=2)\npoints(X, dnorm(X,mu1,sigma1), type=\"h\", col=2)\n\n\n\n\nYou see that the points away from 7 have extremely low probability, which brings the likelihood of this model way down."
  },
  {
    "objectID": "likelihoods.html#the-maximum-likelihood-estimator",
    "href": "likelihoods.html#the-maximum-likelihood-estimator",
    "title": "\n6  Likelihood Theory\n",
    "section": "\n6.2 The Maximum Likelihood Estimator\n",
    "text": "6.2 The Maximum Likelihood Estimator\n\nSo - how do we find the parameters that maximize the likelihood?\nThese parameters are called the Maximum Likelihood Estimators (MLE’s), and are the “best” parameters in that they are the most precise. Remember, every estimate is a random variable and therefore comes with some variance. It can be shown that MLE’s have the smallest of those possible variances:\n\\[  \\{ \\widehat\\theta_\\mathrm{mle}\\} \\subseteq \\{ \\underset{\\theta\\in\\Theta}{\\operatorname{arg\\,max}}\\ {\\cal L}(\\theta\\,|\\,X_1,\\ldots,X_n) \\}\\]\nTranslating to English:\n the MLE estimators of parameters \\(\\theta\\) are those values of \\(\\theta\\) for which the likelihood function is maximized."
  },
  {
    "objectID": "likelihoods.html#the-likelihood-profile",
    "href": "likelihoods.html#the-likelihood-profile",
    "title": "\n6  Likelihood Theory\n",
    "section": "\n6.3 The Likelihood profile",
    "text": "6.3 The Likelihood profile\nLet’s look at a range of possible values for \\(\\widehat{\\mu}\\) and \\(\\widehat{\\sigma}\\). We can do this pretty efficiently in R:\nFirst, pick some values to explore:\n\nmus <- seq(6,8,.05)\nsigmas <- seq(.1,1,.02)\n\nNow, write a function that computes the likelihood (which, as we recall, is a function of \\(\\mu\\) and \\(\\sigma\\), and “assumes” \\(\\bf X\\))\n\nLikelihood <- function(mu, sigma)\n  prod(dnorm(X, mu, sigma))\n\nAnd compute this likelihood for all the combinations of \\(\\mu\\) and \\(\\sigma\\) above, using the mighty outer() function:\n\nL.matrix <- outer(mus, sigmas, Vectorize(Likelihood))\n\nNote (as a technical aside) that to get the Likelihood() function to work within outer(), it had to be “vectorized”. Happily, there is a function (Vectorize()) that does just that.\nWe can visualize our likelihood profile over those ranges of \\(\\mu\\) and \\(\\sigma\\)\n\n\n\n\n\nClearly, there is a sharp peak in the likelihood around \\(\\widehat{\\mu} = 7\\) and somewhere just under \\(\\widehat{\\sigma} = 0.3\\). From our limited sets of values, we can find the values at the maximum:\n\n(max.indices <- which(L.matrix == max(L.matrix), arr.ind = TRUE))\n\n     row col\n[1,]  21  10\n\n(mu.hat <- mus[max.indices[1]])\n\n[1] 7\n\n(sigma.hat <- sigmas[max.indices[2]])\n\n[1] 0.28\n\n\nAnd the (usually irrelevant) value of the likelihood is\n\nmax(L.matrix)\n\n[1] 0.4580006\n\n\n\n6.3.1 Numerically finding the MLE\nWe don’t need to do this search ourselves - that’s why we invented electronic computing devices. The powerhouse function for optimizing functions in R is optim(), but it takes some getting used to. The (minimal) syntax is:\n\noptim(p, FUN, ...)\n\nwhere:\n\n\np is a required vector of your initial guess for the parameters.\n\nFUN(p, ...) is a function that takes as its first argument a vector of parameters p\n\nThe ... refers to other arguments passed to FUN. Most importantly, this will be data!\n\n\nSee how I make it work below:\n\noptim(c(6,1), function(p) -Likelihood(p[1], p[2]))\n\n$par\n[1] 7.0000003 0.2828426\n\n$value\n[1] -0.4582359\n\n$counts\nfunction gradient \n      89       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\nSo - lots of output, but the key numbers we’re interested in are the top two, under $par. These are: \\(\\widehat{\\mu} = 7.000\\) and \\(\\widehat{\\sigma} = 0.2828\\). Good! it is what we expected.\nThe $value is very close to the (negative of the) maximum likelihood that we “hand-calculated”.\n\n6.3.2 Comparing models side by side\n\nPlotLikelihood <- function(mu, sigma, X, ...)\n{curve(dnorm(x, mu, sigma), ...)\npoints(X, dnorm(X,mu,sigma), pch=19, col=2)\npoints(X, dnorm(X,mu,sigma), type=\"h\", col=2)}\n\npar(mfrow=c(1,3), bty=\"l\")\n\nPlotLikelihood(mu = 6, sigma=0.5, X, xlim=c(4,8))\ntitle(\"Null Model\")\n\nPlotLikelihood(mu = 7, sigma=0.001, X, xlim=c(6.2,7.8), n = 1000)\ntitle(\"Model 1\")\n\npars <- optim(c(6,1), function(p) -Likelihood(p[1], p[2]))$par\nPlotLikelihood(mu = pars[1], sigma=pars[2], X, xlim=c(5,9))\ntitle(\"MLE Model\")\n\n\n\n\n\n\n\nYou can get a feel for the constraints on the MLE model: if \\(\\sigma\\) were smaller, the contribution of the outlying points would become much smaller and bring the likelihood down; if \\(\\sigma\\) were any larger, then the flattening would bring down too far the contribution of the central points.\n\n6.3.3 Comparing with Method of Moment Estimators (MME’s)\nNow … you might think that’s a whole lot of crazy to estimate a simple MEAN and VARIANCE.\nYou may have guessed that the best estimates are the sample mean:\n\\[\\widehat{\\mu} = \\overline{X}\\] and the sample standard deviation:\n\\[\\widehat{\\sigma^2} = s_x^2 = {1\\over n-1}\\sum (X_i - \\overline{X})^2\\]\nThe mean estimate matches, but the sample standard deviation doesn’t quite:\n\nsd(X)\n\n[1] 0.3162278\n\n\nThis suggests that the MLE of the variance - for all of its great qualities - is at least somewhat biased (remember: \\(E(s_x^2) = \\sigma^2\\)).\n Can you recognize what analytical formula gives the MLE of \\(\\sigma\\)? \n\nNote: this is a special case where the MLE is actually computable by hand, but in general it is not - and best to obtain numerically, as the examples that follow show.\n\n\n6.3.4 Log-likelihoods\nFor a combination of practical and theoretical reasons, what we actually maximize is not the Likelihood but the log-Likelihood. The maximum will be in the same place for both functions, but the latter is MUCH easier to work with.\nLikelihood: \\[{\\cal L}(\\mu_0, \\sigma_0 | {\\bf X}) = \\prod_{i=1}^n f(X_i|\\mu_0, \\sigma_0) \\]\nLog-likelihood: \\[{\\cal l}(\\mu_0, \\sigma_0 | {\\bf X}) = \\log({\\cal L}) = \\sum_{i=1}^n \\log(f(X_i|\\mu_0, \\sigma_0)) \\]\nSums are much easier to manipulate with algebra and handle computaitonally. Also, they help turn very very very small numbers and very very very large numbers to very very very ordinary numbers.\n\nThe number of particles in the universe is about 10^80 … its log is 184. The ratio of the mass of an electron to the size of the sun is 10^-60 … its log is -134.\n\n\nL.matrix <- outer(mus, sigmas, Vectorize(Likelihood))\nimage(mus, sigmas, L.matrix, col=topo.colors(1000))\ncontour(mus, sigmas, L.matrix, add=TRUE, col=\"white\")\n\n\n\n\n\nll.matrix <- outer(mus, sigmas, Vectorize(logLikelihood))\nimage(mus, sigmas, ll.matrix, col=topo.colors(1000))\ncontour(mus, sigmas, ll.matrix, add=TRUE, nlevels=100, col=rgb(0,0,0,.2))\n\n\n\n\n\n6.3.5 Confidence Intervals: A bit of theory\nThe peak of the (log)-likelihood surface gives you point estimates of parameters.\nLikelihood theory provides an additional enormously handy (asymptotically correct) result with respect to standard errors around the estimates. Specifically (in words): The variance around the point estimates is equal to the negative reciprocal of the second derivative of the log-likelihood at the maximum.\nThis is actually very intuitive! The sharper the peak, the MORE NEGATIVE the second derivative, the SMALLER the (POSITIVE) variance. The flatter the peak, the LESS NEGATIVE the second derivative, i.e. the LARGER the variance.\n\\[\\Sigma(\\theta) = {\\cal I}(\\theta)^{-1}\\]\nOften (e.g. in these examples) we have several parameters here, so the jargon is fancier, but the idea is the same:\n\nthe Hessian is an n-dimensional second derivative (a matrix)\nthe Fisher Information (\\(\\cal{I}\\)) is the Hessian of the log likelihood.\nthe inverse is the n-dimensional equivalent of “reciprocal”.\n\\(\\Sigma\\) is the variance-covariance matrix of the parameter estimates.\n\n6.3.6 Confidence Intervals: Application\nAsk optim() to compute the Hessian:\n\nlogLikelihood <- function(mu, sigma)\n  sum(log(dnorm(X, mu, sigma)))\n(param.fit <- optim(c(1,1),  function(p) -logLikelihood(p[1], p[2]), hessian=TRUE))\n\n$par\n[1] 6.9999642 0.2828735\n\n$value\n[1] 0.7803712\n\n$counts\nfunction gradient \n      97       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n            [,1]         [,2]\n[1,] 62.48637509   0.01580303\n[2,]  0.01580303 124.94594137\n\n\nThe inverse of a matrix (in R) is solve(M):\n\n(Sigma <- solve(param.fit$hessian))\n\n              [,1]          [,2]\n[1,]  1.600349e-02 -2.024104e-06\n[2,] -2.024104e-06  8.003462e-03\n\n\nThe square root of the diagonal gives standard errors\n\n(se <- sqrt(diag(Sigma)))\n\n[1] 0.12650490 0.08946207\n\n\nAnd the confidence intervals are just:\n\ncbind(hat = param.fit$par, CI.low = param.fit$par  - 1.96*se, CI.high = param.fit$par  + 1.96*se)\n\n           hat    CI.low   CI.high\n[1,] 6.9999642 6.7520146 7.2479138\n[2,] 0.2828735 0.1075279 0.4582192\n\n\n\noptim: Is the engine under a great many hoods of R functions!\n\n\nJohn Nash: creator (and skeptic) of optim\nsee: http://www.ibm.com/developerworks/library/ba-optimR-john-nash/"
  },
  {
    "objectID": "likelihoods.html#testing-hypotheses-and-model-selection-with-likelihoods",
    "href": "likelihoods.html#testing-hypotheses-and-model-selection-with-likelihoods",
    "title": "\n6  Likelihood Theory\n",
    "section": "\n6.4 Testing hypotheses and model selection with likelihoods",
    "text": "6.4 Testing hypotheses and model selection with likelihoods\n\n6.4.1 Likelihood Ratio Test:\n\nModel 0 and Model 1 are NESTED\n(i.e. Model 0 is a special case of Model 1) with \\(k_0\\) and \\(k_1\\) parameters.\nCompute MLE’s: \\(\\widehat{\\theta_0}\\) and \\(\\widehat{\\theta_1}\\)\nCompute likelihoods: \\({\\cal L_0(\\theta_0|X)}\\) and \\({\\cal L_1(\\theta_1|X)}\\)\nimportant: the data \\(X\\) must be identical!\nLikelihood Ratio Test Statistic: \\[\\Lambda = -2 \\log \\left( \\frac{L_0}{L_1}  \\right) = 2 (l_1 - l_0)\\]\nunder Null hypothesis (i.e. Model 1) has distribution \\(\\Lambda \\sim \\text{Chi-squared} (d.f. = k_1 - k_0)\\)\n\nAn example:\n\n\n\n\n\nCompeting models: \\[M0: Y_i = \\beta_0 + \\epsilon_i\\] \\[M1: Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\] where \\(\\epsilon\\) are iid Gaussian.\nDefining (negative) log-likelihood functions\n\nLL0 <- function(par = c(beta0, sigma), X, Y){\n  -sum(dnorm(Y, mean = par['beta0'], sd = par['sigma'], log=TRUE) )\n}\nLL1 <- function(par = c(beta0, beta1, sigma), X, Y){\n  Y.hat <- par['beta0'] + par['beta1']*X\n  -sum(dnorm(Y, mean = Y.hat, sd = par['sigma'], log=TRUE) )\n}\n\nObtaining estimates\n\nfit0 <- optim(c(beta0=0, sigma=1), LL0, X = X, Y = Y)\nfit1 <- optim(c(beta0 = 0, beta1 = 0, sigma = 1), LL1, X = X, Y = Y)\n\nPerforming test:\n\nLRT <- 2*(-fit1$value + fit0$value)\n1-pchisq(LRT, 1)\n\n[1] 0.1083316\n\n\n\n6.4.2 Information Criteria\nIf models are not nested (most interesting sets of models aren’t) we can’t use a likelihood ratio test. Instead, we use the very very widely applied Akaike Information Criterion (AIC):\\[ AIC =  - 2 \\log(\\cal L) + 2 k\\]\n\nc(AIC0  = 2*(fit0$value + 2*2),\n  AIC1  = 2*(fit1$value + 2*3))\n\n    AIC0     AIC1 \n79.56228 80.98389 \n\n\nOr … Bayesian Information Criterion (BIC) \\[ BIC =  - 2 \\log(\\cal L) + k \\log(n) \\]\n\nc(BIC0  = 2*(fit0$value + 2 * log(length(X))),\n  BIC1  = 2*(fit1$value + 3 * log(length(X))))\n\n    BIC0     BIC1 \n83.54521 86.95828 \n\n\nWhich? Why? - a complicated debate, different underlying assumptions But generally - if you want to be more parsimonious (i.e. protect from overfitting) BIC is a better bet.\n\n6.4.3 In summary\nIf you have data and you can write a probability model in terms of parameters, no matter how strange or arbitrary seeming, there’s a good chance you can:\n\n\nestimate those parameters,\n\n\ncompare competing models,\nand probably obtain confidence intervals."
  },
  {
    "objectID": "likelihoods.html#panda-pregnancy-test-example",
    "href": "likelihoods.html#panda-pregnancy-test-example",
    "title": "\n6  Likelihood Theory\n",
    "section": "\n6.5 Panda pregnancy test example",
    "text": "6.5 Panda pregnancy test example\n\n\nLikelihood ratio test = pregnancy test\n\n\\(p\\)-value of birth: 0.08\n\n\\(p\\)-value of birth: 1e-11"
  },
  {
    "objectID": "likelihoods.html#applying-likelihoods-to-movement-data",
    "href": "likelihoods.html#applying-likelihoods-to-movement-data",
    "title": "\n6  Likelihood Theory\n",
    "section": "\n6.6 Applying Likelihoods to movement data",
    "text": "6.6 Applying Likelihoods to movement data\nWe have seen that movement data often has:\n\nSkewed, Positive, step Length Distribution\nWrapped turning angle distributions, clustered around 0^o\n\n\n\n\nThis combination is known as the Correlated Random Walk (CRW) model, probably the most commonly used basic movement model in ecology.\n\n\n\n\n6.6.1 Correlated Random Walk\nTypical step-length model - the Weibull distribution: \\[f(x;\\alpha, \\beta) = \\frac{\\alpha}{\\beta}\\left(\\frac{x}{\\beta}\\right)^{\\alpha-1}e^{-(x/\\beta)^{k}}\\] \\(\\alpha\\) and \\(\\beta\\) are the shape and scale parameter, respectively.\n\ncurve(dweibull(x, 1, 2), xlim=c(0,6), ylim=c(0,1.2), ylab=\"density\", xlab=\"\", col=2)\ncurve(dweibull(x, 2, 2), add=TRUE, col=3)\ncurve(dweibull(x, 6, 2), add=TRUE, col=4)\nlegend(\"topright\", col=2:4, lwd=2, legend=c(1,2,6), title=\"Shape parameter\", bty=\"n\", cex=1.5)\n\n\n\n\nTypical turning angle model - wrapped Cauchy distribution: \\[f(\\theta;\\mu,\\kappa)=\\frac{1}{2\\pi}\\,\\,\\frac{\\sinh\\kappa}{\\cosh\\kappa-\\cos(\\theta-\\mu)}\\] \\(\\mu\\) is the mean angle (usually 0) and \\(\\kappa\\) is clustering parameter- equal to \\(E(cos(\\theta))\\).\n\nrequire(CircStats) # NOT in base!\ncurve(dwrpcauchy(x, 0, 0), xlim=c(-pi,pi), ylim=c(0,1.5), lwd=2, ylab=\"density\", xlab=\"\", col=2)\ncurve(dwrpcauchy(x, 0, 0.5),  add=TRUE, col=3)\ncurve(dwrpcauchy(x, 0, 0.8),add=TRUE, col=4, n=1001)\nlegend(\"topright\", col=2:4, legend=c(0,0.5,0.8), title=\"Shape parameter\", bty=\"n\", cex=1.5, lwd=2)\n\n\n\n\n\n6.6.2 MLE of Weibull parameters\nHere’s some sample data:\n\nY <- cumsum(arima.sim(n=100, model=list(ar=.7)))\nZ <- X + 1i*Y\nplot(Z, type=\"o\", asp=1)\n\n\n\n\nA function that returns the likelihood as a function of the parameters and data\n\nWeibull.Like <- function(p, Z)\n{ \n  S = Mod(diff(Z))\n  -sum(dweibull(S, p[1], p[2], log=TRUE))\n}\n\n\nNote 1: we use log() because it is much easier to SUM LOGS than it is to MULTIPLY SMALL NUMBERS.\n\n\nNote 2: we return the NEGATIVE of the likelihood because the optim() function likes to MINIMIZE rather than MAXIMIZE.\n\nRun the optimization:\n\n(Weibull.fit <- optim(c(1,1), Weibull.Like, Z=Z))\n\n$par\n[1] 2.020257 1.926387\n\n$value\n[1] 124.6168\n\n$counts\nfunction gradient \n      63       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\nVisually assess the fit:\n\nhist(Mod(diff(Z)), freq=FALSE, col=\"grey\", breaks=10)\ncurve(dweibull(x, Weibull.fit$par[1], Weibull.fit$par[2]), add=TRUE, col=2, lwd=2)\n\n\n\n\nNot Bad!"
  },
  {
    "objectID": "likelihoods.html#exercise-1",
    "href": "likelihoods.html#exercise-1",
    "title": "\n6  Likelihood Theory\n",
    "section": "\n6.7 Exercise 1",
    "text": "6.7 Exercise 1\nUse optim() to estimate the \\(\\kappa\\) parameter in the Wrapped cauchy distribution Visualize the quality of the fits by comparing distribution curves to histograms.\nFollow the template for the Weibull:\n\nWeibull.Like <- function(p, Z)\n{ \n  S = Mod(diff(Z))\n  -sum(dweibull(S, p[1], p[2], log=TRUE))\n}\n(Weibull.fit <- optim(c(1,1), Weibull.Like, Z=Z))\n\n\n\n\n6.7.1 Exercise 2\nFollowing the template below:\n\nEstimate the Weibull step-length and wrapped Cauchy turning angle parameters for your movement data of choice.\nVisualize the quality of the fits by comparing distribution curves to histograms.\nExtra Credit: Compute standard errors around those estimates.\n\n For discussion: What do YOU think about the CRW model?  \n\nWeibull.fit <- optim(c(1,1), Weibull.Like, Z=Z, hessian=TRUE)\nSigma <- solve(Weibull.fit$hessian)\nse <- sqrt(diag(Sigma))\ncbind(hat = Weibull.fit$par, CI.low = Weibull.fit$par  - 1.96*se, CI.high = Weibull.fit$par  + 1.96*se)"
  },
  {
    "objectID": "dependent_data.html",
    "href": "dependent_data.html",
    "title": "\n7  Dependent Data\n",
    "section": "",
    "text": "time series, autocorrelated data\n\nNB talks about time warps\n\n\nspatial correlation\ngls - nlme"
  },
  {
    "objectID": "discrete_movement_models.html#simple-random-walk",
    "href": "discrete_movement_models.html#simple-random-walk",
    "title": "\n8  Discrete time movement models\n",
    "section": "\n8.1 Simple random walk",
    "text": "8.1 Simple random walk"
  },
  {
    "objectID": "discrete_movement_models.html#correlated-random-walk",
    "href": "discrete_movement_models.html#correlated-random-walk",
    "title": "\n8  Discrete time movement models\n",
    "section": "\n8.2 Correlated random walk",
    "text": "8.2 Correlated random walk\nStep-length distributions\nTurning angle distributions"
  },
  {
    "objectID": "discrete_movement_models.html#biased-crw",
    "href": "discrete_movement_models.html#biased-crw",
    "title": "\n8  Discrete time movement models\n",
    "section": "\n8.3 Biased CRW",
    "text": "8.3 Biased CRW"
  },
  {
    "objectID": "home_ranges.html#mcp",
    "href": "home_ranges.html#mcp",
    "title": "\n10  Home ranges\n",
    "section": "\n10.1 MCP",
    "text": "10.1 MCP"
  },
  {
    "objectID": "home_ranges.html#kernel",
    "href": "home_ranges.html#kernel",
    "title": "\n10  Home ranges\n",
    "section": "\n10.2 kernel",
    "text": "10.2 kernel"
  },
  {
    "objectID": "home_ranges.html#locoh",
    "href": "home_ranges.html#locoh",
    "title": "\n10  Home ranges\n",
    "section": "\n10.3 LoCoH",
    "text": "10.3 LoCoH"
  },
  {
    "objectID": "home_ranges.html#akde",
    "href": "home_ranges.html#akde",
    "title": "\n10  Home ranges\n",
    "section": "\n10.4 akde",
    "text": "10.4 akde"
  },
  {
    "objectID": "behavioral_changes.html#change-points",
    "href": "behavioral_changes.html#change-points",
    "title": "\n11  Behavioral Changes\n",
    "section": "\n11.1 Change points",
    "text": "11.1 Change points"
  },
  {
    "objectID": "behavioral_changes.html#segmentation",
    "href": "behavioral_changes.html#segmentation",
    "title": "\n11  Behavioral Changes\n",
    "section": "\n11.2 Segmentation",
    "text": "11.2 Segmentation"
  },
  {
    "objectID": "behavioral_changes.html#state-space-models",
    "href": "behavioral_changes.html#state-space-models",
    "title": "\n11  Behavioral Changes\n",
    "section": "\n11.3 State-space models",
    "text": "11.3 State-space models"
  },
  {
    "objectID": "behavioral_changes.html#migration-dispersal",
    "href": "behavioral_changes.html#migration-dispersal",
    "title": "\n11  Behavioral Changes\n",
    "section": "\n11.4 Migration & dispersal",
    "text": "11.4 Migration & dispersal"
  },
  {
    "objectID": "hmms.html#what-are-ssms-and-hmms",
    "href": "hmms.html#what-are-ssms-and-hmms",
    "title": "\n12  Hidden Markov and State-Space Modeling\n",
    "section": "\n12.1 What are SSM’s and HMM’s?",
    "text": "12.1 What are SSM’s and HMM’s?"
  },
  {
    "objectID": "hmms.html#implementation-with-stan",
    "href": "hmms.html#implementation-with-stan",
    "title": "\n12  Hidden Markov and State-Space Modeling\n",
    "section": "\n12.2 Implementation with STAN",
    "text": "12.2 Implementation with STAN"
  }
]