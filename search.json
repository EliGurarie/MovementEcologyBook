[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Techniques and Concepts in Movement Ecology",
    "section": "",
    "text": "Preface\nThis is a living (and collaborative) document which primarily serves as course materials for EFB 796: Techniques and Concepts in Spatial and Movement Ecology - a graduate-level course I am currently (Spring 2023) teaching in the Dep’t of Environmental Biology at SUNY-ESF, together with some able assistance and collaboration. But it is also being developed - perhaps presumptuously - as the beginnings of a book. It is, of course, far more typical for people who embark on putting together a “book” to first teach a course for several years (or decades!) before making the leap into a text. But the smoothness of generating these materials in an organized way makes the line between course materials and a book somewhat blurred1.1 Big shout out to the team behind Quarto, and - in particular - for the template we baldly borrowed in R for Data Science (2e), for blurring these lines in the best possible way.\nLectures will gradually be modified into something resemble “chapters”, the structure might change, more materials might flow in or out, references will be added, some proof-reading might occur, and there may be some more ephemeral course specific materials popping up here and there. It will (at least throughout the duration of the semester) be a chaotic and dynamic place. There will be incomplete sentences and incomplete thoughts and tremendous gaps. But the goal of this effort is, really, to put together the modern digital equivalent of a book that serves as a single, organized source for mathematical, statistical and computational techniques, complete with examples, for the modeling and analysis of movement (and some other kinds of spatial) data.\n\nNote: This is an open project! Any suggestions, corrections, additions, materials, feedback, please do email me.\n\nThe authors: E. Gurarie, N. Barbour, O. Couriot, others."
  },
  {
    "objectID": "introductions.html#to-the-course",
    "href": "introductions.html#to-the-course",
    "title": "1  Introduction(s)",
    "section": "1.1 … to the Course",
    "text": "1.1 … to the Course\nMovement is an ubiquitous and fundamental property of all biological organisms. In this course/book, we will learn to model and analyze spatial patterns and animal movement data. Starting with a strong statistical foundation on time-series and spatial processes, we will cover topics such as point patterns, migration, dispersal, home-ranging, behavioral change point analysis, multi-state modeling, continuous-time movement models, and perhaps touch on cutting edge topics like cognitive movement ecology and collective movements. The overarching goal will be to match mathematical models, statistical techniques and computational tools to ecologically meaningful questions. Conceptual and theoretical lectures will be combined with practical work in R, including developing novel Bayesian MCMC tools in STAN. The course will culminate with student-led research projects involving some non-trivial analysis of spatial data.\nThe structure of the class will be somewhat improvised. But there will be a combination of lectures and labs, and perhaps some guest lecturers on special topics. There will be weekly homework assignments and a final project that involves a novel, non-trivial analysis to be worked on and presented at the end of the semester.\nAll materials will be on this course website and integrated into the document-style lecture materials.\nThus, for example, assignments will be posted at this link: Assignments."
  },
  {
    "objectID": "introductions.html#to-the-book",
    "href": "introductions.html#to-the-book",
    "title": "1  Introduction(s)",
    "section": "1.2 …to the Book",
    "text": "1.2 …to the Book\nCollecting movement data is one of the main tools of wildlife ecologists, whether via satellite, GPS, GSM, acoustic, terrestrial, marine, freshwater, avian. Technologies are constantly improving, datasets are increasing at dizzying rates [refs]. Following in the swirling wake (and occasionally flashing ahead) of this data tsunami is a colorful and confounding array of methodological tools. To some extent, this is to be expected, since every tool is designed to address a particular kind of ecological or behavioral question and, at least in development, most tools are also designed around particular systems and particularities of the data collected. An extensive and influential branch of movement analysis, for example, was built around analyzing movements of marine organisms (e.g. sea turtles) from extremely gappy and notoriously error-ridden (but remarkable for the time) ARGOS data (Jonsen, Myers, and Flemming 2003, 2005). But the questions marine researchers were asking (e.g. can we adequately infer where the animal is at all from gappy and error-ridden data?) are profoundly different from the questions asked in a terrestrial contexts, where the question might be how are movement behaviors changing in a particular habitat? (Morales et al. 2004; Forester et al. 2007).\nMovement ecology - at this point - is no longer such a young field1. There is a dedicated journal, thousands of papers reporting on some (non-trivial) analysis of animal movement data, hundred(s?) just introducing and developing methodological tools, dozen(s) of R packages with which to implement those tools, and so on. But - remarkably - there are very few single “go-to” sources for students or practitioners to refer to in starting out in analysis.1 Set aside the fact that Aristotle discussed the migration of birds in his Historia Animalium 2500 years ago, much less the recent suggestion that the quantitative observation of animal migrations possibly predates written language(Bacon et al. 2023).\nCertainly, two books are worth mentioning: Peter Turchin’s Quantitative Analysis of Movement (Turchin 1998) is a “seminal” text with much to offer, but not particularly useful in a modern context. Only fairly recently have there been a book-length synthesis of some of the statistical models for animal movement data by several outstanding statistical (movement) ecologists in Hooten, Johnson, McClintock and Morales’ Animal Movement: Statistical Models for Telemetry Data (Hooten et al. 2017). While this text is an absolutely essential compendium of the main developments in movement modeling in the past two decades, it can be technically daunting for students or practitioners without strong statistical training. Also - in what is clearly the correct decision for a statistical, printed text that should stand the test of time - it provides many equations and figures, but no R code.\n\n\n\n\n\n\n\nTurchin et al. 1998\n\n\n\n\n\n\n\nHooten et al. 2017\n\n\n\n\nFigure 1.1: Nearly the only existing full length academic texts on analysis of animal movement data.\n\n\nThe goals of this eventual book are to provide a rigorous, comprehensive, and somewhat less technical, but highly visual and example-driven guide through working with spatial and movement data and understanding some of the most important tools and techniques for interpreting and analyzing those data.\n\n\nEadweard Muybridge demonstrated that bison move"
  },
  {
    "objectID": "introductions.html#references",
    "href": "introductions.html#references",
    "title": "1  Introduction(s)",
    "section": "References",
    "text": "References\n\n\n\n\nBacon, Bennett, Azadeh Khatiri, James Palmer, Tony Freeth, Paul Pettitt, and Robert Kentridge. 2023. “An Upper Palaeolithic Proto-writing System and Phenological Calendar.” Cambridge Archaeological Journal, January, 1–19. https://doi.org/10.1017/S0959774322000415.\n\n\nForester, J. D., A. R. Ives, M. G. Turner andD P. Anderson, D. Fortin, H. L. Beyer, D. W. Smith, and M. S. Boyce. 2007. “State-Space Models Link Elk Movement Patterns to Landscape Characteristics in Yellowstone National Park.” Ecological Monographs 77: 285–99. https://doi.org/10.1890/06-0534.\n\n\nHooten, Mevin B., Devin S. Johnson, Brett T. McClintock, and Juan M. Morales. 2017. Animal Movement: Statistical Models for Telemetry Data. First. Boca Raton : CRC Press, 2017.: CRC Press. https://doi.org/10.1201/9781315117744.\n\n\nJonsen, I. D., R. A. Myers, and J. M. Flemming. 2003. “Meta-Analysis of Animal Movement Using State-Space Models.” Wildlife Research 21: 149–61.\n\n\n———. 2005. “Robust State-Space Modeling of Animal Movement Data.” Ecology 86: 2874–80. https://doi.org/10.1890/04-1852.\n\n\nMorales, Juan Manuel, Daniel T. Haydon, Jacqui Frair, Kent E. Holsinger, and John M. Fryxell. 2004. “Extracting More Out of Relocation Data: Building Movement Models as Mixtures of Random Walks.” Ecology 85 (9): 2436–45. https://doi.org/10.1890/03-0269.\n\n\nTurchin, P. 1998. Quantitative Analysis of Movement: Measuring and Modeling Population Redistribution in Animals and Plants. Sunderland, Mass.: Sinauer."
  },
  {
    "objectID": "whyspecialstats.html#spatial-data",
    "href": "whyspecialstats.html#spatial-data",
    "title": "\n3  Why do we need special statistics?\n",
    "section": "\n3.1 Spatial data",
    "text": "3.1 Spatial data\nSpatial data is simply data that contains spatial coordinates - in practice, we are almost always only concerned with two dimensions (\\({\\bf X, Y}\\)) with perhaps a set of additional observations \\(A\\). footnote: We use boldfacing to indicate vectors (e.g. of length \\(n\\)), and capitals to indicate matrices such that \\(A\\) is a \\(n \\times k\\) matrix for \\(k\\) observations. Spatial data may represent a complete set of observations, for example it can represent the locations of all trees of a certain size in a given study plot. Often in such cases there are questions that can be asked about the generating process which determines the distribution of those locations. When describing a process, the coordinates themselves and alone can be considered a “response”. For example, a typical question might be: are the trees clustered, inhibiting each other, or randomly distributed? This is referred to as analyzing a point process (see Chapter XX).\n\n\n\n\nCoordinates of American beech (Fagus grandifolia) in a research site at Adirondack Station, SUNY-ESF, New York State. The sizes reflect diameter at breast height in 2009. (Gienke et al. 2014, data: M. Dovciak)\n\n\n\n\nAlternatively, spatial data can be considered a sampling of a continuous spatial structure. For example, we might measure a set of soil properties (pH, saility, moisture, thickness) over a study area, and have some questions about why the soil is structured the way it is. In those cases, the spatial coordinates are not of intrinsic interest (they were established by the reserach), but more similar to a “nuisance” parameter. To make sure that any inference we make on those data is “correct”, i.e. not unduly influenced by that autocorrelation, we have to take that autocorrelation into account. On the other hand, the scale of spatial autocorrelation of a process can itself be a variable of interest. Proper ways to do this is very much a central topic in spatial ecology as well (Chapter XX)."
  },
  {
    "objectID": "whyspecialstats.html#movement-data",
    "href": "whyspecialstats.html#movement-data",
    "title": "\n3  Why do we need special statistics?\n",
    "section": "\n3.2 Movement data",
    "text": "3.2 Movement data\nMovement data - representing a measured sequence of locations for an individual organism - can be thought of as spatial data that is indexed in time, or - equivalently - as a two-dimensional time-series.\n\n one-dimensional: Multi-Dimensional! (X,Y,Time) > Solution: get comfortable with complex numbers.\n independent: Highly dependent! > Solution: get comfortable with  correlated / dependent data .\n identically distributed!: Complex and heterogeneous! > Solution: behavioral change point analysis / segmentation, hidden markov models.\n often normal: Circular and skewed distributions! > Solution: Flexible Likelihood-based modelling,\n randomly/uniformly sampled: Gappy / irregular / oddly duty-cycled data > Solution: Continuous movement modeling."
  },
  {
    "objectID": "complex_numbers.html#sec-complexnumbers",
    "href": "complex_numbers.html#sec-complexnumbers",
    "title": "\n4  Complex Numbers are Simple\n",
    "section": "\n4.1 Components of complex number",
    "text": "4.1 Components of complex number\nComplex numbers are a handy bookkeeping tool to package together two dimensions in a single quantity. They expand the one-dimensional (“Real”) number line into a second dimension (“Imaginary”).\nWe write a complex number \\(Z = X + iY\\)\n\n\n\\(X\\) is the real part.\n\n\\(Y\\) is the imaginary part.\n\nThe same number can be written in terms of distances and angles (which are a more “natural” language for spatial data):\n\\[ Z = R \\, \\exp(i \\theta) \\]\n\n\n\\(R\\) is the length of the vector from the origin, aka modulus.\n\n\\(\\theta\\) is the orientation of the vector, aka argument.\n\nThis angle, \\(\\theta\\) is defined in radians that turn counter-clockwise from the x-axis, \\(\\pi/2\\) is on the y-axis, \\(\\pi\\) is on the negative x-axis, etc. You can see that by simply putting in some numbers:\n\\[Z = 1 = \\exp{0}\\] \\[Z = 0 + 1i = \\exp{(i \\pi/2)}\\]\nAll of these have modulus 1.\nMore that the algebraic / trigonometric way to get distances (moduli) and angles (arguments) from a vector is a bit tedious. Thus, for \\(Z = X + iY\\):\n\\[R = \\sqrt{X^2 + Y^2}\\] Not terrible. But check out how to compute the angle from x and y:\n\\[\n\\theta(x,y) =\n\\begin{cases}\n\\arctan\\left(\\frac y x\\right) &\\text{if } x > 0, \\\\[5mu]\n\\arctan\\left(\\frac y x\\right) + \\pi &\\text{if } x < 0 \\text{ and } y \\ge 0, \\\\[5mu]\n\\arctan\\left(\\frac y x\\right) - \\pi &\\text{if } x < 0 \\text{ and } y < 0, \\\\[5mu]\n+\\frac{\\pi}{2} &\\text{if } x = 0 \\text{ and } y > 0, \\\\[5mu]\n-\\frac{\\pi}{2} &\\text{if } x = 0 \\text{ and } y < 0, \\\\[5mu]\n\\text{undefined} &\\text{if } x = 0 \\text{ and } y = 0.\n\\end{cases}\n\\]\nYikes!\nOn the other hand, these quantities are obtained instantly and easily in R.\n\n4.1.1 In R\n\nX <- c(3,4,-2)\nY <- c(0,3,2)\nZ <- X + 1i*Y\nZ\n\n[1]  3+0i  4+3i -2+2i\n\n\nAlternatively:\n\nZ <- complex(re = X, im=Y)\n\n\nplot(Z, pch=19, col=1:3, asp=1)\narrows(rep(0,length(Z)), rep(0,length(Z)), Re(Z), Im(Z), lwd=2, col=1:3)\n\n\n\n\nNote: ALWAYS use asp=1 - “aspect ratio = 1:1” - when plotting (properly projected) movement data!\nObtaining summary statistics is nearly instant. Obtain lengths of vectors:\n\nMod(Z)\n\n[1] 3.000000 5.000000 2.828427\n\n\nObtain orientation of vectors:\n\nArg(Z)\n\n[1] 0.0000000 0.6435011 2.3561945\n\n\nNote, the orientations are in radians, i.e. range from \\(0\\) to \\(2\\pi\\) going counter-clockwise from the \\(x\\)-axis. Compass directions go from 0 to 360 clockwise, so, to convert:\n\n90-(Arg(Z)*180)/pi\n\n[1]  90.0000  53.1301 -45.0000"
  },
  {
    "objectID": "complex_numbers.html#quickly-simulating-a-path",
    "href": "complex_numbers.html#quickly-simulating-a-path",
    "title": "\n4  Complex Numbers are Simple\n",
    "section": "\n4.2 Quickly simulating a path",
    "text": "4.2 Quickly simulating a path\nQuick code for a correlated random walk:\n\nX <- cumsum(arima.sim(n=100, model=list(ar=.7)))\nY <- cumsum(arima.sim(n=100, model=list(ar=.7)))\nZ <- X + 1i*Y\nplot(Z, type=\"o\", asp=1)\n\n\n\n\nInstant summary statistics of a trajectory:\nThe average location\n\nmean(Z)\n\n[1] 17.91147-41.70098i\n\n\nThe step vectors:\n\ndZ <- diff(Z)\nplot(dZ, asp=1, type=\"n\")\narrows(rep(0, length(dZ)), rep(0, length(dZ)), Re(dZ), Im(dZ), col=rgb(0,0,0,.5), lwd=2, length=0.1)\n\n\n\n\nDistribution of step lengths:\n\nS <- Mod(dZ)\nsummary(S)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.1983  0.9115  1.5089  1.5806  2.0551  4.0748 \n\nhist(S, col=\"grey\", bor=\"darkgrey\", freq=FALSE)\nlines(density(S), col=2, lwd=2)\n\n\n\n\nWhat about angles?\n\nThe absolute orientations:\n\n\nPhi <- Arg(dZ)\nhist(Phi, col=\"grey\", bor=\"darkgrey\", freq=FALSE, breaks=seq(-pi,pi,pi/3))\n\n\n\n\n\nTurning angles\n\n\nTheta <- diff(Phi)\nhist(Theta, col=\"grey\", bor=\"darkgrey\", freq=FALSE)\n\n\n\n\n QUESTION: What is a problem with this histogram?\nCircular statistics:\nAngles are a wrapped continuous variable, i.e. \\(180^o > 0^o = 360^o < 180^o\\). The best way to visualize the distribution of wrapped variables is with Rose-Diagrams. An R package that deals with circular data is circular.\n\nrequire(circular)\nTheta <- as.circular(Theta)\nPhi <- as.circular(Phi)\nrose.diag(Phi, bins=16, col=\"grey\", prop=2, main=expression(Phi))\nrose.diag(Theta, bins=16, col=\"grey\", prop=2, main=expression(Theta))\n\n\n\n\nLAB EXERCISE\n\n\nLoad movement data of choice!\nConvert the locations to a complex variable Z.\nObtain a vector of time stamps T, draw a histogram of the time intervals. Then, ignore those differences.\nObtain, summarize and illustrate:\n\n\nthe step lengths\nthe absolute orientation\nthe turning angles"
  },
  {
    "objectID": "complex_numbers.html#complex-manipulations-are-easy",
    "href": "complex_numbers.html#complex-manipulations-are-easy",
    "title": "\n4  Complex Numbers are Simple\n",
    "section": "\n4.3 Complex manipulations (are easy)",
    "text": "4.3 Complex manipulations (are easy)\n\n4.3.1 Addition & substration of vectors\nAddition and subtraction of vectors:\n\\[ Z_1 = X_1 + i Y_1; Z_2 = X_2 + i Y_2\\] \\[ Z_1 + Z_2 = (X_1 + X_2) + i(Y_1 + Y_2)\\]\nUseful, e.g., for shifting locations:\n\nplot(Z, asp=1, type=\"l\", col=\"darkgrey\", xlim=c(-2,2)*max(Mod(Z)))\nlines(Z - mean(Z), col=2, lwd=2)\nlines(Z + Z[length(Z)], col=3, lwd=2)\n\n\n\n\n\n4.3.2 Rotation of vectors\nMultiplication of complex vectors \\[Z_1 = R_1 \\exp(i \\theta_1); Z_2 = R_2 \\exp(i \\theta_2)\\] \\[ Z_1 Z_2 = R_1 R_2 \\exp(i (\\theta_1 + \\theta_2))\\] Note the magic of the rotation summing! If \\(\\text{Mod}(Z_2) = 1\\), multiplications rotates by \\(\\text{Arg}(Z_2)\\)\n\ntheta1 <- complex(mod=1, arg=pi/4)\ntheta2 <- complex(mod=1, arg=-pi/4)\n\nplot(Z, asp=1, type=\"l\", col=\"darkgrey\", lwd=3)\nlines(Z*theta1, col=2, lwd=2)\nlines(Z*theta2, col=3, lwd=2)\n\n\n\n\nA colorful loop:\n\nrequire(gplots)\nplot(Z, asp=1, type=\"n\", xlim=c(-1,1)*max(Mod(Z)), ylim=c(-1,1)*max(Mod(Z)))\ncols <- rich.colors(1000,alpha=0.1)\nthetas <- seq(0,2*pi,length=100)\nfor(i in 1:1000)\n  lines(Z*complex(mod=1, arg=thetas[i]), col=cols[i], lwd=4)\n\n\n\n\nI know you’re probably thinking …\n\n“Thanks for teaching me how to make a weird swirling rainbow thing … but why in the world would I want to shift and rotate my precious, precious data, which was just perfect the way it was?\n\nMy response:  Null Sets for Pseudo Absences!"
  },
  {
    "objectID": "complex_numbers.html#example-with-finnish-wolves",
    "href": "complex_numbers.html#example-with-finnish-wolves",
    "title": "\n4  Complex Numbers are Simple\n",
    "section": "\n4.4 Example with Finnish Wolves",
    "text": "4.4 Example with Finnish Wolves\n\nIn-depth summer predation study, questions related to habitat use, landscape type - forest/bog/field - linear elements - roads/rivers/power lines, etc.\n\n\n4.4.1 Defining Null Sets\n\nObtain all the steps and turning angles\nRotate them by the orientation of the last step (\\(Arg(Z_1-Z_0)\\))\nAdd the rotated steps to the last step (\\(Z_1\\))\n\n\n\n\nObtain all the steps and turning angles\nRotate them by the orientation of the last step (\\(Arg(Z_1-Z_0)\\))\n\n\nn <- length(Z)\nS <- Mod(diff(Z))\nPhi <- Arg(diff(Z))\nTheta <- diff(Phi)\nRelSteps <- complex(mod = S[-1], arg=Theta)\n\nZ0 <- Z[-((n-1):n)]\nZ1 <- Z[-c(1,n)]\nZ2 <- Z[-(1:2)]\n\nRotate <- complex(mod = 1, arg=Arg(Z1-Z0))\n\n\nplot(c(0, RelSteps), asp=1, xlab=\"x\", ylab=\"y\", pch=19)\narrows(rep(0,n-2), rep(0, n-2), Re(RelSteps), Im(RelSteps), col=\"darkgrey\")\n\n\n\n\n\nNote: in practice (i.e. with tons of data), it is sufficient to randomly sample some smaller number (e.g. 30) null steps at each location.\n\n\nAdd the rotated steps to the last step\n\n\nZ.null <- matrix(0, ncol=n-2, nrow=n-2)\nfor(i in 1:length(Z1))\n  Z.null[i,] <- Z1[i] + RelSteps * Rotate[i]\n\n\nMake the fuzzy catterpillar plot\n\n\nplot(Z, type=\"o\", col=1:length(Z), pch=19, asp=1)\nfor(i in 1:nrow(Z.null))\n  segments(rep(Re(Z1[i]), n-2), rep(Im(Z1[i]), n-2), \n           Re(Z.null[i,]), Im(Z.null[i,]), col=i+1)\n\n\n\n\n\n4.4.2 Null set\nThe use of the null set is a way to test a narrower null hypothesis that accounts for auto correlation in the data.\nThe places the animal COULD HAVE but DID NOT go to are pseudo-absences, against which you can fit, e.g., logistic regression models (aka Step-selection functions).\nOr just be simple/lazy (like us) and compare observed locations with Chi-squared tests:\n\n EXERCISE: Create a fuzzy-catterpillar plot!\nUse (a portion) of the data you analyzed before.\n\n# get pieces\nn <- length(Z)\nSteps <- diff(Z)\nS <- Mod(Steps)\nPhi <- Arg(Steps)\nTheta <- diff(Phi)\nRelSteps <- complex(mod = S[-1], arg = Theta)\n\n# calculate null set\nZ0 <- Z[1:(n-2)]\nZ1 <- Z[2:(n-1)]\nZ2 <- Z[3:n]\nRotate <- complex(mod = 1, arg = Arg(Z1-Z0))\n\nZ.null <- matrix(NA, ncol=n-2, nrow=n-2)\nfor(i in 1:length(Z1))\n  Z.null[i,] <- Z1[i] + sample(RelSteps) * Rotate[i]\n\n# plot\nplot(Z, type=\"o\", col=1:10, pch=19, asp=1)\nfor(i in 1:nrow(Z.null))\n  segments(rep(Re(Z1[i]), n-2), rep(Im(Z1[i]), n-2), \n           Re(Z.null[i,]), Im(Z.null[i,]), col=i+1)\n\n\n Fuzzy Polar Bear Catterpillar!"
  },
  {
    "objectID": "spatial_data_in_R.html",
    "href": "spatial_data_in_R.html",
    "title": "\n5  Spatial data in R\n",
    "section": "",
    "text": "In this chapter we cover spatial data in R - notably sf, sp, and raster, and some basic manipulations of spatial data objects. We may refer somewhat heavily to other resources on the web, for example: https://cengel.github.io/R-spatial/"
  },
  {
    "objectID": "movement_data_in_R.html#preamble",
    "href": "movement_data_in_R.html#preamble",
    "title": "\n6  Movement data in R\n",
    "section": "\n6.1 Preamble",
    "text": "6.1 Preamble\nThe goal of this tutorial is to provide the tools to comprehend the structure of certain common types of animal movement data, as well as to load, process and visualize movement data using R in Rstudio.\nIn particular we will be accessing data from Movebank, a free, online database of animal tracking data, where users can store, process, and share (or not share) location data. In the diverse and highly fragmented world of animal movement ecology, it is the closest to a “standard” format for movement data.\nFor this tutorial, several packages for downloading, processing and visualizing data are required. We will use: - plyr - dplyr - magrittr - lubridate - move - sf - mapview - ggplot2. Notably, the move package allows you to access and process data from Movebank directly from R.\nInstall and load these packages, for example via:\n\n# create packages list\npackages <- list(\"plyr\",\"dplyr\",\"magrittr\", \"lubridate\",\"move\",\"sp\",\"sf\", \"mapview\",\"ggplot2\")\nsapply(packages, require, character = TRUE)"
  },
  {
    "objectID": "movement_data_in_R.html#movebank-data",
    "href": "movement_data_in_R.html#movebank-data",
    "title": "\n6  Movement data in R\n",
    "section": "\n6.2 Movebank Data",
    "text": "6.2 Movebank Data\nHere as an example we will use elk (Cervus elaphus1) movement data from the Ya Ha Tinda Long-Term Elk Monitoring Project (Hebblewhite et al. 2020), which are hosted on Movebank (https://www.movebank.org). The dataset that we are using is a subset of the data, and contains GPS locations of 10 female Elk from Banff National Park, in Alberta. You can access the data as a .csv file at this link. But it is worth visiting MoveBank, getting an account, and learning how to navigate the website. For example, you can log in on MoveBank, clicking Data \\> Studies, and changing from Studies where I am the Data Manager to All studies, and search “Ya Ha Tinda elk project, Banff National Park, 2001-2020 (females)” . Click on Download > Download data and follow further instructions.1 Or is it Cervus canadensis?\n\n6.2.1 Loading movement data in R\nIf you are working from data on Movebank - including data with controlled or limited access that you have access to - you can load the data with the move::getMovebankData() function [^1: recall, the move:: notation just means that the function comes from the move package] using your own login and password. This is a two step process. First you have to create an object that contains your login information:\n\nrequire(move)\nmylogin <- movebankLogin(username='XXX', password='XXX')\n\nReplacing the XXX’s with your private information. Next, you use that login in the getMovebankData function:\n\nelk_move <- getMovebankData(study = \"Ya Ha Tinda elk project, Banff National Park, 2001-2020 (females)\",\n    login = mylogin, removeDuplicatedTimestamps=TRUE)\n\nThis step might take a few minutes, since the data contains more than 1.5 million observations (GPS locations) and it depends on the quality and the speed of your Internet Connection.\n\n(OC?): WOULD BE GOOD TO INCLUDE HERE THE CODE THAT SUBSETS THE MOVEMENT DATA TO THE SUBSET YOU USE BELOW.\n\n\n6.2.2 Load the data from your computer\nIf you downloaded the data on your computer, from the website. You can use the read.csv() function to open it on R, using the path of the file on your Disk.\n\nelk_df <- read.csv(\"data/example_df.csv\")\n\n\n6.2.3 Data structure\nNow that we loaded the data, we can explore its structure, dimensions, data types, etc. The following functions all provide some important or, at least, useful information:\nThe “class” of the object is given by looking at the object in the Environment on the right, or the class() or, more informative, is() functions:\n\nclass(elk_df)\n\n[1] \"data.frame\"\n\nis(elk_df)\n\n[1] \"data.frame\" \"list\"       \"oldClass\"   \"vector\"    \n\n\nWhen using the getMovebankData() function, the resulting object is a MoveStack. We can use the as.data.frame() (or just data.frame()), to transform it into a data frame and see all the elements of the object. If you loaded the data from your computer, it is a data.frame.\nThe dimensions (number or rows and columns), by dim().\n\ndim(elk_df)\n\n[1] 97825     7\n\n\nThe names of the columns with names():\n\nnames(elk_df)\n\n[1] \"ID\"   \"Time\" \"sex\"  \"Lon\"  \"Lat\"  \"X\"    \"Y\"   \n\n\nThe first few rows with head(), useful for a quick look:\n\nhead(elk_df)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nTime\nsex\nLon\nLat\nX\nY\n\n\n\nGR122\n2006-03-06 16:22:41\nf\n-115.5828\n51.75451\n597820.5\n5734685\n\n\nGR122\n2006-03-06 19:01:07\nf\n-115.5830\n51.75455\n597806.6\n5734689\n\n\nGR122\n2006-03-06 23:01:48\nf\n-115.5865\n51.73794\n597600.9\n5732837\n\n\nGR122\n2006-03-07 03:01:46\nf\n-115.5774\n51.73691\n598231.4\n5732735\n\n\nGR122\n2006-03-07 11:01:46\nf\n-115.5828\n51.75442\n597820.7\n5734675\n\n\nGR122\n2006-03-07 15:01:05\nf\n-115.5828\n51.75447\n597820.6\n5734680\n\n\n\n\n\nMovement data at the very minimum contains x and y (here, columns X and Y) or longitude/latitude (Lon and Lat) coordinates, and an associated timestamp (Time). If there is more than one individual, there will also be a unique identifier (ID). Additional columns might also be provided, as here with the sex of the individual.22 A warning: you might have loaded data with the readr::read_csv() function instead of read.csv() - that is the default behavior if you click on a data file in newer versions of Rstudio, for example. This function will convert all of the sex column to the Boolean FALSE - implying that all of these elk are abstinent. This is an example of new and improved versions of software occasionally being a wee bit too smart for their own good.\nMore detail about the structure of the data frame is given by str():\n\nstr(elk_df)\n\n'data.frame':   97825 obs. of  7 variables:\n $ ID  : chr  \"GR122\" \"GR122\" \"GR122\" \"GR122\" ...\n $ Time: chr  \"2006-03-06 16:22:41\" \"2006-03-06 19:01:07\" \"2006-03-06 23:01:48\" \"2006-03-07 03:01:46\" ...\n $ sex : chr  \"f\" \"f\" \"f\" \"f\" ...\n $ Lon : num  -116 -116 -116 -116 -116 ...\n $ Lat : num  51.8 51.8 51.7 51.7 51.8 ...\n $ X   : num  597821 597807 597601 598231 597821 ...\n $ Y   : num  5734685 5734689 5732837 5732735 5734675 ...\n\n\nWe see that all of the variables are character or numeric. For easy manipulation and visualization, it is more convenient if ID and sex are factors. To do so, we can use the mutate function, from the plyr package, which allows to create, delete or modify columns.\n\n# set ID and sex as factors\nelk_df <- elk_df %>% mutate(ID = as.factor(ID), sex = as.factor(sex))\n\n\n6.2.4 Manipulating time\nIn addition, the characteristic of movement data is that they are spatio-temporal. This means that each GPS location is associated to a Date and Time. In our case, the Time column is a character object. The function as.POSIXct() converts a time column (e.g., considered as a character) into a POSIXct object - a standard computer format for temporal data (see ?as.POSIXct for more information). Note that the Date in R is always in the format Year-Month-Day Hour:Minute:Second (yyyy-mm-dd hh:MM:ss). Similar to transforming our variables in factors, we use the plyr::mutate() function to transform Time into POSIXct format.\n\nelk_df <- elk_df %>% mutate(Time = as.POSIXct(Time, tz = \"UTC\"))\n\nThis can also be done with some generally more easy-to-use lubridate package functions. For example: ymd_hms() will convert from this format to POSIX:\n\nelk_df <- elk_df %>% mutate(Time = ymd_hms(Time, tz = \"UTC\"))\n\nwhereas if the data were loaded from the original file in some obnoxious format, like the U.S. standard mm/dd/yyyy, this could be converted via:\n\nelk_df <- elk_df %>% mutate(Time = mdy_hms(Time, tz = \"UTC\"))\n\nand so on.\nThe lubridate also offers useful functions for extracting infromation from POSIX times. For example, the hour of day (hour()), the day of year (yday()), the month (month() or the year (year()). A convenient feature of mutate() is that multiple just commands can be strung together in a single command, for example:\n\nelk_df <- elk_df %>% mutate(doy = yday(Time),\n                            Month = month(Time),\n                            Year = year(Time))\nhead(elk_df)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nTime\nsex\nLon\nLat\nX\nY\ndoy\nMonth\nYear\n\n\n\nGR122\n2006-03-06 16:22:41\nf\n-115.5828\n51.75451\n597820.5\n5734685\n65\n3\n2006\n\n\nGR122\n2006-03-06 19:01:07\nf\n-115.5830\n51.75455\n597806.6\n5734689\n65\n3\n2006\n\n\nGR122\n2006-03-06 23:01:48\nf\n-115.5865\n51.73794\n597600.9\n5732837\n65\n3\n2006\n\n\nGR122\n2006-03-07 03:01:46\nf\n-115.5774\n51.73691\n598231.4\n5732735\n66\n3\n2006\n\n\nGR122\n2006-03-07 11:01:46\nf\n-115.5828\n51.75442\n597820.7\n5734675\n66\n3\n2006\n\n\nGR122\n2006-03-07 15:01:05\nf\n-115.5828\n51.75447\n597820.6\n5734680\n66\n3\n2006\n\n\n\n\n\nOur data set is more or less as we want it. Depending on your research questions, you might have additional variables, and you should make sure that those variables are in the correct format. It is also worth noting that once you know what your raw data set looks like and you know how it should look like before starting processing, all those manipulations (loading the data, redefining the class of the columns, adding new columns) can be done in one line of code:\n\nelk_df <- read.csv(\"data/example_df.csv\") %>% \n  mutate(ID = as.factor(ID), sex = as.factor(sex),\n         Time = ymd_hms(Time, tz = \"UTC\"),\n         doy = yday(Time), Month = month(Time), Year=year(Time))"
  },
  {
    "objectID": "movement_data_in_R.html#exploring-raw-movement-data",
    "href": "movement_data_in_R.html#exploring-raw-movement-data",
    "title": "\n6  Movement data in R\n",
    "section": "\n6.3 Exploring raw movement data",
    "text": "6.3 Exploring raw movement data\nNow that the data set is more manageable, we can explore the data, and especially look at the number of individuals, their sex, the duration of their monitoring, the fix rate, the number of missing data, etc.\nHowever, as movement data is a time series, it is important when manipulating to FIRST order it by Individual and Time. The function arrange from the plyr (or dplyr) package is very handy:\n\nelk_df <- elk_df %>% arrange(ID, Time)\n\nHere, we walk through a sequence of some basic questions to be asked of movement data. s\n\n6.3.1 How many individuals are there?\nAnd what are their ID?\n\nlength(unique(elk_df$ID)) \n\n[1] 10\n\nunique(elk_df$ID)\n\n [1] GR122       GR133       GR159       GR513       OR39        YL115_OR34 \n [7] YL121_BL250 YL5         YL77        YL89       \n10 Levels: GR122 GR133 GR159 GR513 OR39 YL115_OR34 YL121_BL250 YL5 ... YL89\n\n\n\n6.3.2 How many females/males?\nThere are several “base” R ways to count the males and females. For example the following three commands all give (essentially) the same output:\n\nunique(elk_df[,c(\"sex\",\"ID\")])\nunique(data.frame(elk_df$sex, elk_df$ID))\nwith(elk_df, unique(data.frame(sex, ID)))\n\n\n\n\n\n\nsex\nID\n\n\n\n1\nf\nGR122\n\n\n21568\nf\nGR133\n\n\n39147\nf\nGR159\n\n\n45195\nf\nGR513\n\n\n59780\nf\nOR39\n\n\n67409\nf\nYL115_OR34\n\n\n83906\nf\nYL121_BL250\n\n\n87390\nf\nYL5\n\n\n90251\nf\nYL77\n\n\n92890\nf\nYL89\n\n\n\n\n\ni.e., a unique combination of sex and ID. An (uninteresting) count of those sexes is given by:\n\nid.sex.count <- unique(elk_df[,c(\"sex\",\"ID\")])\ntable(id.sex.count$sex)\n\n\n f \n10 \n\n\nA more trendy approach is to use functions in the dplyr package, which allows you to summarize information per group.\n\nelk_df %>% group_by(ID) %>% summarize(sex = unique(sex))\n\n# A tibble: 10 × 2\n   ID          sex  \n   <fct>       <fct>\n 1 GR122       f    \n 2 GR133       f    \n 3 GR159       f    \n 4 GR513       f    \n 5 OR39        f    \n 6 YL115_OR34  f    \n 7 YL121_BL250 f    \n 8 YL5         f    \n 9 YL77        f    \n10 YL89        f    \n\n\nThe resulting object is a tibble which is very similar to a data frame, but the preferred output of dplyr manipulations.\n\n6.3.3 How many locations per individual?\nTo look at the average number of GPS locations per individual and other statistics (e.g., standard deviation), by using the mean(), sd(), and related functions.\nBut first, we want to make sure that there are no missing locations (missing Longitude or Latitude), in this data set. is,na() checks whether each element is an NA or not, and table() just counts:\n\n# missing longitude\ntable(is.na(elk_df$Lon))\n\n\nFALSE \n97825 \n\n# missing latitude\ntable(is.na(elk_df$Lat))\n\n\nFALSE \n97825 \n\n\nNo missing locations! That’s a relief.\nLet’s look at the number of locations per individual:\n\n# average number of GPS locations per individual\ntable(elk_df$ID) %>% mean\n\n[1] 9782.5\n\n# standard deviation of the number of locations per individual\ntable(elk_df$ID) %>% sd\n\n[1] 7059.185\n\n\nOn average, an individual has more than 9000 locations. However, the standard deviation is almost as big, which means that some individuals have a lot of locations and some have fewer locations.\n\n# maximum number of GPS locations per individual\ntable(elk_df$ID) %>% max\n\n[1] 21567\n\n# minimum number of GPS locations per individual\ntable(elk_df$ID) %>% min\n\n[1] 2639\n\n\nYou can also access (almost) all the statistics by applying the all-purpose summary() function to the table() of the ID. Note that you need to convert it to a data frame first[^1]: [^1] The output of table() in R is actually quite weird.\n\n# summary of the number of GPS locations per individual\ntable(elk_df$ID) %>% data.frame %>% summary\n\n         Var1        Freq      \n GR122     :1   Min.   : 2639  \n GR133     :1   1st Qu.: 3847  \n GR159     :1   Median : 6838  \n GR513     :1   Mean   : 9782  \n OR39      :1   3rd Qu.:16019  \n YL115_OR34:1   Max.   :21567  \n (Other)   :4                  \n\n\n\n6.3.4 What is the duration of monitoring?\nThe functions min(), max() and range() are self-explanatory, but importantly work excellenty with properly formatted time objects. The diff() function calculates differences among subsequent elements of a vector. But for time (POSIX) data, it is best to use the difftime(t1, t2) function since that allows you to specify the units of the time difference (hours, days, etc.) Otherwise, strange things can happen. For example, the overall time span of the monitoring effort is:\n\nrange(diff(elk_df$Time))\n\nTime differences in secs\n[1] -256413590  293543997\n\n\nThe number of seconds is not SO helpful. But difftime is a bit more useful:\n\ndifftime(max(elk_df$Time), min(elk_df$Time), units = \"days\")\n\nTime difference of 5078.5 days\n\n\nBetter. That’s a lot of days. More than 10 years. Years is not an option with difftime, but to manipulate the output statistically, you need to convert to numeric. Thus the number of years\n\n(difftime(max(elk_df$Time), min(elk_df$Time), units = \"days\") %>% as.numeric)/365.25\n\n[1] 13.90417\n\n\nThat’s a good, long-term dataset!\nAnyways, what we really want is to figure this out for each individual. One approach is to use plyr::ddply commands. This function allows you apply functions to different groups (subsets) of a data set. Here’s an example:\n\nelk_df %>% ddply(\"ID\", plyr::summarize, start= min(Time),end = max(Time)) %>% \n  mutate(duration = difftime(end, start, units = \"days\"))\n\n\n\n\n\nID\nstart\nend\nduration\n\n\n\nGR122\n2006-03-06 16:22:41\n2007-06-08 07:00:58\n458.6099 days\n\n\nGR133\n2006-03-17 18:26:03\n2007-01-06 23:45:31\n295.2219 days\n\n\nGR159\n2005-03-27 16:33:15\n2005-11-09 13:00:50\n226.8525 days\n\n\nGR513\n2015-02-28 01:00:47\n2016-06-17 10:46:06\n475.4065 days\n\n\nOR39\n2014-02-26 01:00:42\n2014-11-23 15:02:51\n270.5848 days\n\n\nYL115_OR34\n2014-02-23 01:01:02\n2017-01-09 17:00:48\n1051.6665 days\n\n\n\n\n\nThe (near) equivalent with dplyr commands - just jumping straight to the duration:\n\nelk_df %>% group_by(ID) %>% \n  dplyr::summarize(duration = difftime(max(Time), min(Time), units = \"days\")) \n\n# A tibble: 10 × 2\n   ID          duration      \n   <fct>       <drtn>        \n 1 GR122        458.6099 days\n 2 GR133        295.2219 days\n 3 GR159        226.8525 days\n 4 GR513        475.4065 days\n 5 OR39         270.5848 days\n 6 YL115_OR34  1051.6665 days\n 7 YL121_BL250  378.1383 days\n 8 YL5          300.4165 days\n 9 YL77         250.0837 days\n10 YL89         229.8036 days\n\n\nTo get statistical summaries of these durations, you have to convert the time range object (which is a unique difftime class) into numeric. Thus:\n\nelk_df %>% group_by(ID) %>% \n  dplyr::summarize(time_range = difftime(max(Time), min(Time), units =\"days\") %>% \n                     as.numeric) %>% summary\n\n          ID      time_range    \n GR122     :1   Min.   : 226.9  \n GR133     :1   1st Qu.: 255.2  \n GR159     :1   Median : 297.8  \n GR513     :1   Mean   : 393.7  \n OR39      :1   3rd Qu.: 438.5  \n YL115_OR34:1   Max.   :1051.7  \n (Other)   :4                   \n\n\nThe median duration of monitoring is ~300 days, or a little bit less than a year. Some individual(s) have ~3 years of monitoring, and some only a few days.\nWe can visualize the monitoring duration for each individual, on a plot, by extracting the start date and the end date of the monitoring for each individual.\n\nn.summary <- elk_df %>% group_by(ID) %>% \n  summarize(start = min(Time), end = max(Time)) \n\nHere’s a version using ggplot2\n\nrequire(ggplot2)\nggplot(n.summary, aes(y = ID, xmin = start, xmax = end)) + \n    geom_linerange() \n\n\n\n\nIt may make more sense to sort the individuals not alphabetically, but by the time of release. Here’s an approach, which relies on reordering the factor levels of the ID column (an often fussy task):\n\nn.summary <- elk_df %>% group_by(ID) %>% \n  summarize(start = min(Time), end = max(Time)) %>% \n  arrange(start) %>% \n  mutate(ID = factor(ID, levels = as.character(ID)))\n  \nggplot(n.summary, aes(y = ID, xmin = start, xmax = end)) + \n    geom_linerange() \n\n\n\n\nThis is quick and easy and attractive enough. But, for the record, if you wanted to use base plotting functions (which, for many applications, can be much more easy to customize), code for a similar plot would look something like this:\n\nwith(n.summary, {\n  plot(start, ID, xlim = range(start, end), \n       type = \"n\", yaxt = \"n\", ylab = \"\", xlab = \"\")\n  segments(start, as.integer(ID), end, as.integer(ID), lwd = 2)\n  mtext(side = 2, at = 1:nrow(n.summary), ID, cex = 0.7, las = 1, line = .2)\n  })\n\n\n\n\nIn any case, on this figure each line represents the duration of the monitoring (x axis) for a given individual (y axis). While we see the beginning and end of the monitoring for each individual, we cannot see if there are any gaps in the monitoring.\nTo see if there is one or multiple gaps, we can create a vector of Date for each individual (i.e., containing only the date and not the time, to simplify the it and get only get one row per day per individual). To get the date from a time vector, we use the as.Date function. We then use the slice() command to keep only row per day for each individual. Do not forget to arrange per ID and date as all these manipulation can sometimes mess up the ordering of your data.\n\ndates <- elk_df %>% mutate(date = as.Date(Time)) %>% \n  group_by(ID, date) %>% \n  slice() %>% arrange(ID, date) \n\n\nggplot(dates, aes(y = ID, x = date)) + \n    geom_point(shape = 20, size = .5, alpha = .1) \n\n\n\n\nSimilar to the previous figure, each line represent the monitoring dates for a given individual. But from this figure, we can see that there are some gaps in the monitoring of some individuals.\n\n6.3.5 What is the fix rate?\nThe fix rate, or the time lag between successive locations, can be extracted by using the difftime() function on the Time column. Again, this function needs to be applied to each individual separately. Here, we are subsetting the data set per ID, and applying a function which is adding a column difftime to each subset. Note that since the vector of time difference is smaller than the vector of time, we add a missing value at the beginning of each vector, for each value to represent the difference in time to the previous location.\n\nelk_df <- elk_df %>% ddply(\"ID\", mutate, \n        dtime = c(NA, difftime(Time[-1], Time[-length(Time)], units = \"hours\")))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nTime\nsex\nLon\nLat\nX\nY\ndoy\nMonth\nYear\n\n\n\nGR122\n2006-03-06 16:22:41\nf\n-115.5828\n51.75451\n597820.5\n5734685\n65\n3\n2006\n\n\nGR122\n2006-03-06 19:01:07\nf\n-115.5830\n51.75455\n597806.6\n5734689\n65\n3\n2006\n\n\nGR122\n2006-03-06 23:01:48\nf\n-115.5865\n51.73794\n597600.9\n5732837\n65\n3\n2006\n\n\nGR122\n2006-03-07 03:01:46\nf\n-115.5774\n51.73691\n598231.4\n5732735\n66\n3\n2006\n\n\nGR122\n2006-03-07 11:01:46\nf\n-115.5828\n51.75442\n597820.7\n5734675\n66\n3\n2006\n\n\nGR122\n2006-03-07 15:01:05\nf\n-115.5828\n51.75447\n597820.6\n5734680\n66\n3\n2006\n\n\n\n\n\nWhat are the statistics (min, max, mean, median, …) of this fix rate?\n\nelk_df$dtime %>% summary\n\nLength  Class   Mode \n     0   NULL   NULL \n\n\nOn average, the fix rate is ~1 hour (median is 15 minutes). The smallest one is 1 second and the biggest is a little more than a year (10664 hours ~ 178 days). This one probably comes from an animal that has been captured and equipped once, then recaptured a year after the end of the monitoring. Understanding the sources of these gaps underscores the importance of having a relationship with those people that actually collected the data, to understand their monitoring strategy and structure of the data, and understand how to process the data depending on your research questions."
  },
  {
    "objectID": "movement_data_in_R.html#processing-data",
    "href": "movement_data_in_R.html#processing-data",
    "title": "\n6  Movement data in R\n",
    "section": "\n6.4 Processing data",
    "text": "6.4 Processing data\nThe previous section illustrated a few typical approaches to exploring a movement dataset. Processing the data - broadly speaking - implies that we will be organizing it, filtering it, or adding information to the data frame in ways that contributes in a meaningful way to some key research question. By that definition, for example, the inclusion of the individual specific fix rate above is a key bit of data processing.\nAs an example, we might investigate the following question: Is there a difference in movement rates between winter and summer, for female elk?\n\n6.4.1 Subsetting by season\nTo answer this question, we need to focus on movement data of female elk during winter and summer only. Let’s say (arbitrarily) that winter is only January and February, and summer is just July and August. We can use month() to subset the data accordingly.\n\nelk_winter_summer <- elk_df %>% mutate(Month = month(Time)) %>%\n  subset(Month %in% c(1,2,6,7))\n\nTo add a column “season”, we can use the ifelse(), function which returns different values depending on whether a given element in a vector satisfies a condition.\n\nelk_winter_summer <- elk_winter_summer %>% \n  mutate(season = ifelse(Month < 3, \"Winter\", \"Summer\"))\ntable(elk_winter_summer$season)\n\n\nSummer Winter \n 29959   2306 \n\n\nYou could also perform this operation with a vector of days of year and the useful cut() function, which transforms numeric data to ordered factors. Thus, to obtain breaks:\n\nseason.dates <- c(winter.start = yday(\"2023-01-01\"), \n                  winter.end = yday(\"2023-02-28\"),\n                  summer.start = yday(\"2023-07-01\"),\n                  summer.end = yday(\"2023-08-31\"))\nseason.dates\n\nwinter.start   winter.end summer.start   summer.end \n           1           59          182          243 \n\n\n\ncut.dates <- c(season.dates, 367)\nelk_winter_summer <- elk_df %>% \n  mutate(season = cut(yday(Time), cut.dates, labels = c(\"winter\",\"other\",\"summer\",\"other\"))) %>% \n  subset(season != \"other\") %>% droplevels\ntable(elk_winter_summer$season)\n\n\nwinter summer \n  2176  17212 \n\n\n\n6.4.2 Estimating movement rate\nTo estimate the movement rate between subsequent steps for each individual and each season, we will use what we learned in chapter Section 4.1.\n\nCreate a Z vector combining the X and Y coordinates\nCalculate the step lengths (SL)\nCalculate the time difference of the steps\nCalculate step’s movement rate\n\nAs we need to do this for each individual, year and season separately, we will use the ddply(), as before. Note, that to do this most effectively, it is nice to write our own function that makes all the computations we need. The key in this function is that the movement rate (MR) is the step length divided by the time difference, converted ti km/hour. Here’s one such function:\n\ngetMoveStats <- function(df){\n  # df - is a generic data frame that will contain X,Y and Time columns\n  Z <- df$X + 1i*df$Y\n  Time <- df$Time\n  Step <- c(NA, diff(Z)) # we add the extra NA because there is no step to the first location\n  dT <- c(NA, difftime(Time[-1], Time[-length(Time)], hours) %>% as.numeric)\n  \n  SL <- Mod(Step)/1e3 # convert to km - so the speed is in km/hour\n  MR <- SL/dT # computing the movement rate\n\n  # this is what the function returns\n  data.frame(df, Z, dT, Step, SL, MR)\n}\n\nWe took care to pick this function apart into individual pieces. And understand that it returns a new data frame with the additional columns appended to the original data frame. THe ddply command will apply this function to every individual in every season in every year. This is now very quick:\n\nelk_winter_summer <- elk_winter_summer %>% \n  plyr::ddply(c(\"ID\", \"Year\", \"season\"), getMoveStats)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nTime\nsex\nLon\nLat\nX\nY\ndoy\nMonth\nYear\nseason\nZ\ndT\nStep\nSL\nMR\n\n\n\nGR122\n2006-07-02 00:00:19\nf\n-115.5844\n51.68669\n597856.4\n5727141\n183\n7\n2006\nsummer\n597856+5727141i\nNA\nNA\nNA\nNA\n\n\nGR122\n2006-07-02 00:15:17\nf\n-115.5841\n51.68658\n597877.4\n5727129\n183\n7\n2006\nsummer\n597877+5727129i\n14.96667\n20.9744-11.830i\n0.0240806\n0.0016089\n\n\nGR122\n2006-07-02 00:30:16\nf\n-115.5836\n51.68594\n597913.3\n5727058\n183\n7\n2006\nsummer\n597913+5727058i\n14.98333\n35.9429-70.504i\n0.0791373\n0.0052817\n\n\nGR122\n2006-07-02 00:45:16\nf\n-115.5828\n51.68509\n597970.4\n5726965\n183\n7\n2006\nsummer\n597970+5726965i\n15.00000\n57.1349-93.454i\n0.1095356\n0.0073024\n\n\nGR122\n2006-07-02 01:00:48\nf\n-115.5822\n51.68451\n598013.2\n5726901\n183\n7\n2006\nsummer\n598013+5726901i\n15.53333\n42.7284-63.696i\n0.0767000\n0.0049378\n\n\nGR122\n2006-07-02 02:00:28\nf\n-115.5820\n51.68433\n598027.4\n5726881\n183\n7\n2006\nsummer\n598027+5726881i\n59.66667\n14.2142-19.749i\n0.0243324\n0.0004078\n\n\n\n\n\n\n6.4.3 Quick analysis of movement rates\nWe are ready now (finally) to answer the question: Is there a difference in movement rate between winter and summer? We can start with a quick boxplot of the movement rates against individual ID’s and season. The distributions are highly skewed and quite variable, with some animals really on the move, and some spending a lot of time not moving at all.\n\nggplot(elk_winter_summer, aes(ID, MR)) + geom_boxplot() + facet_wrap(.~season)\n\n\n\n\nSome season & individual summary stats:\n\nmr_summarystats <- elk_winter_summer %>% ddply(c(\"ID\",\"season\"), summarize,\n  min = min(MR, na.rm = TRUE), max = max(MR, na.rm = TRUE),\n  n = length(MR), NA.count = sum(is.na(MR)),\n  Zero.count = sum(MR == 0, na.rm = TRUE))\n\n\nmr_summarystats %>% kable\n\n\n\nID\nseason\nmin\nmax\nn\nNA.count\nZero.count\n\n\n\nGR122\nsummer\n0.0000000\n0.0827437\n3806\n1\n96\n\n\nGR133\nwinter\n0.0000000\n0.0777082\n314\n1\n10\n\n\nGR133\nsummer\n0.0000000\n0.0871294\n3782\n1\n44\n\n\nGR159\nsummer\n0.0000000\n0.1078212\n3012\n1\n21\n\n\nGR513\nwinter\n0.0000000\n0.6543712\n677\n2\n10\n\n\nGR513\nsummer\n0.0000093\n0.0271023\n711\n1\n0\n\n\nOR39\nwinter\n0.0000000\n0.5450862\n35\n1\n1\n\n\nOR39\nsummer\n0.0005555\n1.8531111\n572\n1\n0\n\n\nYL115_OR34\nwinter\n0.0000000\n0.8679974\n270\n3\n1\n\n\nYL115_OR34\nsummer\n0.0000000\n2.6096342\n1279\n2\n2\n\n\nYL121_BL250\nwinter\n0.0000000\n1.6492821\n539\n1\n1\n\n\nYL121_BL250\nsummer\n0.0011136\n2.7874279\n541\n1\n0\n\n\nYL5\nwinter\n0.0000000\n0.0761834\n251\n1\n3\n\n\nYL5\nsummer\n0.0000183\n0.0480011\n515\n1\n0\n\n\nYL77\nwinter\n0.0005551\n1.3129208\n90\n1\n0\n\n\nYL77\nsummer\n0.0011128\n2.0078617\n622\n1\n0\n\n\nYL89\nsummer\n0.0000000\n0.1279309\n2372\n1\n14\n\n\n\n\n\nNote there are lot of 0’s in the data, which is rather remarkable considering the relative imprecision of GPS data - certainly high enough to show variation at the 1 meter scale. This is an evident  red flag  … one might be inclined to simply remove those data entirely.\nWithout going into too much detail of the statistical analysis of these data, the most appropriate statistical test would use a linear mixed effects model with individuals as random effects, and look something like this:\n\nfit <- lme4::lmer(log(MR) ~ season + (1 + season|ID), \n            data = elk_winter_summer %>% subset(MR != 0 & !is.na(MR)))\n\nPerhaps surprisingly, movement rates are slightly lower in the summer - but not with meaningful statistical significance (|\\(t\\) value| < 1).\nNote, the residuals are quite well-behaved with the log-transformation.\n\nplot(fit)"
  },
  {
    "objectID": "movement_data_in_R.html#conclusions",
    "href": "movement_data_in_R.html#conclusions",
    "title": "\n6  Movement data in R\n",
    "section": "\n6.5 Conclusions",
    "text": "6.5 Conclusions"
  },
  {
    "objectID": "movement_data_in_R.html#references",
    "href": "movement_data_in_R.html#references",
    "title": "\n6  Movement data in R\n",
    "section": "\n6.6 References",
    "text": "6.6 References\n\n\n\n\nHebblewhite, Mark, Evelyn H. Merrill, Hans Martin, Jodi E. Berg, Holger Bohm, and Scott L. Eggeman. 2020. “Data from: Study \"Ya Ha Tinda Elk Project, Banff National Park, 2001-2020 (Females)\".” https://doi.org/10.5441/001/1.5G4H5T6C."
  },
  {
    "objectID": "likelihoods.html#likelihoods-are",
    "href": "likelihoods.html#likelihoods-are",
    "title": "\n8  Likelihood Theory\n",
    "section": "\n8.1 Likelihoods are:",
    "text": "8.1 Likelihoods are:\n\nthe link between: Data -> Probability Models -> Inference\nthe most important tool, in practice, for fitting models … i.e. when we estimate parameters we like to Maximize the Likelihood,\nNote: there are some theoretical reasons for this - MLE’s are proven to be CONSISTENT and have the LOWEST VARIANCE of any other estimator*\nprovide a flexible / general framework for comparing / selecting models, via Likelihood Ratio Tests, Information Criteria (AIC, BIC, etc.)\n\nAnd, very strangely:\n\nThey are almost NEVER TAUGHT in undergraduate statistics classes!\nThough they are EASY TO UNDERSTAND.\n\n\nWe are basically peeking under the hood of a lot of statistical machinery.\n\nIn short:\n\nLikelihoods turn Probabilities on their heads!\n\n8.1.1 Probability statements\nLet’s say the average height of a human male is \\[X \\sim {\\cal N}(\\mu_0 = 6, \\sigma_0 = 0.5)\\] This is a probability model, i.e. it tells us that: \\[P(x < X < x+dx) = f(x|\\mu, \\sigma)dx\\] where \\(f(x|\\mu, \\sigma)\\), is the density function of the normal distribution with \\(\\mu\\) and \\(\\sigma\\).\nThis probability statement means: If you take (for example) a single 7 foot tall person, you can say with certainty that:\n\nthe probability that a person is exactly 7 feet tall is exactly 0\nbut the per-foot “probability” that he’s around 7 feet tall is\n\n\\[f(7|\\mu=6, \\sigma=0.5) = 0.11  1/foot\\]\n\ndnorm(7,6,.5)\n\n[1] 0.1079819\n\n\nBut the per-foot “probability” that he’s around 7 feet tall is\n\\[f(7|\\mu=6, \\sigma=0.5) = 0.11  1/foot\\]\n\ndnorm(7,6,.5)\n\n[1] 0.1079819\n\n\nWhats a “1/foot”? Convert it to “per inch”:\n\ndnorm(7,6,.5)/12\n\n[1] 0.008998494\n\n\nJust under 1% probability that a person is within one inch of 7 feet.\n\n8.1.2 Defining a Likelihood\nSay, you don’t know what those parameters are, but you are curious! So you collect one data point: a single man, and he is 7 feet tall.\n\\[ X_1 = \\{7\\}\\]\nTo “flip this on its head”“, we ask a new question: What is the likelihood that the mean and standard deviation of human heights are 6 and 0.5, GIVEN that we observed a man who is \\(X_1 = 7\\) feet tall?\nWe write this as: \\[ {\\cal L}(\\mu, \\sigma | x).\\]\nIt is “flipped on its head” because it as a function of the parameters given an observation, rather than as the probability of an observation given some parameters. But, by definition, the likelihood is numerically equal to the probability:\n\\[L_0 = {\\cal L}(\\mu_0, \\sigma_0 | X_1) = f(X_1|\\mu_0, \\sigma_0) = 0.11\\]z\nIn colloquial language, a “likelihood” is sort of similar to a “probability” - i.e. the statement: “Event A is likely.” seems to have similar meaning to the statement: “Event A has high probability.”\nIn statistics, the “likelihood” and the “probability” are, in fact EQUAL - but there is an inversion of what is considered “known” and “unknown: - A probability tells you something about a random event given parameter values.\n- The likelihood tells you something about parameter values given observations.\n In practice - statistical inference is about having the data and guessing the parameters. Thus the concept of Likelihoods is extremely useful and natural. \nIn contrast to Probabilities, the actual raw value of the likelihood is almost NEVER of interest - We ONLY care about its value when compared with likelihoods of different models.\nfor example, we can compare the likelihood of the parameters \\(\\mu=6\\) and \\(\\sigma=0.5\\), GIVEN the observation \\(X_1 = 7\\), with an alternative probability model … say, \\(\\mu_1 = 7\\) and \\(\\sigma_1 = 0.001\\). That likelihood is given by:\n\\[ L_1 = {\\cal L}(\\mu_1, \\sigma_1 | X_1 = 7) = f(7 | 7, .001)\\]\n\n(L1 <- dnorm(7,7,.001))\n\n[1] 398.9423\n\n\n\\(L_1\\) is, clearly, much much greater than our original likelihood, i.e. this set of parameters is much likelier than the original set of parameters. Indeed, the ratio \\(L_1 / L_0 = 3693\\) ….\n\nso we can say that model 2 is more than 3000x more likely than Model 1!\n\n(Of course, the likelihood ratio test for these two likelihoods has zero power, because we only have one data point. But we can still quantify their relative likelihoods.)\n\n8.1.3 Joint Likelihoods\n\nLet’s sample 5 individuals from a professional basketball arena, and record their heights: 6.6, 6.8, 7.0, 7.2, 7.4 feet.\nAt this point, I might have reasonable suspicion that our null-model might not be appropriate for this subset of humans.\nVisualize these data-points against our null-model:\n\nmu0 <- 6\nsigma0 <- 0.5\nX <- c(6.6,6.8,7.0,7.2,7.4)\ncurve(dnorm(x, mu0, sigma0), xlim=c(4,8))\npoints(X, dnorm(X,mu0,sigma0), pch=19, col=2)\npoints(X, dnorm(X,mu0,sigma0), type=\"h\", col=2)\n\n\n\n\nNow, we can compute the likelihood of the null parameters given all of these observations.The likelihood (a joint likelihood) is just the product of the likelihood for each of these points, because it is equal to the joint density distribution … this is because the Probability of \\(n\\) independent events is the product of the probabilities:\n\\[{\\cal L}(\\mu_0, \\sigma_0 | {\\bf X}) = \\prod_{i=1}^n f(X_i|\\mu_0, \\sigma_0) \\]\n\n(L0 <- prod(dnorm(X, mu0, sigma0)))\n\n[1] 6.596596e-06\n\n\nThis is a very small number, but again - it is meaningless without having a comparison. Let’s compute the joint likelihood of our alternative model:\n\nmu1 <- 7\nsigma1 <- 0.001\n(L1 <- prod(dnorm(X, mu1, sigma1)))\n\n[1] 0\n\n\nTo machine power, the second model is MUCH LESS likely than the first model! To illustrate this:\n\ncurve(dnorm(x, mu1, sigma1), xlim=c(6.2,7.8), n=1000)\npoints(X, dnorm(X,mu1,sigma1), pch=19, col=2)\npoints(X, dnorm(X,mu1,sigma1), type=\"h\", col=2)\n\n\n\n\nYou see that the points away from 7 have extremely low probability, which brings the likelihood of this model way down."
  },
  {
    "objectID": "likelihoods.html#the-maximum-likelihood-estimator",
    "href": "likelihoods.html#the-maximum-likelihood-estimator",
    "title": "\n8  Likelihood Theory\n",
    "section": "\n8.2 The Maximum Likelihood Estimator\n",
    "text": "8.2 The Maximum Likelihood Estimator\n\nSo - how do we find the parameters that maximize the likelihood?\nThese parameters are called the Maximum Likelihood Estimators (MLE’s), and are the “best” parameters in that they are the most precise. Remember, every estimate is a random variable and therefore comes with some variance. It can be shown that MLE’s have the smallest of those possible variances:\n\\[  \\{ \\widehat\\theta_\\mathrm{mle}\\} \\subseteq \\{ \\underset{\\theta\\in\\Theta}{\\operatorname{arg\\,max}}\\ {\\cal L}(\\theta\\,|\\,X_1,\\ldots,X_n) \\}\\]\nTranslating to English:\n the MLE estimators of parameters \\(\\theta\\) are those values of \\(\\theta\\) for which the likelihood function is maximized."
  },
  {
    "objectID": "likelihoods.html#the-likelihood-profile",
    "href": "likelihoods.html#the-likelihood-profile",
    "title": "\n8  Likelihood Theory\n",
    "section": "\n8.3 The Likelihood profile",
    "text": "8.3 The Likelihood profile\nLet’s look at a range of possible values for \\(\\widehat{\\mu}\\) and \\(\\widehat{\\sigma}\\). We can do this pretty efficiently in R:\nFirst, pick some values to explore:\n\nmus <- seq(6,8,.05)\nsigmas <- seq(.1,1,.02)\n\nNow, write a function that computes the likelihood (which, as we recall, is a function of \\(\\mu\\) and \\(\\sigma\\), and “assumes” \\(\\bf X\\))\n\nLikelihood <- function(mu, sigma)\n  prod(dnorm(X, mu, sigma))\n\nAnd compute this likelihood for all the combinations of \\(\\mu\\) and \\(\\sigma\\) above, using the mighty outer() function:\n\nL.matrix <- outer(mus, sigmas, Vectorize(Likelihood))\n\nNote (as a technical aside) that to get the Likelihood() function to work within outer(), it had to be “vectorized”. Happily, there is a function (Vectorize()) that does just that.\nWe can visualize our likelihood profile over those ranges of \\(\\mu\\) and \\(\\sigma\\)\n\n\n\n\n\nClearly, there is a sharp peak in the likelihood around \\(\\widehat{\\mu} = 7\\) and somewhere just under \\(\\widehat{\\sigma} = 0.3\\). From our limited sets of values, we can find the values at the maximum:\n\n(max.indices <- which(L.matrix == max(L.matrix), arr.ind = TRUE))\n\n     row col\n[1,]  21  10\n\n(mu.hat <- mus[max.indices[1]])\n\n[1] 7\n\n(sigma.hat <- sigmas[max.indices[2]])\n\n[1] 0.28\n\n\nAnd the (usually irrelevant) value of the likelihood is\n\nmax(L.matrix)\n\n[1] 0.4580006\n\n\n\n8.3.1 Numerically finding the MLE\nWe don’t need to do this search ourselves - that’s why we invented electronic computing devices. The powerhouse function for optimizing functions in R is optim(), but it takes some getting used to. The (minimal) syntax is:\n\noptim(p, FUN, ...)\n\nwhere:\n\n\np is a required vector of your initial guess for the parameters.\n\nFUN(p, ...) is a function that takes as its first argument a vector of parameters p\n\nThe ... refers to other arguments passed to FUN. Most importantly, this will be data!\n\n\nSee how I make it work below:\n\noptim(c(6,1), function(p) -Likelihood(p[1], p[2]))\n\n$par\n[1] 7.0000003 0.2828426\n\n$value\n[1] -0.4582359\n\n$counts\nfunction gradient \n      89       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\nSo - lots of output, but the key numbers we’re interested in are the top two, under $par. These are: \\(\\widehat{\\mu} = 7.000\\) and \\(\\widehat{\\sigma} = 0.2828\\). Good! it is what we expected.\nThe $value is very close to the (negative of the) maximum likelihood that we “hand-calculated”.\n\n8.3.2 Comparing models side by side\n\nPlotLikelihood <- function(mu, sigma, X, ...)\n{curve(dnorm(x, mu, sigma), ...)\npoints(X, dnorm(X,mu,sigma), pch=19, col=2)\npoints(X, dnorm(X,mu,sigma), type=\"h\", col=2)}\n\npar(mfrow=c(1,3), bty=\"l\")\n\nPlotLikelihood(mu = 6, sigma=0.5, X, xlim=c(4,8))\ntitle(\"Null Model\")\n\nPlotLikelihood(mu = 7, sigma=0.001, X, xlim=c(6.2,7.8), n = 1000)\ntitle(\"Model 1\")\n\npars <- optim(c(6,1), function(p) -Likelihood(p[1], p[2]))$par\nPlotLikelihood(mu = pars[1], sigma=pars[2], X, xlim=c(5,9))\ntitle(\"MLE Model\")\n\n\n\n\n\n\n\nYou can get a feel for the constraints on the MLE model: if \\(\\sigma\\) were smaller, the contribution of the outlying points would become much smaller and bring the likelihood down; if \\(\\sigma\\) were any larger, then the flattening would bring down too far the contribution of the central points.\n\n8.3.3 Comparing with Method of Moment Estimators (MME’s)\nNow … you might think that’s a whole lot of crazy to estimate a simple MEAN and VARIANCE.\nYou may have guessed that the best estimates are the sample mean:\n\\[\\widehat{\\mu} = \\overline{X}\\] and the sample standard deviation:\n\\[\\widehat{\\sigma^2} = s_x^2 = {1\\over n-1}\\sum (X_i - \\overline{X})^2\\]\nThe mean estimate matches, but the sample standard deviation doesn’t quite:\n\nsd(X)\n\n[1] 0.3162278\n\n\nThis suggests that the MLE of the variance - for all of its great qualities - is at least somewhat biased (remember: \\(E(s_x^2) = \\sigma^2\\)).\n Can you recognize what analytical formula gives the MLE of \\(\\sigma\\)? \n\nNote: this is a special case where the MLE is actually computable by hand, but in general it is not - and best to obtain numerically, as the examples that follow show.\n\n\n8.3.4 Log-likelihoods\nFor a combination of practical and theoretical reasons, what we actually maximize is not the Likelihood but the log-Likelihood. The maximum will be in the same place for both functions, but the latter is MUCH easier to work with.\nLikelihood: \\[{\\cal L}(\\mu_0, \\sigma_0 | {\\bf X}) = \\prod_{i=1}^n f(X_i|\\mu_0, \\sigma_0) \\]\nLog-likelihood: \\[{\\cal l}(\\mu_0, \\sigma_0 | {\\bf X}) = \\log({\\cal L}) = \\sum_{i=1}^n \\log(f(X_i|\\mu_0, \\sigma_0)) \\]\nSums are much easier to manipulate with algebra and handle computaitonally. Also, they help turn very very very small numbers and very very very large numbers to very very very ordinary numbers.\n\nThe number of particles in the universe is about 10^80 … its log is 184. The ratio of the mass of an electron to the size of the sun is 10^-60 … its log is -134.\n\n\nL.matrix <- outer(mus, sigmas, Vectorize(Likelihood))\nimage(mus, sigmas, L.matrix, col=topo.colors(1000))\ncontour(mus, sigmas, L.matrix, add=TRUE, col=\"white\")\n\n\n\n\n\nll.matrix <- outer(mus, sigmas, Vectorize(logLikelihood))\nimage(mus, sigmas, ll.matrix, col=topo.colors(1000))\ncontour(mus, sigmas, ll.matrix, add=TRUE, nlevels=100, col=rgb(0,0,0,.2))\n\n\n\n\n\n8.3.5 Confidence Intervals: A bit of theory\nThe peak of the (log)-likelihood surface gives you point estimates of parameters.\nLikelihood theory provides an additional enormously handy (asymptotically correct) result with respect to standard errors around the estimates. Specifically (in words): The variance around the point estimates is equal to the negative reciprocal of the second derivative of the log-likelihood at the maximum.\nThis is actually very intuitive! The sharper the peak, the MORE NEGATIVE the second derivative, the SMALLER the (POSITIVE) variance. The flatter the peak, the LESS NEGATIVE the second derivative, i.e. the LARGER the variance.\n\\[\\Sigma(\\theta) = {\\cal I}(\\theta)^{-1}\\]\nOften (e.g. in these examples) we have several parameters here, so the jargon is fancier, but the idea is the same:\n\nthe Hessian is an n-dimensional second derivative (a matrix)\nthe Fisher Information (\\(\\cal{I}\\)) is the Hessian of the log likelihood.\nthe inverse is the n-dimensional equivalent of “reciprocal”.\n\\(\\Sigma\\) is the variance-covariance matrix of the parameter estimates.\n\n8.3.6 Confidence Intervals: Application\nAsk optim() to compute the Hessian:\n\nlogLikelihood <- function(mu, sigma)\n  sum(log(dnorm(X, mu, sigma)))\n(param.fit <- optim(c(1,1),  function(p) -logLikelihood(p[1], p[2]), hessian=TRUE))\n\n$par\n[1] 6.9999642 0.2828735\n\n$value\n[1] 0.7803712\n\n$counts\nfunction gradient \n      97       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n            [,1]         [,2]\n[1,] 62.48637509   0.01580303\n[2,]  0.01580303 124.94594137\n\n\nThe inverse of a matrix (in R) is solve(M):\n\n(Sigma <- solve(param.fit$hessian))\n\n              [,1]          [,2]\n[1,]  1.600349e-02 -2.024104e-06\n[2,] -2.024104e-06  8.003462e-03\n\n\nThe square root of the diagonal gives standard errors\n\n(se <- sqrt(diag(Sigma)))\n\n[1] 0.12650490 0.08946207\n\n\nAnd the confidence intervals are just:\n\ncbind(hat = param.fit$par, CI.low = param.fit$par  - 1.96*se, CI.high = param.fit$par  + 1.96*se)\n\n           hat    CI.low   CI.high\n[1,] 6.9999642 6.7520146 7.2479138\n[2,] 0.2828735 0.1075279 0.4582192\n\n\n\noptim: Is the engine under a great many hoods of R functions!\n\n\nJohn Nash: creator (and skeptic) of optim\nsee: http://www.ibm.com/developerworks/library/ba-optimR-john-nash/"
  },
  {
    "objectID": "likelihoods.html#testing-hypotheses-and-model-selection-with-likelihoods",
    "href": "likelihoods.html#testing-hypotheses-and-model-selection-with-likelihoods",
    "title": "\n8  Likelihood Theory\n",
    "section": "\n8.4 Testing hypotheses and model selection with likelihoods",
    "text": "8.4 Testing hypotheses and model selection with likelihoods\n\n8.4.1 Likelihood Ratio Test:\n\nModel 0 and Model 1 are NESTED\n(i.e. Model 0 is a special case of Model 1) with \\(k_0\\) and \\(k_1\\) parameters.\nCompute MLE’s: \\(\\widehat{\\theta_0}\\) and \\(\\widehat{\\theta_1}\\)\nCompute likelihoods: \\({\\cal L_0(\\theta_0|X)}\\) and \\({\\cal L_1(\\theta_1|X)}\\)\nimportant: the data \\(X\\) must be identical!\nLikelihood Ratio Test Statistic: \\[\\Lambda = -2 \\log \\left( \\frac{L_0}{L_1}  \\right) = 2 (l_1 - l_0)\\]\nunder Null hypothesis (i.e. Model 1) has distribution \\(\\Lambda \\sim \\text{Chi-squared} (d.f. = k_1 - k_0)\\)\n\nAn example:\n\n\n\n\n\nCompeting models: \\[M0: Y_i = \\beta_0 + \\epsilon_i\\] \\[M1: Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\] where \\(\\epsilon\\) are iid Gaussian.\nDefining (negative) log-likelihood functions\n\nLL0 <- function(par = c(beta0, sigma), X, Y){\n  -sum(dnorm(Y, mean = par['beta0'], sd = par['sigma'], log=TRUE) )\n}\nLL1 <- function(par = c(beta0, beta1, sigma), X, Y){\n  Y.hat <- par['beta0'] + par['beta1']*X\n  -sum(dnorm(Y, mean = Y.hat, sd = par['sigma'], log=TRUE) )\n}\n\nObtaining estimates\n\nfit0 <- optim(c(beta0=0, sigma=1), LL0, X = X, Y = Y)\nfit1 <- optim(c(beta0 = 0, beta1 = 0, sigma = 1), LL1, X = X, Y = Y)\n\nPerforming test:\n\nLRT <- 2*(-fit1$value + fit0$value)\n1-pchisq(LRT, 1)\n\n[1] 0.1083316\n\n\n\n8.4.2 Information Criteria\nIf models are not nested (most interesting sets of models aren’t) we can’t use a likelihood ratio test. Instead, we use the very very widely applied Akaike Information Criterion (AIC):\\[ AIC =  - 2 \\log(\\cal L) + 2 k\\]\n\nc(AIC0  = 2*(fit0$value + 2*2),\n  AIC1  = 2*(fit1$value + 2*3))\n\n    AIC0     AIC1 \n79.56228 80.98389 \n\n\nOr … Bayesian Information Criterion (BIC) \\[ BIC =  - 2 \\log(\\cal L) + k \\log(n) \\]\n\nc(BIC0  = 2*(fit0$value + 2 * log(length(X))),\n  BIC1  = 2*(fit1$value + 3 * log(length(X))))\n\n    BIC0     BIC1 \n83.54521 86.95828 \n\n\nWhich? Why? - a complicated debate, different underlying assumptions But generally - if you want to be more parsimonious (i.e. protect from overfitting) BIC is a better bet.\n\n8.4.3 In summary\nIf you have data and you can write a probability model in terms of parameters, no matter how strange or arbitrary seeming, there’s a good chance you can:\n\n\nestimate those parameters,\n\n\ncompare competing models,\nand probably obtain confidence intervals."
  },
  {
    "objectID": "likelihoods.html#panda-pregnancy-test-example",
    "href": "likelihoods.html#panda-pregnancy-test-example",
    "title": "\n8  Likelihood Theory\n",
    "section": "\n8.5 Panda pregnancy test example",
    "text": "8.5 Panda pregnancy test example\n\n\nLikelihood ratio test = pregnancy test\n\n\\(p\\)-value of birth: 0.08\n\n\\(p\\)-value of birth: 1e-11"
  },
  {
    "objectID": "likelihoods.html#applying-likelihoods-to-movement-data",
    "href": "likelihoods.html#applying-likelihoods-to-movement-data",
    "title": "\n8  Likelihood Theory\n",
    "section": "\n8.6 Applying Likelihoods to movement data",
    "text": "8.6 Applying Likelihoods to movement data\nWe have seen that movement data often has:\n\nSkewed, Positive, step Length Distribution\nWrapped turning angle distributions, clustered around 0^o\n\n\n\n\nThis combination is known as the Correlated Random Walk (CRW) model, probably the most commonly used basic movement model in ecology.\n\n\n\n\n8.6.1 Correlated Random Walk\nTypical step-length model - the Weibull distribution: \\[f(x;\\alpha, \\beta) = \\frac{\\alpha}{\\beta}\\left(\\frac{x}{\\beta}\\right)^{\\alpha-1}e^{-(x/\\beta)^{k}}\\] \\(\\alpha\\) and \\(\\beta\\) are the shape and scale parameter, respectively.\n\ncurve(dweibull(x, 1, 2), xlim=c(0,6), ylim=c(0,1.2), ylab=\"density\", xlab=\"\", col=2)\ncurve(dweibull(x, 2, 2), add=TRUE, col=3)\ncurve(dweibull(x, 6, 2), add=TRUE, col=4)\nlegend(\"topright\", col=2:4, lwd=2, legend=c(1,2,6), title=\"Shape parameter\", bty=\"n\", cex=1.5)\n\n\n\n\nTypical turning angle model - wrapped Cauchy distribution: \\[f(\\theta;\\mu,\\kappa)=\\frac{1}{2\\pi}\\,\\,\\frac{\\sinh\\kappa}{\\cosh\\kappa-\\cos(\\theta-\\mu)}\\] \\(\\mu\\) is the mean angle (usually 0) and \\(\\kappa\\) is clustering parameter- equal to \\(E(cos(\\theta))\\).\n\nrequire(CircStats) # NOT in base!\ncurve(dwrpcauchy(x, 0, 0), xlim=c(-pi,pi), ylim=c(0,1.5), lwd=2, ylab=\"density\", xlab=\"\", col=2)\ncurve(dwrpcauchy(x, 0, 0.5),  add=TRUE, col=3)\ncurve(dwrpcauchy(x, 0, 0.8),add=TRUE, col=4, n=1001)\nlegend(\"topright\", col=2:4, legend=c(0,0.5,0.8), title=\"Shape parameter\", bty=\"n\", cex=1.5, lwd=2)\n\n\n\n\n\n8.6.2 MLE of Weibull parameters\nHere’s some sample data:\n\nY <- cumsum(arima.sim(n=100, model=list(ar=.7)))\nZ <- X + 1i*Y\nplot(Z, type=\"o\", asp=1)\n\n\n\n\nA function that returns the likelihood as a function of the parameters and data\n\nWeibull.Like <- function(p, Z)\n{ \n  S = Mod(diff(Z))\n  -sum(dweibull(S, p[1], p[2], log=TRUE))\n}\n\n\nNote 1: we use log() because it is much easier to SUM LOGS than it is to MULTIPLY SMALL NUMBERS.\n\n\nNote 2: we return the NEGATIVE of the likelihood because the optim() function likes to MINIMIZE rather than MAXIMIZE.\n\nRun the optimization:\n\n(Weibull.fit <- optim(c(1,1), Weibull.Like, Z=Z))\n\n$par\n[1] 2.020257 1.926387\n\n$value\n[1] 124.6168\n\n$counts\nfunction gradient \n      63       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\nVisually assess the fit:\n\nhist(Mod(diff(Z)), freq=FALSE, col=\"grey\", breaks=10)\ncurve(dweibull(x, Weibull.fit$par[1], Weibull.fit$par[2]), add=TRUE, col=2, lwd=2)\n\n\n\n\nNot Bad!"
  },
  {
    "objectID": "likelihoods.html#exercise-1",
    "href": "likelihoods.html#exercise-1",
    "title": "\n8  Likelihood Theory\n",
    "section": "\n8.7 Exercise 1",
    "text": "8.7 Exercise 1\nUse optim() to estimate the \\(\\kappa\\) parameter in the Wrapped cauchy distribution Visualize the quality of the fits by comparing distribution curves to histograms.\nFollow the template for the Weibull:\n\nWeibull.Like <- function(p, Z)\n{ \n  S = Mod(diff(Z))\n  -sum(dweibull(S, p[1], p[2], log=TRUE))\n}\n(Weibull.fit <- optim(c(1,1), Weibull.Like, Z=Z))\n\n\n\n\n8.7.1 Exercise 2\nFollowing the template below:\n\nEstimate the Weibull step-length and wrapped Cauchy turning angle parameters for your movement data of choice.\nVisualize the quality of the fits by comparing distribution curves to histograms.\nExtra Credit: Compute standard errors around those estimates.\n\n For discussion: What do YOU think about the CRW model?  \n\nWeibull.fit <- optim(c(1,1), Weibull.Like, Z=Z, hessian=TRUE)\nSigma <- solve(Weibull.fit$hessian)\nse <- sqrt(diag(Sigma))\ncbind(hat = Weibull.fit$par, CI.low = Weibull.fit$par  - 1.96*se, CI.high = Weibull.fit$par  + 1.96*se)"
  },
  {
    "objectID": "dependent_data.html",
    "href": "dependent_data.html",
    "title": "\n9  Dependent Data\n",
    "section": "",
    "text": "time series, autocorrelated data\n\nNB talks about time warps\n\n\nspatial correlation\ngls - nlme"
  },
  {
    "objectID": "discrete_movement_models.html#simple-random-walk",
    "href": "discrete_movement_models.html#simple-random-walk",
    "title": "\n10  Discrete time movement models\n",
    "section": "\n10.1 Simple random walk",
    "text": "10.1 Simple random walk"
  },
  {
    "objectID": "discrete_movement_models.html#correlated-random-walk",
    "href": "discrete_movement_models.html#correlated-random-walk",
    "title": "\n10  Discrete time movement models\n",
    "section": "\n10.2 Correlated random walk",
    "text": "10.2 Correlated random walk\nStep-length distributions\nTurning angle distributions"
  },
  {
    "objectID": "discrete_movement_models.html#biased-crw",
    "href": "discrete_movement_models.html#biased-crw",
    "title": "\n10  Discrete time movement models\n",
    "section": "\n10.3 Biased CRW",
    "text": "10.3 Biased CRW"
  },
  {
    "objectID": "home_ranges.html#mcp",
    "href": "home_ranges.html#mcp",
    "title": "\n12  Home ranges\n",
    "section": "\n12.1 MCP",
    "text": "12.1 MCP"
  },
  {
    "objectID": "home_ranges.html#kernel",
    "href": "home_ranges.html#kernel",
    "title": "\n12  Home ranges\n",
    "section": "\n12.2 kernel",
    "text": "12.2 kernel"
  },
  {
    "objectID": "home_ranges.html#locoh",
    "href": "home_ranges.html#locoh",
    "title": "\n12  Home ranges\n",
    "section": "\n12.3 LoCoH",
    "text": "12.3 LoCoH"
  },
  {
    "objectID": "home_ranges.html#akde",
    "href": "home_ranges.html#akde",
    "title": "\n12  Home ranges\n",
    "section": "\n12.4 akde",
    "text": "12.4 akde"
  },
  {
    "objectID": "behavioral_changes.html#change-points",
    "href": "behavioral_changes.html#change-points",
    "title": "\n13  Behavioral Changes\n",
    "section": "\n13.1 Change points",
    "text": "13.1 Change points"
  },
  {
    "objectID": "behavioral_changes.html#segmentation",
    "href": "behavioral_changes.html#segmentation",
    "title": "\n13  Behavioral Changes\n",
    "section": "\n13.2 Segmentation",
    "text": "13.2 Segmentation"
  },
  {
    "objectID": "behavioral_changes.html#state-space-models",
    "href": "behavioral_changes.html#state-space-models",
    "title": "\n13  Behavioral Changes\n",
    "section": "\n13.3 State-space models",
    "text": "13.3 State-space models"
  },
  {
    "objectID": "behavioral_changes.html#migration-dispersal",
    "href": "behavioral_changes.html#migration-dispersal",
    "title": "\n13  Behavioral Changes\n",
    "section": "\n13.4 Migration & dispersal",
    "text": "13.4 Migration & dispersal"
  },
  {
    "objectID": "hmms.html#what-are-ssms-and-hmms",
    "href": "hmms.html#what-are-ssms-and-hmms",
    "title": "\n14  Hidden Markov and State-Space Modeling\n",
    "section": "\n14.1 What are SSM’s and HMM’s?",
    "text": "14.1 What are SSM’s and HMM’s?"
  },
  {
    "objectID": "hmms.html#implementation-with-stan",
    "href": "hmms.html#implementation-with-stan",
    "title": "\n14  Hidden Markov and State-Space Modeling\n",
    "section": "\n14.2 Implementation with STAN",
    "text": "14.2 Implementation with STAN"
  }
]