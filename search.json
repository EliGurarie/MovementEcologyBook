[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Techniques and Concepts in Movement Ecology",
    "section": "",
    "text": "Preface\nThis is a living (and collaborative) document which primarily serves as course materials for EFB 796: Techniques and Concepts in Spatial and Movement Ecology - a graduate-level course I am currently (Spring 2023) teaching in the Dep’t of Environmental Biology at SUNY-ESF, together with some able assistance and collaboration. But it is also being developed - perhaps presumptuously - as the beginnings of a book. It is, of course, far more typical for people who embark on putting together a “book” to first teach a course for several years (or decades!) before making the leap into a text. But the smoothness of generating these materials in an organized way makes the line between course materials and a book somewhat blurred1.1 Big shout out to the team behind Quarto, and - in particular - for the template we baldly borrowed in R for Data Science (2e), for blurring these lines in the best possible way.\nLectures will gradually be modified into something resemble “chapters”, the structure might change, more materials might flow in or out, references will be added, some proof-reading might occur, and there may be some more ephemeral course specific materials popping up here and there. It will (at least throughout the duration of the semester) be a chaotic and dynamic place. There will be incomplete sentences and incomplete thoughts and tremendous gaps. But the goal of this effort is, really, to put together the modern digital equivalent of a book that serves as a single, organized source for mathematical, statistical and computational techniques, complete with examples, for the modeling and analysis of movement (and some other kinds of spatial) data.\n\nNote: This is an open project! Any suggestions, corrections, additions, materials, feedback, please do email me.\n\nThe authors: E. Gurarie, N. Barbour, O. Couriot, others."
  },
  {
    "objectID": "introductions.html#to-the-course",
    "href": "introductions.html#to-the-course",
    "title": "1  Introduction(s)",
    "section": "1.1 … to the Course",
    "text": "1.1 … to the Course\nMovement is an ubiquitous and fundamental property of all biological organisms. In this course/book, we will learn to model and analyze spatial patterns and animal movement data. Starting with a strong statistical foundation on time-series and spatial processes, we will cover topics such as point patterns, migration, dispersal, home-ranging, behavioral change point analysis, multi-state modeling, continuous-time movement models, and perhaps touch on cutting edge topics like cognitive movement ecology and collective movements. The overarching goal will be to match mathematical models, statistical techniques and computational tools to ecologically meaningful questions. Conceptual and theoretical lectures will be combined with practical work in R, including developing novel Bayesian MCMC tools in STAN. The course will culminate with student-led research projects involving some non-trivial analysis of spatial data.\nThe structure of the class will be somewhat improvised. But there will be a combination of lectures and labs, and perhaps some guest lecturers on special topics. There will be weekly homework assignments and a final project that involves a novel, non-trivial analysis to be worked on and presented at the end of the semester.\nAll materials will be on this course website and integrated into the document-style lecture materials.\nThus, for example, assignments will be posted at this link: Assignments."
  },
  {
    "objectID": "introductions.html#to-the-book",
    "href": "introductions.html#to-the-book",
    "title": "1  Introduction(s)",
    "section": "1.2 …to the Book",
    "text": "1.2 …to the Book\nCollecting movement data is one of the main tools of wildlife ecologists, whether via satellite, GPS, GSM, acoustic, terrestrial, marine, freshwater, avian. Technologies are constantly improving, datasets are increasing at dizzying rates [refs]. Following in the swirling wake (and occasionally flashing ahead) of this data tsunami is a colorful and confounding array of methodological tools. To some extent, this is to be expected, since every tool is designed to address a particular kind of ecological or behavioral question and, at least in development, most tools are also designed around particular systems and particularities of the data collected. An extensive and influential branch of movement analysis, for example, was built around analyzing movements of marine organisms (e.g. sea turtles) from extremely gappy and notoriously error-ridden (but remarkable for the time) ARGOS data (Jonsen, Myers, and Flemming 2003, 2005). But the questions marine researchers were asking (e.g. can we adequately infer where the animal is at all from gappy and error-ridden data?) are profoundly different from the questions asked in a terrestrial contexts, where the question might be how are movement behaviors changing in a particular habitat? (Morales et al. 2004; Forester et al. 2007).\nMovement ecology - at this point - is no longer such a young field1. There is a dedicated journal, thousands of papers reporting on some (non-trivial) analysis of animal movement data, hundred(s?) just introducing and developing methodological tools, dozen(s) of R packages with which to implement those tools, and so on. But - remarkably - there are very few single “go-to” sources for students or practitioners to refer to in starting out in analysis.1 Set aside the fact that Aristotle discussed the migration of birds in his Historia Animalium 2500 years ago, much less the recent suggestion that the quantitative observation of animal migrations possibly predates written language(Bacon et al. 2023).\nCertainly, two books are worth mentioning: Peter Turchin’s Quantitative Analysis of Movement (Turchin 1998) is a “seminal” text with much to offer, but not particularly useful in a modern context. Only fairly recently have there been a book-length synthesis of some of the statistical models for animal movement data by several outstanding statistical (movement) ecologists in Hooten, Johnson, McClintock and Morales’ Animal Movement: Statistical Models for Telemetry Data (Hooten et al. 2017). While this text is an absolutely essential compendium of the main developments in movement modeling in the past two decades, it can be technically daunting for students or practitioners without strong statistical training. Also - in what is clearly the correct decision for a statistical, printed text that should stand the test of time - it provides many equations and figures, but no R code.\n\n\n\n\n\n\n\nTurchin et al. 1998\n\n\n\n\n\n\n\nHooten et al. 2017\n\n\n\n\nFigure 1.1: Nearly the only existing full length academic texts on analysis of animal movement data.\n\n\nThe goals of this eventual book are to provide a rigorous, comprehensive, and somewhat less technical, but highly visual and example-driven guide through working with spatial and movement data and understanding some of the most important tools and techniques for interpreting and analyzing those data.\n\n\nEadweard Muybridge demonstrated that bison move"
  },
  {
    "objectID": "introductions.html#references",
    "href": "introductions.html#references",
    "title": "1  Introduction(s)",
    "section": "References",
    "text": "References\n\n\n\n\nBacon, Bennett, Azadeh Khatiri, James Palmer, Tony Freeth, Paul Pettitt, and Robert Kentridge. 2023. “An Upper Palaeolithic Proto-writing System and Phenological Calendar.” Cambridge Archaeological Journal, January, 1–19. https://doi.org/10.1017/S0959774322000415.\n\n\nForester, J. D., A. R. Ives, M. G. Turner andD P. Anderson, D. Fortin, H. L. Beyer, D. W. Smith, and M. S. Boyce. 2007. “State-Space Models Link Elk Movement Patterns to Landscape Characteristics in Yellowstone National Park.” Ecological Monographs 77: 285–99. https://doi.org/10.1890/06-0534.\n\n\nHooten, Mevin B., Devin S. Johnson, Brett T. McClintock, and Juan M. Morales. 2017. Animal Movement: Statistical Models for Telemetry Data. First. Boca Raton : CRC Press, 2017.: CRC Press. https://doi.org/10.1201/9781315117744.\n\n\nJonsen, I. D., R. A. Myers, and J. M. Flemming. 2003. “Meta-Analysis of Animal Movement Using State-Space Models.” Wildlife Research 21: 149–61.\n\n\n———. 2005. “Robust State-Space Modeling of Animal Movement Data.” Ecology 86: 2874–80. https://doi.org/10.1890/04-1852.\n\n\nMorales, Juan Manuel, Daniel T. Haydon, Jacqui Frair, Kent E. Holsinger, and John M. Fryxell. 2004. “Extracting More Out of Relocation Data: Building Movement Models as Mixtures of Random Walks.” Ecology 85 (9): 2436–45. https://doi.org/10.1890/03-0269.\n\n\nTurchin, P. 1998. Quantitative Analysis of Movement: Measuring and Modeling Population Redistribution in Animals and Plants. Sunderland, Mass.: Sinauer."
  },
  {
    "objectID": "questionsinmovementecology.html",
    "href": "questionsinmovementecology.html",
    "title": "2  Kinds of movements, and kinds of questions",
    "section": "",
    "text": "Movement ecology is a relatively new discipline that coalesced with the publication of a paper by Ran Nathan and colleagues (Nathan et al. 2008). Currently, it can be defined as a science studying the causes, mechanisms, and patterns of animal movement. In Nathan et al, “movement ecology” was distinguished from other ecological studies on behavior by its focus on the movement of individuals, where this individual movement is a process with multiple spatial and temporal scales. Nathan et al defined a movement ecology conceptual framework or paradigm (Figure 2.1), where the movement path of an individual is affected by both internal factors (e.g. the motion capacity, the navigation capacity, and the internal state) and external factors. Each of these factors answer a different question about the drivers of animal movement, including the why to move (the internal state - seeking resources? escaping a predator?), the how to move (the motion capacity - walk? run? fly? swim? how to biomechanically move?), and the where to move (navigation capacity - how to cognitively move? an internal compass sense? chemosensory abilities? visual clues?). External factors (e.g. the environment) interact with all these factors and shapes the trajectory or movement path of an individual.\n\n\n\n\nFigure 2.1: Conceptual framework of movement ecology (Nathan et al. 2008).\n\n\n\nAn animal’s immediate and fine-scale movement can be captured with the step or the displacement of that animal between locations over some time period (Fig x.A). At an intermediate scale, a series of steps with similar characteristics can be lumped together to create movement phases (e.g., a mixture of movements that are straight and fast with those that are slow and tortuous, Fig x.B). At the broadest scale, all of these movement phases can be combined to determine behavior of an individual throughout its entire life history (Fig x.C).\n\n\nFig x. Scales of movement, Nathan et al 2008.\n\nSo what (broad) types of movement are ecologists interested in getting data for? Movement types can be sedentary, with local or “home range” movements between resource patches (Fig x.A); dispersive, with an individual moving from one location or population to another location or population (Fig x.B); migratory, with cyclical or seasonal movements between distinct habitats (Fig x.C); or nomadic, with wandering behavior and seemingly random movements (Fig x.D).\n\n\nFig x. Different broad types of movement.\n\nThese different movement types bring variation in movement that is seen across individuals within species. For example, an individual itself could demonstrate these different movement types within its lifetime (intra-individual variation) or individuals within a population could demonstrate these different movement types (inter-individual variation) or all individuals within a population may demonstrate the same movement type synchronously but may vary from other populations (inter-population variation). Variations can also be temporal (frequency and timing of movement) and spatial (distance covered, direction of movement). The different causes of movement (e.g., intra- or inter-specific interactions, abiotic factors, Fig x.A) interact with an individual’s internal state, perception, and genotype to create individual patterns (Fig x.B) of movement that have echoing effects for groups and populations, communities, and even ecosystems (Fig x.C).\n\n\nFig x. Causes, patterns, and consequences of variability in movement, Shaw 2020.\n\nA primary driver of movement, environmental variation, often comes about with phenological or cyclic/seasonal changes in the resource landscape (e.g., the spatial and temporal configuration of resources). These changes can trigger fine-scale movement responses, termed resource tracking, that result in patterns of movement (e.g., migrations) at the population scale or higher. Resource landscapes have been defined as having 6 interacting “axes” that stimulate resource tracking. These include abundance, timing, ephemerality, and predictability of resource patches, which can result in different spatial configurations and variance of the resource landscape (Abrahms et al 2020). Understanding these axes can help create predictions of animal movement and resource tracking, such as predictions on patch occupancy (Fig x.A, even lower quality patches will be occupied in abundance over time with phenological variation in resource patches); tracking probability (Fig x.B, animals will move or resource track more when resources have high turn-over); site fidelity (Fig x.C, animals will demonstrate higher site fidelity when resources have high long-term predictability); and movement distance (Fig x.D, animals will have higher rates of movement to track resources with broad-scale variability). Thus, understanding animal movement is often dependent on being able to measure and quantify not just the movement of an animal, but also the abiotic environment and resource landscape around it.\n\n\nFig x. Examples of the six axes for prediction of animal resource tracking, Abrahms et al 2020.\n\nA key limitation with using movement data to understand behavior is its dependence on both the sampling frequency and duration (Nathan et al 2008). In reality, the movement path or trajectory of an individual is continuous in time. However, movement data resolution is usually dependent on whatever biotelemetry device is being used to record the animal’s locations over time. The fix rate of these devices are generally discrete in time (and prone to error) and just a sample of the animal’s locations along its real trajectory. If one had an endless monitoring of movement for an animal, one could determine the lifetime track of an individual (Fig x.C). However, available technology and the high expense of biotelemetry monitoring has made this almost impossible for most animals (but see Cheng et al 2019 on the lifetime tracks of white storks) and instead, we are often limited to understanding shorter “chunks” of movement or behavioral phases (Fig x.B) for an animal. Due to the challenges of tagging newborn or juvenile individuals (e.g., high mortality rates, small body-to-tag ratios), our understanding of movement-based behavior is often even more limited to adult lifestages.\nBiotelemetry devices, e.g. tags, have the ability to record, at the minimum, an individual’s personal identifier (id) and at the maximum, a slew of information such as the animal’s location in time, surrounding environmental data (e.g. air or water temperature, depth, etc), and even biological data (e.g. heart rate, accelerometry, mouth movements). Tag types range from passive monitors, such as with some acoustic tags that depend on the animal passing by a fixed receiver to record its location and id, to active monitors, such as with VHF radio telemetry devices, where researchers actively follow collared animals to pinpoint their locations. For animals that move or migrate long distances (e.g. the cryptic leatherback turtle, which performs the longest reptile migrations from one side of the ocean to the other), tags with the ability to record locations and other interesting information remotely and more long-term, such as GPS or ARGOS satellite tags, are often used. The trade-off with using a particular tag often comes with its cost, its size, and its spatial/temporal resolution…long term (6 months or more) ARGOS satellite tags, for example, are often only able to track adult animals due to the heavy weights of batteries and sensors. Additionally, they often result in positional errors greater than 10 km, requiring some fancy locational filters, interpolation methods, or movement models to reduce the error around positions.\nAll of this is what makes movement ecology such a complex study of animal behavior - it is a question-driven science that is also co-dependent on the rapidly evolving fields of both technology and data science (e.g., the Integrated Bio-logging Framework, Fig x). Researchers must be adept in both of these areas, as well as statistics, geographical mapping, and other fun quantitative skills. Thankfully, there is a plethora of resources out there for the budding movement ecologist (e.g., the handy R for Ecology online resource) and this book is meant to be one of them!\n\n\nFig x. The Integrated Bio-logging Framework, Williams et al 2019.\n\nSo what kinds of questions might you be interested in asking and answering in movement ecology? With advancements in technology and data science, the range of questions we can answer has expanded tremendously. Our focus has shifted somewhat from seeking to quantify and understand the movement of individuals (Nathan et al 2008) to quantifying and understanding the collective movement and migrations of individuals within a population. Refined abilities to simulate animal movement and accurately model both the internal state and external environment of an animal has allowed for questions on the role of learning and perception and memory for animal movement and navigation to be addressed. Increasingly, movement ecology faces pressure to create solutions and tools that address issues in animal conservation (Fig x), as the static conservation approaches of the past are often not effective for animals that migrate and have spatially and temporally dynamic movements and habitat use.\n\n\nFig x. A framework for merging movement data with management goals, Allen and Singh 2016.\n\nThe advent of smaller biotelemetry devices with longer duration batteries and a variety of sensors has further allowed for modeling and quantifying of animal movement behaviors to expand beyond two-dimensional movement (e.g., changes in latitude, longitude) to multi-dimensional movement (e.g., changes in latitude, longitude, and alitude or depth). Increasingly, we get closer with these advances to being able to quantify and understand the lifetime track of individual(s), with a rise in biotelemetry studies on juveniles and other cryptic lifestages. Some other questions of key interest in modern movement ecology studies include sociality of movement, impacts of movement on biodiversity research, and the impacts of anthropogenic activities and climate change. In the following chapters of this book, we will discuss a variety of quantitative approaches to analyzing a variety of movement data but the analysis you choose will largely be dependent on your question and the resolution, frequency, and sample size of your data.\n\n\n\n\nNathan, Ran, Wayne M Getz, Eloy Revilla, Marcel Holyoak, Ronen Kadmon, David Saltz, and Peter E Smouse. 2008. “A Movement Ecology Paradigm for Unifying Organismal Movement Research.” Proceedings of the National Academy of Sciences 105 (49): 19052–59."
  },
  {
    "objectID": "whyspecialstats.html#spatial-data",
    "href": "whyspecialstats.html#spatial-data",
    "title": "3  Why do we need special statistics?",
    "section": "\n3.1 Spatial data",
    "text": "3.1 Spatial data\nSpatial data is simply data that contains spatial coordinates - in practice, we are almost always only concerned with two dimensions (\\({\\bf X, Y}\\)) with perhaps a set of additional observations \\(A\\).1. Spatial data may represent a complete set of observations, for example it can represent the locations of all trees of a certain size in a given study plot. Often in such cases there are questions that can be asked about the generating process which determines the distribution of those locations. When describing a process, the coordinates themselves and alone can be considered a “response”. For example, a typical question might be: are the trees clustered, inhibiting each other, or randomly distributed? This is referred to as analyzing a point process (see Chapter XX).1 We use boldfacing to indicate vectors (e.g. of length \\(n\\)), and capitals to indicate matrices such that \\(A\\) is a \\(n \\times k\\) matrix for \\(k\\) observations.\n\n\n\n\nCoordinates of American beech (Fagus grandifolia) in a research site at Adirondack Station, SUNY-ESF, New York State. The sizes reflect diameter at breast height in 2009. (Gienke et al. 2014, data: M. Dovciak)\n\n\n\n\nAlternatively, spatial data can be considered a sampling of a continuous spatial structure. For example, we might measure a set of soil properties (pH, saility, moisture, thickness) over a study area, and have some questions about why the soil is structured the way it is. In those cases, the spatial coordinates are not of intrinsic interest (they were established by the reserach), but more similar to a “nuisance” parameter. To make sure that any inference we make on those data is “correct”, i.e. not unduly influenced by that autocorrelation, we have to take that autocorrelation into account. On the other hand, the scale of spatial autocorrelation of a process can itself be a variable of interest. Proper ways to do this is very much a central topic in spatial ecology as well (Chapter XX)."
  },
  {
    "objectID": "whyspecialstats.html#movement-data",
    "href": "whyspecialstats.html#movement-data",
    "title": "3  Why do we need special statistics?",
    "section": "\n3.2 Movement data",
    "text": "3.2 Movement data\nMovement data - representing a measured sequence of locations for an individual organism - can be thought of as spatial data that is indexed in time, or - equivalently - as a two-dimensional time-series.\n\n one-dimensional: Multi-Dimensional! (X,Y,Time) > Solution: get comfortable with complex numbers.\n independent: Highly dependent! > Solution: get comfortable with  correlated / dependent data .\n identically distributed!: Complex and heterogeneous! > Solution: behavioral change point analysis / segmentation, hidden markov models.\n often normal: Circular and skewed distributions! > Solution: Flexible Likelihood-based modelling,\n randomly/uniformly sampled: Gappy / irregular / oddly duty-cycled data > Solution: Continuous movement modeling."
  },
  {
    "objectID": "complex_numbers.html#sec-complexnumbers",
    "href": "complex_numbers.html#sec-complexnumbers",
    "title": "\n4  Complex Numbers are Simple\n",
    "section": "\n4.1 Components of complex number",
    "text": "4.1 Components of complex number\nComplex numbers are a handy bookkeeping tool to package together two dimensions in a single quantity. They expand the one-dimensional (“Real”) number line into a second dimension (“Imaginary”).\nWe write a complex number \\(Z = X + iY\\)\n\n\n\\(X\\) is the real part.\n\n\\(Y\\) is the imaginary part.\n\nThe same number can be written in terms of distances and angles (which are a more “natural” language for spatial data):\n\\[ Z = R \\, \\exp(i \\theta) \\]\n\n\n\\(R\\) is the length of the vector from the origin, aka modulus.\n\n\\(\\theta\\) is the orientation of the vector, aka argument.\n\nThis angle, \\(\\theta\\) is defined in radians that turn counter-clockwise from the x-axis, \\(\\pi/2\\) is on the y-axis, \\(\\pi\\) is on the negative x-axis, etc. You can see that by simply putting in some numbers:\n\\[Z = 1 = \\exp{0}\\] \\[Z = 0 + 1i = \\exp{(i \\pi/2)}\\]\nAll of these have modulus 1.\nMore that the algebraic / trigonometric way to get distances (moduli) and angles (arguments) from a vector is a bit tedious. Thus, for \\(Z = X + iY\\):\n\\[R = \\sqrt{X^2 + Y^2}\\] Not terrible. But check out how to compute the angle from x and y:\n\\[\n\\theta(x,y) =\n\\begin{cases}\n\\arctan\\left(\\frac y x\\right) &\\text{if } x > 0, \\\\[5mu]\n\\arctan\\left(\\frac y x\\right) + \\pi &\\text{if } x < 0 \\text{ and } y \\ge 0, \\\\[5mu]\n\\arctan\\left(\\frac y x\\right) - \\pi &\\text{if } x < 0 \\text{ and } y < 0, \\\\[5mu]\n+\\frac{\\pi}{2} &\\text{if } x = 0 \\text{ and } y > 0, \\\\[5mu]\n-\\frac{\\pi}{2} &\\text{if } x = 0 \\text{ and } y < 0, \\\\[5mu]\n\\text{undefined} &\\text{if } x = 0 \\text{ and } y = 0.\n\\end{cases}\n\\]\nYikes!\nOn the other hand, these quantities are obtained instantly and easily in R.\n\n4.1.1 In R\n\nX <- c(3,4,-2)\nY <- c(0,3,2)\nZ <- X + 1i*Y\nZ\n\n[1]  3+0i  4+3i -2+2i\n\n\nAlternatively:\n\nZ <- complex(re = X, im=Y)\n\n\nplot(Z, pch=19, col=1:3, asp=1)\narrows(rep(0,length(Z)), rep(0,length(Z)), Re(Z), Im(Z), lwd=2, col=1:3)\n\n\n\n\nNote: ALWAYS use asp=1 - “aspect ratio = 1:1” - when plotting (properly projected) movement data!\nObtaining summary statistics is nearly instant. Obtain lengths of vectors:\n\nMod(Z)\n\n[1] 3.000000 5.000000 2.828427\n\n\nObtain orientation of vectors:\n\nArg(Z)\n\n[1] 0.0000000 0.6435011 2.3561945\n\n\nNote, the orientations are in radians, i.e. range from \\(0\\) to \\(2\\pi\\) going counter-clockwise from the \\(x\\)-axis. Compass directions go from 0 to 360 clockwise, so, to convert:\n\n90-(Arg(Z)*180)/pi\n\n[1]  90.0000  53.1301 -45.0000"
  },
  {
    "objectID": "complex_numbers.html#quickly-simulating-a-path",
    "href": "complex_numbers.html#quickly-simulating-a-path",
    "title": "\n4  Complex Numbers are Simple\n",
    "section": "\n4.2 Quickly simulating a path",
    "text": "4.2 Quickly simulating a path\nQuick code for a correlated random walk:\n\nX <- cumsum(arima.sim(n=100, model=list(ar=.7)))\nY <- cumsum(arima.sim(n=100, model=list(ar=.7)))\nZ <- X + 1i*Y\nplot(Z, type=\"o\", asp=1)\n\n\n\n\nInstant summary statistics of a trajectory:\nThe average location\n\nmean(Z)\n\n[1] -7.14423-10.61551i\n\n\nThe step vectors:\n\ndZ <- diff(Z)\nplot(dZ, asp=1, type=\"n\")\narrows(rep(0, length(dZ)), rep(0, length(dZ)), Re(dZ), Im(dZ), col=rgb(0,0,0,.5), lwd=2, length=0.1)\n\n\n\n\nDistribution of step lengths:\n\nS <- Mod(dZ)\nsummary(S)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.1316  1.1292  1.6811  1.7293  2.2438  4.5716 \n\nhist(S, col=\"grey\", bor=\"darkgrey\", freq=FALSE)\nlines(density(S), col=2, lwd=2)\n\n\n\n\nWhat about angles?\n\nThe absolute orientations:\n\n\nPhi <- Arg(dZ)\nhist(Phi, col=\"grey\", bor=\"darkgrey\", freq=FALSE, breaks=seq(-pi,pi,pi/3))\n\n\n\n\n\nTurning angles\n\n\nTheta <- diff(Phi)\nhist(Theta, col=\"grey\", bor=\"darkgrey\", freq=FALSE)\n\n\n\n\n QUESTION: What is a problem with this histogram?\nCircular statistics:\nAngles are a wrapped continuous variable, i.e. \\(180^o > 0^o = 360^o < 180^o\\). The best way to visualize the distribution of wrapped variables is with Rose-Diagrams. An R package that deals with circular data is circular.\n\nrequire(circular)\nTheta <- as.circular(Theta)\nPhi <- as.circular(Phi)\nrose.diag(Phi, bins=16, col=\"grey\", prop=2, main=expression(Phi))\nrose.diag(Theta, bins=16, col=\"grey\", prop=2, main=expression(Theta))\n\n\n\n\nLAB EXERCISE\n\n\nLoad movement data of choice!\nConvert the locations to a complex variable Z.\nObtain a vector of time stamps T, draw a histogram of the time intervals. Then, ignore those differences.\nObtain, summarize and illustrate:\n\n\nthe step lengths\nthe absolute orientation\nthe turning angles"
  },
  {
    "objectID": "complex_numbers.html#complex-manipulations-are-easy",
    "href": "complex_numbers.html#complex-manipulations-are-easy",
    "title": "\n4  Complex Numbers are Simple\n",
    "section": "\n4.3 Complex manipulations (are easy)",
    "text": "4.3 Complex manipulations (are easy)\n\n4.3.1 Addition & substration of vectors\nAddition and subtraction of vectors:\n\\[ Z_1 = X_1 + i Y_1; Z_2 = X_2 + i Y_2\\] \\[ Z_1 + Z_2 = (X_1 + X_2) + i(Y_1 + Y_2)\\]\nUseful, e.g., for shifting locations:\n\nplot(Z, asp=1, type=\"l\", col=\"darkgrey\", xlim=c(-2,2)*max(Mod(Z)))\nlines(Z - mean(Z), col=2, lwd=2)\nlines(Z + Z[length(Z)], col=3, lwd=2)\n\n\n\n\n\n4.3.2 Rotation of vectors\nMultiplication of complex vectors \\[Z_1 = R_1 \\exp(i \\theta_1); Z_2 = R_2 \\exp(i \\theta_2)\\] \\[ Z_1 Z_2 = R_1 R_2 \\exp(i (\\theta_1 + \\theta_2))\\] Note the magic of the rotation summing! If \\(\\text{Mod}(Z_2) = 1\\), multiplications rotates by \\(\\text{Arg}(Z_2)\\)\n\ntheta1 <- complex(mod=1, arg=pi/4)\ntheta2 <- complex(mod=1, arg=-pi/4)\n\nplot(Z, asp=1, type=\"l\", col=\"darkgrey\", lwd=3)\nlines(Z*theta1, col=2, lwd=2)\nlines(Z*theta2, col=3, lwd=2)\n\n\n\n\nA colorful loop:\n\nrequire(gplots)\nplot(Z, asp=1, type=\"n\", xlim=c(-1,1)*max(Mod(Z)), ylim=c(-1,1)*max(Mod(Z)))\ncols <- rich.colors(1000,alpha=0.1)\nthetas <- seq(0,2*pi,length=100)\nfor(i in 1:1000)\n  lines(Z*complex(mod=1, arg=thetas[i]), col=cols[i], lwd=4)\n\n\n\n\nI know you’re probably thinking …\n\n“Thanks for teaching me how to make a weird swirling rainbow thing … but why in the world would I want to shift and rotate my precious, precious data, which was just perfect the way it was?\n\nMy response:  Null Sets for Pseudo Absences!"
  },
  {
    "objectID": "complex_numbers.html#example-with-finnish-wolves",
    "href": "complex_numbers.html#example-with-finnish-wolves",
    "title": "\n4  Complex Numbers are Simple\n",
    "section": "\n4.4 Example with Finnish Wolves",
    "text": "4.4 Example with Finnish Wolves\n\nIn-depth summer predation study, questions related to habitat use, landscape type - forest/bog/field - linear elements - roads/rivers/power lines, etc.\n\n\n4.4.1 Defining Null Sets\n\nObtain all the steps and turning angles\nRotate them by the orientation of the last step (\\(Arg(Z_1-Z_0)\\))\nAdd the rotated steps to the last step (\\(Z_1\\))\n\n\n\n\nObtain all the steps and turning angles\nRotate them by the orientation of the last step (\\(Arg(Z_1-Z_0)\\))\n\n\nn <- length(Z)\nS <- Mod(diff(Z))\nPhi <- Arg(diff(Z))\nTheta <- diff(Phi)\nRelSteps <- complex(mod = S[-1], arg=Theta)\n\nZ0 <- Z[-((n-1):n)]\nZ1 <- Z[-c(1,n)]\nZ2 <- Z[-(1:2)]\n\nRotate <- complex(mod = 1, arg=Arg(Z1-Z0))\n\n\nplot(c(0, RelSteps), asp=1, xlab=\"x\", ylab=\"y\", pch=19)\narrows(rep(0,n-2), rep(0, n-2), Re(RelSteps), Im(RelSteps), col=\"darkgrey\")\n\n\n\n\n\nNote: in practice (i.e. with tons of data), it is sufficient to randomly sample some smaller number (e.g. 30) null steps at each location.\n\n\nAdd the rotated steps to the last step\n\n\nZ.null <- matrix(0, ncol=n-2, nrow=n-2)\nfor(i in 1:length(Z1))\n  Z.null[i,] <- Z1[i] + RelSteps * Rotate[i]\n\n\nMake the fuzzy catterpillar plot\n\n\nplot(Z, type=\"o\", col=1:length(Z), pch=19, asp=1)\nfor(i in 1:nrow(Z.null))\n  segments(rep(Re(Z1[i]), n-2), rep(Im(Z1[i]), n-2), \n           Re(Z.null[i,]), Im(Z.null[i,]), col=i+1)\n\n\n\n\n\n4.4.2 Null set\nThe use of the null set is a way to test a narrower null hypothesis that accounts for auto correlation in the data.\nThe places the animal COULD HAVE but DID NOT go to are pseudo-absences, against which you can fit, e.g., logistic regression models (aka Step-selection functions).\nOr just be simple/lazy (like us) and compare observed locations with Chi-squared tests:\n\n EXERCISE: Create a fuzzy-catterpillar plot!\nUse (a portion) of the data you analyzed before.\n\n# get pieces\nn <- length(Z)\nSteps <- diff(Z)\nS <- Mod(Steps)\nPhi <- Arg(Steps)\nTheta <- diff(Phi)\nRelSteps <- complex(mod = S[-1], arg = Theta)\n\n# calculate null set\nZ0 <- Z[1:(n-2)]\nZ1 <- Z[2:(n-1)]\nZ2 <- Z[3:n]\nRotate <- complex(mod = 1, arg = Arg(Z1-Z0))\n\nZ.null <- matrix(NA, ncol=n-2, nrow=n-2)\nfor(i in 1:length(Z1))\n  Z.null[i,] <- Z1[i] + sample(RelSteps) * Rotate[i]\n\n# plot\nplot(Z, type=\"o\", col=1:10, pch=19, asp=1)\nfor(i in 1:nrow(Z.null))\n  segments(rep(Re(Z1[i]), n-2), rep(Im(Z1[i]), n-2), \n           Re(Z.null[i,]), Im(Z.null[i,]), col=i+1)\n\n\n Fuzzy Polar Bear Catterpillar!"
  },
  {
    "objectID": "spatial_data_in_R.html#what-is-spatial-data",
    "href": "spatial_data_in_R.html#what-is-spatial-data",
    "title": "\n5  Spatial data in R\n",
    "section": "\n5.1 What is spatial data?",
    "text": "5.1 What is spatial data?\nSpatial data is simply data with locational information. It has both spatial attributes (e.g., geographic location) and non-spatial attributes (e.g., descriptive information, such as id name or quantitative measurements).\nThe locations of spatial data correspond to geographic coordinates that relate the data to where on Earth’s surface it occurs. These coordinates are positions on both the latitude (Y) and longitude (X) lines composing Earth’s graticular network.\n\nSo, if someone gave you a set of coordinates (longitude, latitude values), could you tell me exactly where on Earth those coordinates would be?\nProbably not, because Earth is an imperfect, spinning sphere! All of those characteristics mean that the perfect spherical graticule we have for lines of latitude and longitude are not a realistic model for Earth’s true surface.\n\nGeographic Coordinate Systems (GCS’s) are models of Earth’s surface that relate graticule (latitude, longitude) values to real locations on Earth. For example, the World Geodetic System 1984 (WGS 1984) is a GCS commonly used for mapping global data. However, their reliance on the graticule (which assumes a perfect spherical Earth) means that they are not useful for performing spatial operations where you need to understand the distance between locations.\n\nFor quantitative spatial operations, you instead want to use a Projected Coordinate System (PCS), which essentially flattens a GCS to create a more realistic model for a particular section of the Earth you are interested in. For example, the Universal Transverse Mercator (UTM) system is a common projection that divides the globe into zones. You can transform your spatial data to this projection once you figure out which zone it belongs in, allowing you to perform quantitative spatial operations on your data!\n\nUsing a GCS or PCS depends on what questions you are trying to answer and what kind of spatial operations you wish to perform. For visual or mapping purposes, the use of a GCS is often preferred, whereas any quantitative operations with spatial data generally require a transformation to a PCS. Keep in mind, the units for a GCS are in angular degrees, whereas the units for a PCS are linear measures (e.g., meters)."
  },
  {
    "objectID": "spatial_data_in_R.html#types-of-spatial-data",
    "href": "spatial_data_in_R.html#types-of-spatial-data",
    "title": "\n5  Spatial data in R\n",
    "section": "\n5.2 Types of spatial data",
    "text": "5.2 Types of spatial data\n\nSpatial data can have a variety of characteristics, including size (e.g., length, area, perimeter), shape (e.g., point, line, polygon), and a variety of spatial relationships or topology types, including adjacency, connectivity, overlap, or proximity to other spatial objects or data.\n\nSpatial data has two broad types: vector data and raster data. Vector data is a graphical representation of “real-world” features and can come as points, lines, or polygons. This data is often stored in shapefile format (.shp).\n\nIn contrast, raster data represents data as a grid of pixels, where each pixel has a value and a resolution.\n\nThe resolution of raster data refers to the size of individual cells in the grid. If your data has a PCS, these cells will be in measurable distance units (e.g., a 1 km resolution indicates that individual cells are 1000 m x 1000 m). If your data has a GCS, resolution is referred to in angular degrees (e.g., a 0.01 degree resolution is equivalent to a ~1 km resolution).\n\nYou can cast or change your spatial data type in R. Just keep in mind that you may loose some information switching between vector to raster data (and vice versa) because of their differences in structure. Vectors have fluid boundaries, whereas rasters are composed of a bunch of little squares with sharp edges. Often, we use raster data to represent continuous data, especially from imagery, such as satellite images of abiotic data (land temperature, topography, etc). Raster data is essentially a matrix of measurable cells, making it ideal for quantitative spatial operations on layered data or any kind of matrix operations."
  },
  {
    "objectID": "spatial_data_in_R.html#importance-of-understanding-spatial-relationships-for-movement-ecology",
    "href": "spatial_data_in_R.html#importance-of-understanding-spatial-relationships-for-movement-ecology",
    "title": "\n5  Spatial data in R\n",
    "section": "\n5.3 Importance of understanding spatial relationships for movement ecology",
    "text": "5.3 Importance of understanding spatial relationships for movement ecology\nAs discussed in previous chapters, movement data is inherently spatial. But working with movement data requires a more extensive understanding of spatial relationships and ecology than just being able to quantify and visualize the locational data for your mobile taxa of interest.\nTake a minute to imagine the movement tracks of your taxa in geographic space. Most likely, your taxa exists in a spatial landscape consisting of all sorts of data, such as anthropogenic features (e.g., roads, buildings) and/or natural features (e.g., lakes, trees, topography or changes in elevation). Now, imagine these different peices of the landscape and how you would represent them as spatial data (points, lines, polygons, rasters?) and finally, how your taxa might interact with these features as they move through this landscape. These interactions are the movement-based spatial relationships you might be interested in understanding as a movement ecologist!\n\nYour movement data itself can be represented and analyzed using different types of spatial data. For example, you could characterize behavior from movement data using a functional approach (e.g., represent movement states as annotated line segments), by assessing behavioral intensity (e.g., represent different levels of behavioral intensity as density isopleth polygons), by determining the structural features of a behavior (e.g., represent movement as a network of point-based behaviors), or by relating movement data locations to an underlying continuous behavioral surface (e.g., extract fitness values of locational data using a fitness or energy landscape raster) (Wittemyer, Northrup, and Bastille-Rousseau 2019-07-29, 2019-07).\n\n\nFrom: Wittemyer et al 2019."
  },
  {
    "objectID": "spatial_data_in_R.html#some-tips-and-tricks-for-working-with-spatial-data",
    "href": "spatial_data_in_R.html#some-tips-and-tricks-for-working-with-spatial-data",
    "title": "\n5  Spatial data in R\n",
    "section": "\n5.4 Some “Tips and Tricks” for working with spatial data",
    "text": "5.4 Some “Tips and Tricks” for working with spatial data\nThe more you work with spatial data, the more you will gain your own “tips and tricks” for working with it. But here are a few essentials to start with:\n\nYou need to define the GCS and/or PCS before mapping and doing spatial operations with it. Be sure the GCS or PCS you define matches the metadata or description of the data (when in doubt, contact the owner of the data). Currently, it is easiest to define your coordinate reference system using its EPSG code.\nSimilarly, be sure you know the resolution and spatial extent of your raster data. Defining either your resolutions or GCS/PCS incorrectly will result in incorrect results and wonky maps!\n… which is why you should always check the structure of your data before and after performing spatial operations with it. Mapping it (e.g., with an interactive mapping package such as mapview) is a great way to check for unexpected results!\nWhen performing quantitative operations with spatial data, you should transform your data from a GCS to a PCS.\nWhen mapping multiple layers and spatial data types (e.g., vectors and rasters), it is important that they have the same coordinate reference system and often, the same spatial extent\nWhen doing analyses with spatial data that comes from different sources (e.g., trying to match daily locational data with environmental raster data), it is important that the spatial and temporal resolution you choose for your final product and the data layers in the analyses be logical and biologically informed. For example, if you want to match your daily locations with raster data for temperature and your taxa moves (on average) 25 km/day, does it make more sense to use temperature raster data that has a 0.01 degree (~ 1 km) resolution or that has a 1 degree (~ 100 km) resolution? Your answer is likely the latter (1 degree resolution raster), as this resolution will allow you to better match your locational data (a scale difference of 4 versus a scale difference of 25). However, you choice may also be dependent on the data available and they may not match exactly. Always go back to your research question to see if it makes sense to use these data layers together (e.g., if you expect the animal behavior from the locational data to persist ~3-4 days, this would well match the 1 degree resolution of your raster).\nIf you have to, you can change the spatial or temporal resolutions of your spatial data layers to better match each other (e.g. through aggregation, interpolation, averaging, or resampling) but just keep in mind that this can (and often does) cause issues with extrapolation, data degradation, and inaccuracy. Better to find data that matches well!\nKeep in mind that for raster data, finer resolution data is not always better, both for the issues just mentioned AND that high resolution rasters can be massively large files that take massively long periods of time to perform spatial operations on or map. Sometimes converting your raster data to vector data (e.g., multipolgons) can speed things up.\nWhen in doubt, Google it! There are lots of great websites, forums, books, and even groups that can help you figure out how to perform your desired spatial operations, solve error codes, etc."
  },
  {
    "objectID": "spatial_data_in_R.html#r-examples",
    "href": "spatial_data_in_R.html#r-examples",
    "title": "\n5  Spatial data in R\n",
    "section": "\n5.5 R examples",
    "text": "5.5 R examples\nWe will walk through some examples in R below using recent packages for spatial data analyses and analyses. These examples are just an introduction to the many different spatial operations you can perform in R{^index-1]. There are many helpful resources out there to help with further spatial data analysis and ecology questions. This, for example, is an excellent resource: Using Spatial Data with R.\nFor this tutorial, we will use the following R packages: - dplyr and magrittr for data processing and manipulation, - sf and raster for loading spatial and raster data - ggplot2, mapview, ggspatial, basemaps and mapdata for mapping and visualization. - fasterize and geosphere for a few higher-level operations.\nBe sure all of these have been installed.\n\n# create packages list\npackages <- list(\"dplyr\",\"sf\",\"raster\",\"fasterize\",\n                 \"ggplot2\",\"mapview\",\"ggspatial\",\n                 \"basemaps\",\"geosphere\",\"mapdata\")\nsapply(packages, require, character = TRUE)\n\n\n5.5.1 Load your data\nWe will use two example datasets, one on sperm whales (a public Movebank dataset, study link here, originally published by Irvine et al 2017) and one on the calving grounds of Western Arctic caribou (originally published by Couriot et al 2023).\nWe will load our sperm whale data, stored on Excel csv files, using the “read.csv” function, and our caribou data, stored as shapefiles, using the “st_read” function from the “sf” R package. We will also bring in interesting environmental data for both, including a stack of rasters for daily sea surface temperature for the sperm whales, stored as an Rda object, and a river shapefile for the caribou.\nWe additionally define the CRS (coordinate reference system) of each shapefile as we read it in, using the “st_set_crs” function. We do this using their EPSG code (here, 4326 = WGS 1984 GCS) and then we transform each of them to a PCS using the sf::st_transform() function. We will walk through more examples of this shortly.\n\n## sperm whale tracking data (Movebank)\nsperm_whales <- read.csv(\"./data/spatialdata/SpermWhale_dataset.csv\")\nstr(sperm_whales)\n\n'data.frame':   1220 obs. of  4 variables:\n $ timestamp                  : chr  \"4/3/2007 4:36\" \"3/27/2007 22:17\" \"3/28/2007 1:19\" \"3/28/2007 7:00\" ...\n $ location.long              : num  -112 -112 -112 -112 -113 ...\n $ location.lat               : num  28 28.1 28.2 28.2 28.1 ...\n $ individual.local.identifier: int  4400829 4400837 4400837 4400837 4400837 4400837 4400837 4400837 4400837 4400837 ...\n\n## sea surface temperature (sst, deg Celsius) raster stack data for whales\nload(\"./data/spatialdata/sst_rasterdata_whales.rda\")\nsst_stack\n\nclass      : RasterStack \ndimensions : 305, 364, 111020, 23  (nrow, ncol, ncell, nlayers)\nresolution : 0.01, 0.01  (x, y)\nextent     : -113.561, -109.921, 26.698, 29.748  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nnames      : layer.1, layer.2, layer.3, layer.4, layer.5, layer.6, layer.7, layer.8, layer.9, layer.10, layer.11, layer.12, layer.13, layer.14, layer.15, ... \nmin values :    23.1,    22.6,    23.3,    23.1,    22.7,    23.7,    23.7,    23.7,    23.8,     23.8,     24.6,     28.4,     28.2,     28.9,     27.5, ... \nmax values :    32.5,    32.5,    32.0,    30.7,    31.9,    31.6,    31.6,    30.7,    31.5,     32.3,     32.4,     32.5,     32.5,     32.5,     32.5, ... \n\n## calving ranges and river (kobuk)\n# open ranges (polygons)\ncalving_ranges <- st_read(\"./data/spatialdata/WAH_multiannual_calving_ranges.shp\")  %>% \n  st_set_crs(4326) %>% st_transform(3857)\n\nReading layer `WAH_multiannual_calving_ranges' from data source \n  `/home/elie/teaching/SpatialAndMovementEcologyCourse/data/spatialdata/WAH_multiannual_calving_ranges.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 2 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -161.9217 ymin: 67.98733 xmax: -157.9179 ymax: 70.42118\nGeodetic CRS:  WGS 84\n\n# open river (line)\nkobuk <- st_read(\"./data/spatialdata/Kobuk.shp\") %>% st_set_crs(4326) %>% st_transform(3857)\n\nReading layer `Kobuk' from data source \n  `/home/elie/teaching/SpatialAndMovementEcologyCourse/data/spatialdata/Kobuk.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 3 features and 8 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: -165.0993 ymin: 66.71652 xmax: -149.5818 ymax: 67.20265\nGeodetic CRS:  GCS_unknown\n\n\n\n5.5.2 Data exploration (sperm whale tracks)\nFirst, we will visually look at the structure of our sperm whale data, by plotting the duration of our whale tracks by the id column. We need to also define our timestamp data, however, to be in POSIX format so that R knows the timestamp column contains Data-Time data (see also the lubridate R package for more date-time functions).\n\n# convert timestamp data to POSIXct format (you can also use lubridate for this)\nsperm_whales$timestamp <- as.POSIXct(sperm_whales$timestamp, format=\"%m/%d/%Y %H:%M\")\n\n# make your id column a factor and change the name to something shorter:\ncolnames(sperm_whales)[4] <- \"id\"\nsperm_whales$id<-as.factor(sperm_whales$id) # we have 25 different whales!\n\n\n# make a quick plot of tracks over time and by id:\nggplot(data=sperm_whales,aes(x=timestamp,y=id,color=id))+ # color bars by id levels\n  geom_path(size=1) +\n  theme_classic() + # I like this for a nice background theme\n  xlab(\"Time\") + ylab(\"ID\")+ # change axis names\n  theme(legend.position = \"none\") # don't need a legend since id names are on the left already\n\n\n\n\nThe results show that there are 25 different whales! For example purposes, we will subset for two of these:\n\n## looks like we have 3 separate groups of tracked whales!\n## let's subset for individuals after July 1, 2008:\nsperm_whales2<- sperm_whales %>% \n  filter(timestamp > \"2008-07-01 00:00:00\")\n\nggplot(data=sperm_whales2,aes(x=timestamp,y=id,color=id))+ # color bars by id levels\n  geom_path(size=1) +\n  theme_classic() + # I like this for a nice background theme\n  xlab(\"Time\") + ylab(\"ID\")+ # change axis names\n  theme(legend.position = \"none\") # don't need a legend since id names are on the left already\n\n\n\n\nWe plot the tracks for these individuals without making them into spatial objects, using the “ggplot2” package and specifying longitude as our x-axis and latitude as our y-axis.\n\nggplot(data=sperm_whales2,aes(x=location.long,y=location.lat,color=id))+ # color tracks by id\n  geom_path()+\n  geom_point()+\n  theme_classic()+\n  xlab(\"Longitude\")+ylab(\"Latitude\")\n\n\n\n\nWe can see that our blue individual has a lot more points in time (note also that there are likely some outlier values) and that there is spatial overlap between the individuals. We can use the facet_wrap() function in ggplot to create separate maps for each id (using “scales=free” to allow them to have different extents):\n\nggplot(data=sperm_whales2,aes(x=location.long,y=location.lat,color=id))+ # color tracks by id\n  geom_path()+\n  geom_point()+\n  theme_classic()+\n  xlab(\"Longitude\")+ylab(\"Latitude\")+\n  facet_wrap(~id,scales=\"free\")\n\n\n\n\nBefore we make this data into a spatial object, let’s create a function to look at the median sampling frequency and temporal range for each whale track. This gives us an idea of our data resolution:\n\ndtime <- function(t, ...) {difftime(t[-1], t[-length(t)], ...) %>% as.numeric}\n\ndata_summary <- sperm_whales2 %>% \n  group_by(id) %>% \n  summarize(start = min(timestamp),end = max(timestamp), MedianTimestep_hrs = round(median(dtime(timestamp, units = \"hours\")))) %>%\n  mutate(DataSpan=round(end-start)) %>% data.frame()\n\ndata_summary\n\n       id               start                 end MedianTimestep_hrs DataSpan\n1 4800837 2008-07-02 05:03:00 2008-07-12 01:59:00                 45  10 days\n2 4810843 2008-07-02 00:30:00 2008-07-25 14:13:00                  2  24 days\n\n\nOur tracks are relatively short (10-24 days) and our first whale (ID 4800837) has a large median timestep of 45 hours! This is very typical of marine data, especially for fully pelagic species like whales, that surface unpredictably and irregularly.\n\n5.5.3 Making our data spatial\nBefore we can map our whale tracks and do any spatial operations, we need to define it as a spatial object in R. This requires telling R what columns the coordinates are contained in in our dataframe and the coordinate reference system (CRS). Since our coordinates are in latitude/longitude and degrees, we will use a common GCS (WGS 1984).\n\n# we specify the crs using the espg code\n## for the GCS WGS 1984 it's 4326\nsperm_whales_sf <- sperm_whales2 %>% \n  st_as_sf(coords = c(\"location.long\",\"location.lat\"), crs=4326)\n\n\n5.5.4 Plot data with the “mapview” package\nNow that we have defined our data as spatial objects in R, we can create an interactive map using the “mapview” package. These maps allow you to Zoom in on your data, specify a basemap, plot different variables (similar to ggplot), and toggle specific layers on/off.\nCheck out this link for advanced options and examples for mapview plotting.\nWe are going to plot the spatial version of our whale data, using green and blue colors for the different id levels and an Esri background map.\n\n## we use the \"zcol\" argument use id as our plotting variable and \"col.regions\" to color by the factor levels of id\n## there are lots of different background map options...\nmapview(sperm_whales_sf, zcol=\"id\", col.regions=c(\"green\",\"blue\"),\n        map.types=\"Esri.WorldImagery\",legend=TRUE)\n\n\n\n\n\n\nLooks like our whales are in the Gulf of California!\nWhat if we want to plot the tracks instead of just the locations of the whales? We can convert the feature type from points to lines (grouped by each id) to plot the individual whale tracks:\n\nsperm_whale_tracks <- sperm_whales_sf %>% dplyr::group_by(id) %>% \n  dplyr::summarize(do_union=FALSE) %>% sf::st_cast(\"LINESTRING\")\n\nmapview(sperm_whale_tracks, zcol = \"id\", color=c(\"green\",\"blue\"),\n        map.types=\"Esri.WorldImagery\", legend=TRUE)\n\n\n\n\n\n\nThis operation is a useful one for a practical observational reason: it is good to connect points to get a sense of a track! But it is also computationally a much easier operation for mapview to draw and explore a few lines than a few thousand points.\n\n5.5.5 Map data using the ggspatial R package\nWhat about mapping our tracks with some interesting environmental data? A variable of interest for these whales might be sea surface temperature (sst).\nCurrently, the sst raster data we have (sst_stack) is a “stack” of rasters, where each raster is a layer corresponding to sst data extracted to the timestamp range and spatial extent of our whale data (source: ERDAPP/CoastWatch, NOAA Southwest Fisheries Science Center). This raster data has a temporal resolution in hours and spatial resolution of 0.01 degrees.\nRaster stacks are great for performing operations on large numbers of rasters all at once. For example, if we wanted to create a raster for the mean daily sst in our region to plot beneath our tracks, we could take the mean sst across all layers in the stack:\n\nsst_mean <- mean(sst_stack,na.rm=TRUE)\n\n# you can plot it with base R plot functions and it automatically applies a color scale and a label:\nplot(sst_mean)\nplot(sperm_whales_sf, add=TRUE)\n\n\n\n\nWe can make some “fancier” plots of both our vector (locational points) and raster (sst) data using the ggspatial R package, which adds functions onto ggplot code. This package has access to open-source base-map tiles, as well as functions for adding a north arrow and scale bar to your maps. The zoom argument specifies the resolution of your basemap; this may take some playing with, as it is dependent on the spatial extent/resolution of your data and zooming in too much can take a very long time to plot!\nLet’s start by creating a ggspatial map of just the whale tracks. We use the geom_sf() function to add our spatial whale tracks to the ggspatial map.\n\n## the \"type\" argument let's you choose a basemap type (see \"rosm::osm.types()\" for more)\n## the \"zoom\" argument let's you specify the number of tiles - more tiles = higher resolution but will also take longer to draw! might have to play with this a bit\n\n# let's plot just the tracks first:\nggplot() +\n  annotation_map_tile(type = 'osm', zoom = 7) +\n  annotation_scale()+\n  annotation_north_arrow(height=unit(0.5,\"cm\"), width=unit(0.5,\"cm\"), pad_y = unit(1,\"cm\"))+\n  ylab(\"Latitude\") + xlab(\"Longitude\")+\n  geom_sf(data = sperm_whale_tracks,aes(color=id),size=1)+\n  scale_color_discrete(type = c(\"green\",\"blue\")) + \n  theme(text = element_text(size=12))\n\n\n\n\n\n5.5.6 Transform your data’s CRS\nGenerally, before performing quantitative spatial operations with our data, we need to convert our data CRS from a GCS to a PCS. To do this, you need to find a PCS that corresponds to the area your data was collected in.\nOur whale data came with metadata stating that they belonged to UTM Zone 12N, so we will use this to define our PCS and transform our data, using the st_transform() function from the sf package and the corresponding EPSG code for UTM Zone 12 (32612).\nOur raster data occurs in the same area, so we can also transform its CRS to the same PCS (UTM Zone 12N), but we have to use a function from the raster package, projectRaster().\n\n# convert CRS to planar PRS\nsperm_whales_sf2<- sperm_whales_sf %>%\n  st_transform(4326)\n\nNote - that projecting rasters can take a REALLY long time. As a general rule, it is smarter to\n\n5.5.7 Make a background map using the “mapdata” package\nThe mapdata package has data for world-wide polygons, which we can extract, turn into a spatial objects, and crop to our study area extent. This is great if you just want a polygon of land in the background!\nWe use a slew of functions from the sf package to manipulate the world polygon from mapdata to something we can plot with our other data. We use the “st_cast” function to convert it from a multipolygon to polygon geometry, “st_transform” to change its CRS to a PCS, “st_crop” to crop it to the extent of our projected raster data, and then we transform it back to a GCS so that we can plot it with our whale data.\nWe can, again, use the ggplot2::facet_wrap() function to create separate, side-by-side maps for each unique whale id:\n\n# extract the map data (basically, a bunch of world wide polygons\n# and restrict to Mexico.\n\nw <- map_data(\"worldHires\",ylim = c(-90, 90), xlim = c(-180, 180)) %>% \n  subset(region == \"Mexico\")\n\n# make it into an sf object, change it to a polygon feature, and crop it to the spatial extent of our sst raster data\nw_sf <- w %>%\n  st_as_sf(coords = c(\"long\", \"lat\"), crs = 4326) %>%\n  group_by(group) %>%\n  summarise(geometry = st_combine(geometry)) %>%\n  st_cast(\"POLYGON\") %>%\n  st_crop(extent(sst_mean))\n\n# map it with our whale track\nggplot() +\n  geom_sf(data=w_sf,aes(),color=\"black\",inherit.aes = FALSE) +\n  geom_sf(data=sperm_whale_tracks,aes(color=id),size=0.7) +\n  scale_color_discrete(type=c(\"green\",\"blue\")) + \n  ylab(\"Latitude\")+xlab(\"Longitude\") + theme_classic() +\n  theme(text=element_text(size=12),legend.position = \"none\") +\n  facet_wrap(~id) \n\n\n\n\n\n5.5.8 Map raster and vector data together\nNow that the CRS of our background polygons, whale tracks, and mean sst raster all match (have the same GCS, WGS 1984), we can plot them together! Note also that our “background’ data, the sst raster and cropped world polygon, have the same spatial extent.\nBefore we plot our raster, we should decide how we want assign its values to color bins. If you use the base R “plot” function, it will do this automatically for you but often the bins are not as effective than if you chose them yourself.\nVisualize the distribution of values in your raster data using the “hist” function:\n\nhist(sst_mean)\n\n\n\n\nWe can use this distribution to decide on ~3 bins for our data. The median is close to 30 and the values range from ~ 27-32, so we define our range as 27-32 and our bin color breaks at 28, 29, 30, and 31. We use a yellow-orange-red color scheme with the function “scale_fill_gradientn”, which allows you complete control over assigning color bins to continuous data (also check out “scale_fill_gradient” and “scale_fill_gradient2”, which will bin your data into high/low or high/med/low values).\nWe use the “layer_spatial” function from the ggspatial R package to plot our raster data, telling ggplot that we want to fill the color values of this raster using its values (“stat(band1)”). We then define the colors to fill it with using “scale_fill_gradientn” function.\nWe add our whale tracks, cropped world polygon, and our whale locations, all spatial objects, using the “geom_sf” function from the sf package and assign color values to the locations using the “scale_color_discrete” function.\nFinally, we facet by our different id levels to create side by side maps. Note here that the order you add your layers onto the ggplot code matters. So, here we want our raster data on the bottom, followed by our world polygon, whale tracks, and whale locations, in that order (whale locations are on “top”).\n\n## the \"layer_spatial\" function allows you to plot raster data\n## this time, we also plot the whale location points over the trajectories\nwhale_maps<-ggplot() +\n  layer_spatial(data = sst_mean, aes(fill = stat(band1))) +\n  geom_sf(data=w_sf,aes(),color=\"black\",inherit.aes = FALSE)+\n  geom_sf(data=sperm_whale_tracks,aes(),size=0.7,color=\"grey\")+\n  geom_sf(data=sperm_whales_sf,aes(color=id),size=1)+\n  scale_color_discrete(type=c(\"green\",\"blue\"))+ \n  ylab(\"Latitude\")+xlab(\"Longitude\")+ theme_classic()+\n  # the function below allows you to manually adjust the raster bin values, colors, and legend title\n  scale_fill_gradientn(\"Mean SST (deg C)\",colours = c(\"yellow\",\"orange\",\"red\"), \n                       na.value = NA,limits=c(27,32),\n                       breaks=c(28,29,30,31))+ \n  theme(text=element_text(size=12))+\n  facet_wrap(~id)\n\nwhale_maps\n\n\n\n\n\n5.5.9 Export your map\nNow that you have created this beautiful map, you likely want to save it somewhere. We can export images/plots/maps from R into a variety of formats (tiff, png, jpeg…) and specify both the size and quality of these images.\nWe will export our map to a jpeg with a 300 dpi (high) resolution and 6x3 inches size:\n\njpeg(file=\"./Whale_Maps.jpg\", units=\"in\", width=6, height=3,res=300)\nwhale_maps\ndev.off()\n\n\n5.5.10 Combining vector polygons and raster basemaps\nWe will now demonstrate another R package you can use to map raster and vector data together, the “basemaps” package. This package, similar to ggspatial, allows you to grab interesting, open-source basemaps (such as rasters) for your data. You can change the map resolution using the map_res argument.\nFor this, we will plot our caribou calving range data (polygon) with our Kobuk river shapefile (line vector). First, we create a bounding box for our data, which defines the spatial boundaries of our map. We then extract our basemap to this bounding box, specify the map type (“topographic”), and plot this raster base map with our vector data. We use the function plotRGB() to plot our background map, a topographic raster. This function creates a red-blue-green (3 base colors) plot, a great function for raster data based on color images (e.g, Landsat data).\nTo add our vector data, we first grab only the geometry values of each vector, using the st_geometry() function from the sf package.\n\n# create a \"bbox\" or spatial plotting limits for your map\next <- data.frame(x = c(-170, -148), y = c(65, 72)) %>% \n  st_as_sf(coords=c(\"x\",\"y\"), crs = 4326) %>% \n  st_transform(3857) %>%\n  st_bbox \n\n# a different mapping package and function, also using the open-source osm tiles\n# download the basemap\nmap <- basemap_raster(ext, map_service = \"osm\", map_type=\"topographic\",map_res=0.3)\n\nLoading basemap 'topographic' from map service 'osm'...\n\n# plot\nplotRGB(map) # function to plot rasters with color images (e.g. Landsat data)\nplot(st_geometry(kobuk), add=TRUE)\nplot(st_geometry(calving_ranges)[2], add = TRUE, col = alpha(\"darkblue\", .5))\nplot(st_geometry(calving_ranges)[1], add = TRUE, col = alpha(\"purple\", .5))\n\n\n\n\nWe can create an additional, interactive map of this data using the mapview package again. This time, we specify to use the ESRI shaded relief (topographic) background map:\n\nmapview(calving_ranges,map.types = c(\"Esri.WorldShadedRelief\"),alpha.regions=0.2,col.regions=\"orange\")+\nmapview(kobuk,col.regions=\"blue\")\n\n\n\n\n\n\n\n5.5.11 Spatial operations and manipulations\nIn the next sub-sections, we will demonstrate some examples of how to do spatial operations and manipulations with your spatial data in R.\n\n5.5.11.1 Buffering data (st_buffer)\nThere are many reasons why you may want to create a buffer around your points. Below, we demonstrate how to create a 5000 m (5 km) buffer around your projected whale data, using the st_buffer() function. We visualize the results using an interactive map through mapview:\n\n# first transform to PRG\nsperm_whales_sf_2<- sperm_whales_sf %>% st_transform(32612)\n\n# put 5 km buffer around points\nsperm_whales_buff <-sperm_whales_sf_2 %>% st_buffer(5000) \n\n# look at str\nsperm_whales_buff\n\nSimple feature collection with 191 features and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 246040.3 ymin: 2948880 xmax: 611840.3 ymax: 3298168\nProjected CRS: WGS 84 / UTM zone 12N\nFirst 10 features:\n             timestamp      id                       geometry\n1  2008-07-02 05:03:00 4800837 POLYGON ((375407.4 3143223,...\n2  2008-07-04 02:49:00 4800837 POLYGON ((376950.7 3167917,...\n3  2008-07-04 04:51:00 4800837 POLYGON ((351923 3153146, 3...\n4  2008-07-06 01:22:00 4800837 POLYGON ((325310.5 3179903,...\n5  2008-07-06 05:44:00 4800837 POLYGON ((351383.5 3156810,...\n6  2008-07-08 08:26:00 4800837 POLYGON ((349191.7 3161272,...\n7  2008-07-10 03:59:00 4800837 POLYGON ((323862.6 3181034,...\n8  2008-07-12 01:59:00 4800837 POLYGON ((296722.4 3203883,...\n9  2008-07-02 00:30:00 4810843 POLYGON ((283918.9 3217649,...\n10 2008-07-02 03:27:00 4810843 POLYGON ((367606.1 3121037,...\n\n# plot it and zoom in!\nmapview(sperm_whales_buff,alpha.regions=0.1,col.regions=\"red\")+\n  mapview(sperm_whales_sf_2,col.regions=\"blue\")\n\n\n\n\n\n\n\n5.5.11.2 Combine polygons (st_union)\nIf you want all of the buffers around the whale locations to have fluid/merged boundaries, you can unionize the individual polygons with “st_union” (also see “st_combine”), changing our individual polygons into one giant multipolygon.\n\nsperm_whales_buff2<-sperm_whales_buff %>% st_union()\n\n# look at the str\nsperm_whales_buff2\n\nGeometry set for 1 feature \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 246040.3 ymin: 2948880 xmax: 611840.3 ymax: 3298168\nProjected CRS: WGS 84 / UTM zone 12N\n\n# plot it and zoom in!\nmapview(sperm_whales_buff2,alpha.regions=0.1,col.regions=\"red\") + \n  mapview(sperm_whales_sf_2,col.regions=\"blue\")\n\n\n\n\n\n\n\n5.5.11.3 Convert polygons to rasters and vice versa\nThere are times when you may want to convert your polygon(s) into rasters or vice versa.\nBefore converting our sperm whale, multipolygon buffer into a raster, we need to add a column with some kind of variable (here, we make up a variable and set all values = 1). If your polygons had some kind of information associated with them (e.g., id name, other attribute) that you wanted to use to create a raster with, you could use that column instead. Rasters are grids of numbers so they need some kind of numbers to plot!\nWe transform our CRS back to a GCS, before creating a blank raster (“r”) and defining its extent, CRS, and resolution. We set the extent and CRS equivalent to that of our transformed whale buffer object. We specify the resolution of our raster to be equal to our sst raster data, 0.01 deg (~ 1 km). Keep in mind that the units for the resolution should be the same as your CRS… so if you are defining your raster with a GCS, the units should be in angular degrees (as here) and if you are defining it with a PCS (e.g., UTM), your units should match the units of the PCS (e.g., we would set our resolution to 1000 to be 1 km or 1000 m resolution).\nWe then use the handy little function “fasterize” from the “fasterize” R package to convert our sperm whale buffer to a raster, using our blank raster (“r”) as a template. If your data is either a dataframe/matrix or SpatialPoints/SpatialLines/SpatialPolygon object (instead of sf), you can use the “rasterize” function from the raster package to convert your vector data to a raster.\n\n# first, we need to add a data frame column to our data so the function has something to \"grab\" onto\n## also change back to CRS\nsperm_whales_buff3 <- st_sf(var = 1, sperm_whales_buff2) %>% \n  st_transform(4326)\n\n# first we create a blank raster \nr<-raster::raster() \nextent(r)<-extent(extent(sperm_whales_buff3)) # set the extent to the same as polygons\ncrs(r)<-crs(sperm_whales_buff3) # set the crs the same as polygons\nres(r)<-0.01 # set res to 0.01 deg (~ 1 km)\n\n# look at the str\nr\n\nclass      : RasterLayer \ndimensions : 314, 374, 117436  (nrow, ncol, ncell)\nresolution : 0.01, 0.01  (x, y)\nextent     : -113.6124, -109.8724, 26.65309, 29.79309  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \n\n# convert to a raster using \"fasterize\"\n## you can also use the \"rasterize\" function for non-sf data\nsperm_whales_buff_r<-fasterize(sperm_whales_buff3,r)\n\n# look at the str\nsperm_whales_buff_r\n\nclass      : RasterLayer \ndimensions : 314, 374, 117436  (nrow, ncol, ncell)\nresolution : 0.01, 0.01  (x, y)\nextent     : -113.6124, -109.8724, 26.65309, 29.79309  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nsource     : memory\nnames      : layer \nvalues     : 1, 1  (min, max)\n\n# plot it!\nplot(sperm_whales_buff_r)\n\n\n\n\nIf you wanted to convert your raster back to polygons, you can use the “rasterToPolygons” function from the raster package (see also “rasterToPoints” and “rasterToContour” functions). You can specify not to include NA values in this conversion (can speed up processing time) and you can use the “dissolve=TRUE” argument to combine polygons with the same attributes to create a multipolygon.\nNote here that the output is a SpatialPolygons object, not an sf object (the raster package does not work fluidly with the sf package unfortunately). Note also that the output polygon has jagged instead of smooth edges - that’s because rasters are a bunch of square cells! This issue (smooth boundaries of vectors, square edges of rasters) can have consequences when performing other spatial operations, such as extracting the values of either a raster or polygon to over-lying points, so keep this in mind.\n\n## setting dissolve = TRUE will allow you to combine polygons with the same attributes into multi-polygons\nsperm_whales_buff_poly <- rasterToPolygons(sperm_whales_buff_r, na.rm=TRUE, dissolve = TRUE)\n\n# look at the str\nsperm_whales_buff_poly\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 1 \nextent      : -113.6124, -109.8724, 26.65309, 29.79309  (xmin, xmax, ymin, ymax)\ncrs         : +proj=longlat +datum=WGS84 +no_defs \nvariables   : 1\nnames       : layer \nvalue       :     1 \n\n# plot it!\nmapview(sperm_whales_buff_poly,col.regions=\"yellow\")+\n  mapview(sperm_whales_sf,col.regions=\"blue\")\n\n\n\n\n\n\n\n5.5.11.4 Change raster resolution\nYou may need to change the resolution of your raster to match another spatial layer or desired resolution. Just keep in mind that this can result in issues with extrapolation, data loss, etc!\nIf you want to coarsen the resolution of your raster data, you can use the raster::aggregate() function (see also the raster::resample()). Here, you specify what factor to magnify your resolution to (e.g., if we use a factor of 5 and our original resolution was 0.01 deg, our new resolution will be 0.05 deg).\nIf you want to do additional operations with your raster data as you aggregate it, e.g., take the mean of raster values within each new, larger cell, you can use the fun argument in this function.\n\n## you can also use the \"fun\" argument to do operations with the values of cell\nsperm_whales_buff_agg <- aggregate(sperm_whales_buff_r, fact=5)\n\n# look at the str\nsperm_whales_buff_agg\n\nclass      : RasterLayer \ndimensions : 63, 75, 4725  (nrow, ncol, ncell)\nresolution : 0.05, 0.05  (x, y)\nextent     : -113.6124, -109.8624, 26.64309, 29.79309  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nsource     : memory\nnames      : layer \nvalues     : 1, 1  (min, max)\n\n# plot it!\nplot(sperm_whales_buff_agg)\n\n\n\n\nIf instead you want to decrease the resolution of your raster, you can use a similar function, “disaggregate”, again specifying the factor amount to disaggregate by (e.g., if your resolution is 0.01 deg and you want to use a factor of 5, your new resolution will be 0.002 deg).\n\nsperm_whales_buff_dagg<-disaggregate(sperm_whales_buff_r,fact=5)\n\n# look at the str\nsperm_whales_buff_dagg\n\nclass      : RasterLayer \ndimensions : 1570, 1870, 2935900  (nrow, ncol, ncell)\nresolution : 0.002, 0.002  (x, y)\nextent     : -113.6124, -109.8724, 26.65309, 29.79309  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nsource     : memory\nnames      : layer \nvalues     : 1, 1  (min, max)\n\n# plot it!\nplot(sperm_whales_buff_dagg)\n\n\n\n\n\n5.5.11.5 Extract values of rasters or polygons to points\nIf you want to extract the values of your underlying abiotic data (either vector or raster) to your locational data, you can either perform a spatial join (vector data) or you can extract values (raster data).\nSpatial joins are similar to joins with non-spatial data (left join = all rows in x, (inner join = all rows in x and y, right join = all rows in y, full join = all rows in x or y).\nHere, we perform a left spatial join between our projected sperm whale location data and our projected whale buffer polygon data. We then transform the CRS of the resulting object back to a GCS. Because we performed a left join, the resulting object has the same vector type as x (whale locations) but now there is an additional column (“layer”) that holds the values of the polygons overlapping in space with those locations.\n\n# convert sp to sf\nsperm_whales_buff_poly2<- sperm_whales_buff_poly %>% \n  st_as_sf() %>% \n  st_transform(32612)\n\nsperm_whales_join<-st_join(sperm_whales_sf_2,sperm_whales_buff_poly2,left=TRUE) %>% \n  st_transform(4326)\n\nsperm_whales_join\n\nSimple feature collection with 191 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -113.561 ymin: 26.702 xmax: -109.926 ymax: 29.748\nGeodetic CRS:  WGS 84\nFirst 10 features:\n             timestamp      id layer                geometry\n1  2008-07-02 05:03:00 4800837     1 POINT (-112.323 28.409)\n2  2008-07-04 02:49:00 4800837     1  POINT (-112.31 28.632)\n3  2008-07-04 04:51:00 4800837     1 POINT (-112.564 28.496)\n4  2008-07-06 01:22:00 4800837     1  POINT (-112.84 28.734)\n5  2008-07-06 05:44:00 4800837     1  POINT (-112.57 28.529)\n6  2008-07-08 08:26:00 4800837     1 POINT (-112.593 28.569)\n7  2008-07-10 03:59:00 4800837     1 POINT (-112.855 28.744)\n8  2008-07-12 01:59:00 4800837     1 POINT (-113.137 28.946)\n9  2008-07-02 00:30:00 4810843     1 POINT (-113.271 29.068)\n10 2008-07-02 03:27:00 4810843     1   POINT (-112.4 28.208)\n\n\nWe can additionally extract underlying raster data to points, using the “extract” function from the raster package.\nWe can use the “extract” function with either a dataframe or an sf object. To demonstrate both, we first extract the locational information of our sperm whale points using the “st_coordinates” function from the sf package and store it as an object. We can also convert our sperm whales spatial data back to a dataframe and then add individual columns back for latitude/longitude by extracting the values from the “coords” matrix object.\nWe can then use the “extract” function to extract raster values to those values, using either the “coords” object or the new whale dataframe object. The resulting object is a vector, that we have to manually add back as a column to our dataframe (we can also add it to our sf data).\n\n#extract coordinates from whale location spatial data\ncoords<-st_coordinates(sperm_whales_sf)\n\n# convert spatial whale data back to data frame\nsperm_whales_df<-sperm_whales_sf %>% data.frame() %>% \n  mutate(lon=coords[,1],lat=coords[,2]) # add lat and long to dataframe as separate columns, grabbing them from the coords matrix (x=lon,y=lat)\n\n# look at the str\nstr(sperm_whales_df)\n\n'data.frame':   191 obs. of  5 variables:\n $ timestamp: POSIXct, format: \"2008-07-02 05:03:00\" \"2008-07-04 02:49:00\" ...\n $ id       : Factor w/ 25 levels \"4400829\",\"4400837\",..: 23 23 23 23 23 23 23 23 24 24 ...\n $ geometry :sfc_POINT of length 191; first list element:  'XY' num  -112.3 28.4\n $ lon      : Named num  -112 -112 -113 -113 -113 ...\n  ..- attr(*, \"names\")= chr [1:191] \"1\" \"2\" \"3\" \"4\" ...\n $ lat      : Named num  28.4 28.6 28.5 28.7 28.5 ...\n  ..- attr(*, \"names\")= chr [1:191] \"1\" \"2\" \"3\" \"4\" ...\n\n# method 1: using coords\nsperm_whales_extract<-extract(sperm_whales_buff_r,coords)\n# look at the str\nsperm_whales_extract\n\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[112] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[149] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[186] 1 1 1 1 1 1\n\n# can add as a column to our sperm whales df object:\nsperm_whales_df$raster_values<-sperm_whales_extract\n\nstr(sperm_whales_df)\n\n'data.frame':   191 obs. of  6 variables:\n $ timestamp    : POSIXct, format: \"2008-07-02 05:03:00\" \"2008-07-04 02:49:00\" ...\n $ id           : Factor w/ 25 levels \"4400829\",\"4400837\",..: 23 23 23 23 23 23 23 23 24 24 ...\n $ geometry     :sfc_POINT of length 191; first list element:  'XY' num  -112.3 28.4\n $ lon          : Named num  -112 -112 -113 -113 -113 ...\n  ..- attr(*, \"names\")= chr [1:191] \"1\" \"2\" \"3\" \"4\" ...\n $ lat          : Named num  28.4 28.6 28.5 28.7 28.5 ...\n  ..- attr(*, \"names\")= chr [1:191] \"1\" \"2\" \"3\" \"4\" ...\n $ raster_values: num  1 1 1 1 1 1 1 1 1 1 ...\n\n# method 2: using sf object\nsperm_whales_extract<-extract(sperm_whales_buff_r,sperm_whales_sf)\n# look at the str\nsperm_whales_extract\n\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[112] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[149] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[186] 1 1 1 1 1 1\n\n# can add as a column to our sperm whales sf object:\nsperm_whales_sf$raster_values<-sperm_whales_extract\n\nstr(sperm_whales_sf)\n\nClasses 'sf' and 'data.frame':  191 obs. of  4 variables:\n $ timestamp    : POSIXct, format: \"2008-07-02 05:03:00\" \"2008-07-04 02:49:00\" ...\n $ id           : Factor w/ 25 levels \"4400829\",\"4400837\",..: 23 23 23 23 23 23 23 23 24 24 ...\n $ geometry     :sfc_POINT of length 191; first list element:  'XY' num  -112.3 28.4\n $ raster_values: num  1 1 1 1 1 1 1 1 1 1 ...\n - attr(*, \"sf_column\")= chr \"geometry\"\n - attr(*, \"agr\")= Factor w/ 3 levels \"constant\",\"aggregate\",..: NA NA NA\n  ..- attr(*, \"names\")= chr [1:3] \"timestamp\" \"id\" \"raster_values\"\n\n\n\n5.5.11.6 Determine distance to an object\nIf you want to determine distance to an object, let’s say the distance between each of our whale spatial locations and some point, you can use the st_distance() function from the sf package. Make sure data CRS has been transformed to a PCS first!\nBelow, we first create a spatial point vector (27 deg lat, -111 deg long) and convert its CRS to the matching PCS of our projected whale spatial locations. We visualize where this point is with reference to our whale data using the mapview package:\n\n# create a point with same crs\npoint <- data.frame(lon=-111,lat=27) %>% \n  st_as_sf(coords=c(\"lon\",\"lat\"), crs=4326) %>%\n  st_transform(32612)\n\nmapview(point,col.regions=\"red\")+\n  mapview(sperm_whales_sf_2)\n\n\n\n\n\n\nThe st_distance() function returns the distance between each of our whale locations and this point, in the units of your PRS (here, meters). We can then bind this data (a vector) back to our sf data as an additional column. Note that you can use this function for other geometry types as well (points, polygons, lines…).\nFor some other interesting geometric operations you can perform with sf objects, also see st_area() (get the area of an sf object) and st_length() (get the length of an sf line object).\n\n## return distance in units of PRS (meters)\nsperm_whales_sf_2$dist_to_point <- st_distance(sperm_whales_sf_2,point)\nstr(sperm_whales_sf_2)\n\nClasses 'sf' and 'data.frame':  191 obs. of  4 variables:\n $ timestamp    : POSIXct, format: \"2008-07-02 05:03:00\" \"2008-07-04 02:49:00\" ...\n $ id           : Factor w/ 25 levels \"4400829\",\"4400837\",..: 23 23 23 23 23 23 23 23 24 24 ...\n $ geometry     :sfc_POINT of length 191; first list element:  'XY' num  370407 3143223\n $ dist_to_point: Units: [m] num [1:191, 1] 203412 222109 226329 264042 229403 ...\n - attr(*, \"sf_column\")= chr \"geometry\"\n - attr(*, \"agr\")= Factor w/ 3 levels \"constant\",\"aggregate\",..: NA NA NA\n  ..- attr(*, \"names\")= chr [1:3] \"timestamp\" \"id\" \"dist_to_point\"\n\n\nAn additional method you can use is with the “locator” function. You can use this function to interact with static plots (see ?locator).\nFor example, you can create an sf point, plot it over your sst raster, and then type “locator()” into your console. Now you can interact with your plot! Click on two locations (e.g. the red point and another spot), then hit your “Esc” button and you will get the locations of each of these points. You can then use your “st_distance” function to find the distance between these points (or you can use the geosphere package, which we will bring up next…). We include example code for this below but you will want to run this code in your console:\n\npoint<- data.frame(lon=-111,lat=27) %>%\n  st_as_sf(coords=c(\"lon\",\"lat\"), crs=4326)\n\nplot(sst_mean)\nplot(st_geometry(point),add=TRUE,col=\"red\")\n\nlocator()\n\nAlternatively, you can save your click data automatically to an object (e.g., “data_click”) by preëmptively putting the number of clicks into the locator function (here, 2). After you click twice, it will automatically save your click locations into your object. We show example code for this below:\n\nplot(sst_mean)\nplot(st_geometry(point),add=TRUE,col=\"red\")\n\n# click twice...\ndata_click <- locator(2)\ndata_click\n\n# convert to a sf object\ndata_click_sf<- data_click %>% \n  data.frame() %>% \n  st_as_sf(coords=c(\"x\",\"y\"), crs=4326) %>%\n  st_transform(32612)\n\ndata_click_sf\n\n# extract distance between points\ndist<-st_distance(data_click_sf)\n\n\n5.5.11.7 Spherical trigonometry with the geosphere package\nIf you want to perform spatial operations using spherical trigonometry and without transforming your data to a PCS, the geosphere package can be used. This package has a variety of useful functions for spatial data, including destPoint() (determine the next location given the distance and bearing or direction of travel), “bearing” (determine the direction of travel or bearing for the first location, given two successive locations), distm() (determine the distance between locations), and dist2Line() (determine the distance between points and lines or points and polygons).\nFor example, we can extract our whale location coordinate data using st_coordinates() and then use destPoint() to determine the location of a point that is 5000 m (5 km) and a bearing of 50 deg from each location in our data:\n\n# extract coordinates \ncoords <- st_coordinates(sperm_whales_sf)\n\n# specify bearing in degrees, distance traveled in meters\ndest_point <- destPoint(coords, d=5000, b=50)\n\n# change to a spatial object\ndest_point_sf<-dest_point %>% data.frame() %>%\n  st_as_sf(coords=c(\"lon\",\"lat\"), crs=4326)\n\n# plot with our original points:\nplot(st_geometry(sperm_whales_sf))\nplot(st_geometry(dest_point_sf), add=TRUE, col=\"red\")\n\n\n\n\nWe can also use the bearing() function to determine the direction of travel between points. Note that the last location does not have a direction of travel to calculate. As such, it has an NA value at the end of the output vector.\n\n# get the initial bearing (direction of travel, degrees) between points  \ndest_bearing <- bearing(coords)\nstr(dest_bearing)\n\n num [1:191] 2.94 -121.18 -45.59 130.65 -26.91 ...\n\n\nFinally, you can use the distm() function to determine the shortest distance between points, with the default being to find this distance on the WGS1984 ellipsoid. The output has units in meters.\n\n# determine distance between points\ndist_traveled<-distm(coords)\nstr(dist_traveled)\n\n num [1:191, 1:191] 0 24747 25499 62096 27604 ...\n\n\nIn the next chapter, you will learn how to manipulate and process movement data, which is inherently spatial and relies on many of the skills you just gained from this chapter."
  },
  {
    "objectID": "spatial_data_in_R.html#references",
    "href": "spatial_data_in_R.html#references",
    "title": "\n5  Spatial data in R\n",
    "section": "\n5.6 References",
    "text": "5.6 References\n\n\n\n\nWittemyer, George, Joseph M. Northrup, and Guillaume Bastille-Rousseau. 2019-07-29, 2019-07. “Behavioural Valuation of Landscapes Using Movement Data.” Philosophical Transactions of the Royal Society B: Biological Sciences 374 (1781): 20180046. https://doi.org/10.1098/rstb.2018.0046."
  },
  {
    "objectID": "movement_data_in_R.html#preamble",
    "href": "movement_data_in_R.html#preamble",
    "title": "\n6  Movement data in R\n",
    "section": "\n6.1 Preamble",
    "text": "6.1 Preamble\nThe goal of this tutorial is to provide the tools to comprehend the structure of certain common types of animal movement data, as well as to load, process and visualize movement data using R in Rstudio.\nIn particular we will be accessing data from Movebank, a free, online database of animal tracking data, where users can store, process, and share (or not share) location data. In the diverse and highly fragmented world of animal movement ecology, it is the closest to a “standard” format for movement data.\nFor this tutorial, several packages for downloading, processing and visualizing data are required. We will use: - plyr - dplyr - magrittr - lubridate - move - sf - mapview - ggplot2. Notably, the move package allows you to access and process data from Movebank directly from R.\nInstall and load these packages, for example via:\n\n# create packages list\npackages <- list(\"plyr\",\"dplyr\",\"magrittr\", \n                 \"lubridate\",\"move\",\"sp\",\"sf\", \n                 \"mapview\",\"ggplot2\")\nsapply(packages, require, character = TRUE)"
  },
  {
    "objectID": "movement_data_in_R.html#movebank-data",
    "href": "movement_data_in_R.html#movebank-data",
    "title": "\n6  Movement data in R\n",
    "section": "\n6.2 Movebank Data",
    "text": "6.2 Movebank Data\nHere as an example we will use elk (Cervus elaphus1) movement data from the Ya Ha Tinda Long-Term Elk Monitoring Project (Hebblewhite et al. 2020), which are hosted on Movebank (https://www.movebank.org). The dataset that we are using is a subset of the data, and contains GPS locations of 10 female Elk from Banff National Park, in Alberta. You can access the data as a .csv file at this link. But it is worth visiting MoveBank, getting an account, and learning how to navigate the website. For example, you can log in on MoveBank, clicking Data \\> Studies, and changing from Studies where I am the Data Manager to All studies, and search “Ya Ha Tinda elk project, Banff National Park, 2001-2020 (females)” . Click on Download > Download data and follow further instructions.1 Or is it Cervus canadensis?\n\n6.2.1 Loading movement data in R\nIf you are working from data on Movebank - including data with controlled or limited access that you have access to - you can load the data with the move::getMovebankData() function [^1: recall, the move:: notation just means that the function comes from the move package] using your own login and password. This is a two step process. First you have to create an object that contains your login information:\n\nrequire(move)\nmylogin <- movebankLogin(username='XXX', password='XXX')\n\nReplacing the XXX’s with your private information. Next, you use that login in the getMovebankData function:\n\nelk_move <- getMovebankData(\n  study = \"Ya Ha Tinda elk project, Banff National Park, 2001-2020 (females)\",\n  login = mylogin, removeDuplicatedTimestamps=TRUE)\n\nThis step might take a few minutes, since the data contains more than 1.5 million observations (GPS locations) and it depends on the quality and the speed of your Internet Connection.\nThe resulting object is a MoveStack, which can be converted to a data.frame using the as.data.frame() function. Not that the as.data.frame() function convert the object to a data.frame by extracting hidden elements from the MoveStack object. data.frame(), on the contrary, would create a data.frame of the MoveStack object using only the visible elements.\n\nelk_df <- as.data.frame(elk_move)\n\nFor all the examples of this chapter, we will only use a subset of the Ya Ha Tinda data set (i.e., 10 random individuals) and select only the columns we are interested in and rename them for simplicity.\n\nelk_df <- elk_df %>% mutate(ID = trackId, Time = timestamp, Lon = location_long, Lat = location_lat) %>% subset(select = c(\"ID\", \"Time\", \"Lon\", \"Lat\", \"sex\", \"number_of_events\"))%>% subset(ID %in% sample(unique(ID), 10, replace = FALSE))\n\nThe coordinate reference system of the data set is WGS84. To add coordinates in a metric system, we will convert the data.frame to a sf (i.e., simple feature), using the Lon,Lat coordinates, with the coordinate reference system EPSG:4326. We then reproject to a metric coordinate system (here the UTM Zone 10N, EPSG: 32610; see chapter ?sec-visualization for more details).\n\nelk_df <- elk_df %>% \n  st_as_sf(coords = c(\"Lon\", \"Lat\"), crs = 4326) %>% \n  mutate(Lon = st_coordinates(.)[,1], Lat = st_coordinates(.)[,2]) %>% \n  st_transform(crs = 32610) %>% \n  mutate(X = st_coordinates(.)[,1], Y = st_coordinates(.)[,2]) %>% \n  as.data.frame %>% mutate(geometry = NULL)\n\n\n6.2.2 Load the data from your computer\nIf you downloaded the data on your computer, from the website. You can use the read.csv() function to open it on R, using the path of the file on your Disk.\n\nelk_df <- read.csv(\"data/elk.csv\")\n\n\n6.2.3 Data structure\nNow that we loaded the data, we can explore its structure, dimensions, data types, etc. The following functions all provide some important or, at least, useful information:\nThe “class” of the object is given by looking at the object in the Environment on the right, or the class() or, more informative, is() functions:\n\nclass(elk_df)\n\n[1] \"data.frame\"\n\nis(elk_df)\n\n[1] \"data.frame\" \"list\"       \"oldClass\"   \"vector\"    \n\n\nWhen using the getMovebankData() function, the resulting object is a MoveStack. We can use the as.data.frame() (or just data.frame()), to transform it into a data frame and see all the elements of the object. If you loaded the data from your computer, it is a data.frame.\nThe dimensions (number or rows and columns), by dim().\n\ndim(elk_df)\n\n[1] 97825     7\n\n\nThe names of the columns with names():\n\nnames(elk_df)\n\n[1] \"ID\"   \"Time\" \"sex\"  \"Lon\"  \"Lat\"  \"X\"    \"Y\"   \n\n\nThe first few rows with head(), useful for a quick look:\n\nhead(elk_df)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nTime\nsex\nLon\nLat\nX\nY\n\n\n\nGR122\n2006-03-06 16:22:41\nf\n-115.5828\n51.75451\n597820.5\n5734685\n\n\nGR122\n2006-03-06 19:01:07\nf\n-115.5830\n51.75455\n597806.6\n5734689\n\n\nGR122\n2006-03-06 23:01:48\nf\n-115.5865\n51.73794\n597600.9\n5732837\n\n\nGR122\n2006-03-07 03:01:46\nf\n-115.5774\n51.73691\n598231.4\n5732735\n\n\nGR122\n2006-03-07 11:01:46\nf\n-115.5828\n51.75442\n597820.7\n5734675\n\n\nGR122\n2006-03-07 15:01:05\nf\n-115.5828\n51.75447\n597820.6\n5734680\n\n\n\n\n\nMovement data at the very minimum contains x and y (here, columns X and Y) or longitude/latitude (Lon and Lat) coordinates, and an associated timestamp (Time). If there is more than one individual, there will also be a unique identifier (ID). Additional columns might also be provided, as here with the sex of the individual.22 A warning: you might have loaded data with the readr::read_csv() function instead of read.csv() - that is the default behavior if you click on a data file in newer versions of Rstudio, for example. This function will convert all of the sex column to the Boolean FALSE - implying that all of these elk are abstinent. This is an example of new and improved versions of software occasionally being a wee bit too smart for their own good.\nMore detail about the structure of the data frame is given by str():\n\nstr(elk_df)\n\n'data.frame':   97825 obs. of  7 variables:\n $ ID  : chr  \"GR122\" \"GR122\" \"GR122\" \"GR122\" ...\n $ Time: chr  \"2006-03-06 16:22:41\" \"2006-03-06 19:01:07\" \"2006-03-06 23:01:48\" \"2006-03-07 03:01:46\" ...\n $ sex : chr  \"f\" \"f\" \"f\" \"f\" ...\n $ Lon : num  -116 -116 -116 -116 -116 ...\n $ Lat : num  51.8 51.8 51.7 51.7 51.8 ...\n $ X   : num  597821 597807 597601 598231 597821 ...\n $ Y   : num  5734685 5734689 5732837 5732735 5734675 ...\n\n\nWe see that all of the variables are character or numeric. For easy manipulation and visualization, it is more convenient if ID and sex are factors. To do so, we can use the mutate function, from the plyr package, which allows to create, delete or modify columns.\n\n# set ID and sex as factors\nelk_df <- elk_df %>% mutate(ID = as.factor(ID), sex = as.factor(sex))\n\n\n6.2.4 Manipulating time\nIn addition, the characteristic of movement data is that they are spatio-temporal. This means that each GPS location is associated to a Date and Time. In our case, the Time column is a character object. The function as.POSIXct() converts a time column (e.g., considered as a character) into a POSIXct object - a standard computer format for temporal data (see ?as.POSIXct for more information). Note that the Date in R is always in the format Year-Month-Day Hour:Minute:Second (yyyy-mm-dd hh:MM:ss). Similar to transforming our variables in factors, we use the plyr::mutate() function to transform Time into POSIXct format.\n\nelk_df <- elk_df %>% mutate(Time = as.POSIXct(Time, tz = \"UTC\"))\n\nThis can also be done with some generally more easy-to-use lubridate package functions. For example: ymd_hms() will convert from this format to POSIX:\n\nelk_df <- elk_df %>% mutate(Time = ymd_hms(Time, tz = \"UTC\"))\n\nwhereas if the data were loaded from the original file in some obnoxious format, like the U.S. standard mm/dd/yyyy, this could be converted via:\n\nelk_df <- elk_df %>% mutate(Time = mdy_hms(Time, tz = \"UTC\"))\n\nand so on.\nThe lubridate also offers useful functions for extracting infromation from POSIX times. For example, the hour of day (hour()), the day of year (yday()), the month (month() or the year (year()). A convenient feature of mutate() is that multiple just commands can be strung together in a single command, for example:\n\nelk_df <- elk_df %>% mutate(doy = yday(Time),\n                            Month = month(Time),\n                            Year = year(Time))\nhead(elk_df)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nTime\nsex\nLon\nLat\nX\nY\ndoy\nMonth\nYear\n\n\n\nGR122\n2006-03-06 16:22:41\nf\n-115.5828\n51.75451\n597820.5\n5734685\n65\n3\n2006\n\n\nGR122\n2006-03-06 19:01:07\nf\n-115.5830\n51.75455\n597806.6\n5734689\n65\n3\n2006\n\n\nGR122\n2006-03-06 23:01:48\nf\n-115.5865\n51.73794\n597600.9\n5732837\n65\n3\n2006\n\n\nGR122\n2006-03-07 03:01:46\nf\n-115.5774\n51.73691\n598231.4\n5732735\n66\n3\n2006\n\n\nGR122\n2006-03-07 11:01:46\nf\n-115.5828\n51.75442\n597820.7\n5734675\n66\n3\n2006\n\n\nGR122\n2006-03-07 15:01:05\nf\n-115.5828\n51.75447\n597820.6\n5734680\n66\n3\n2006\n\n\n\n\n\nOur data set is more or less as we want it. Depending on your research questions, you might have additional variables, and you should make sure that those variables are in the correct format. It is also worth noting that once you know what your raw data set looks like and you know how it should look like before starting processing, all those manipulations (loading the data, redefining the class of the columns, adding new columns) can be done in one line of code:\n\nelk_df <- read.csv(\"data/elk.csv\") %>% \n  mutate(ID = as.factor(ID), sex = as.factor(sex),\n         Time = ymd_hms(Time, tz = \"UTC\"),\n         doy = yday(Time), Month = month(Time), Year=year(Time))"
  },
  {
    "objectID": "movement_data_in_R.html#exploring-raw-movement-data",
    "href": "movement_data_in_R.html#exploring-raw-movement-data",
    "title": "\n6  Movement data in R\n",
    "section": "\n6.3 Exploring raw movement data",
    "text": "6.3 Exploring raw movement data\nNow that the data set is more manageable, we can explore the data, and especially look at the number of individuals, their sex, the duration of their monitoring, the fix rate, the number of missing data, etc.\nHowever, as movement data is a time series, it is important when manipulating to FIRST order it by Individual and Time. The function arrange from the plyr (or dplyr) package is very handy:\n\nelk_df <- elk_df %>% arrange(ID, Time)\n\nHere, we walk through a sequence of some basic questions to be asked of movement data. s\n\n6.3.1 How many individuals are there?\nAnd what are their ID?\n\nlength(unique(elk_df$ID)) \n\n[1] 10\n\nunique(elk_df$ID)\n\n [1] GR122       GR133       GR159       GR513       OR39        YL115_OR34 \n [7] YL121_BL250 YL5         YL77        YL89       \n10 Levels: GR122 GR133 GR159 GR513 OR39 YL115_OR34 YL121_BL250 YL5 ... YL89\n\n\n\n6.3.2 How many females/males?\nThere are several “base” R ways to count the males and females. For example the following three commands all give (essentially) the same output:\n\nunique(elk_df[,c(\"sex\",\"ID\")])\nunique(data.frame(elk_df$sex, elk_df$ID))\nwith(elk_df, unique(data.frame(sex, ID)))\n\n\n\n\n\n\nsex\nID\n\n\n\n1\nf\nGR122\n\n\n21568\nf\nGR133\n\n\n39147\nf\nGR159\n\n\n45195\nf\nGR513\n\n\n59780\nf\nOR39\n\n\n67409\nf\nYL115_OR34\n\n\n83906\nf\nYL121_BL250\n\n\n87390\nf\nYL5\n\n\n90251\nf\nYL77\n\n\n92890\nf\nYL89\n\n\n\n\n\ni.e., a unique combination of sex and ID. An (uninteresting) count of those sexes is given by:\n\nid.sex.count <- unique(elk_df[,c(\"sex\",\"ID\")])\ntable(id.sex.count$sex)\n\n\n f \n10 \n\n\nA more trendy approach is to use functions in the dplyr package, which allows you to summarize information per group.\n\nelk_df %>% group_by(ID) %>% summarize(sex = unique(sex))\n\n# A tibble: 10 × 2\n   ID          sex  \n   <fct>       <fct>\n 1 GR122       f    \n 2 GR133       f    \n 3 GR159       f    \n 4 GR513       f    \n 5 OR39        f    \n 6 YL115_OR34  f    \n 7 YL121_BL250 f    \n 8 YL5         f    \n 9 YL77        f    \n10 YL89        f    \n\n\nThe resulting object is a tibble which is very similar to a data frame, but the preferred output of dplyr manipulations.\n\n6.3.3 How many locations per individual?\nTo look at the average number of GPS locations per individual and other statistics (e.g., standard deviation), by using the mean(), sd(), and related functions.\nBut first, we want to make sure that there are no missing locations (missing Longitude or Latitude), in this data set. is,na() checks whether each element is an NA or not, and table() just counts:\n\n# missing longitude\ntable(is.na(elk_df$Lon))\n\n\nFALSE \n97825 \n\n# missing latitude\ntable(is.na(elk_df$Lat))\n\n\nFALSE \n97825 \n\n\nNo missing locations! That’s a relief.\nLet’s look at the number of locations per individual:\n\n# average number of GPS locations per individual\ntable(elk_df$ID) %>% mean\n\n[1] 9782.5\n\n# standard deviation of the number of locations per individual\ntable(elk_df$ID) %>% sd\n\n[1] 7059.185\n\n\nOn average, an individual has more than 9000 locations. However, the standard deviation is almost as big, which means that some individuals have a lot of locations and some have fewer locations.\n\n# maximum number of GPS locations per individual\ntable(elk_df$ID) %>% max\n\n[1] 21567\n\n# minimum number of GPS locations per individual\ntable(elk_df$ID) %>% min\n\n[1] 2639\n\n\nYou can also access (almost) all the statistics by applying the all-purpose summary() function to the table() of the ID. Note that you need to convert it to a data frame first[^1]: [^1] The output of table() in R is actually quite weird.\n\n# summary of the number of GPS locations per individual\ntable(elk_df$ID) %>% data.frame %>% summary\n\n         Var1        Freq      \n GR122     :1   Min.   : 2639  \n GR133     :1   1st Qu.: 3847  \n GR159     :1   Median : 6838  \n GR513     :1   Mean   : 9782  \n OR39      :1   3rd Qu.:16019  \n YL115_OR34:1   Max.   :21567  \n (Other)   :4                  \n\n\n\n6.3.4 What is the duration of monitoring?\nThe functions min(), max() and range() are self-explanatory, but importantly work excellenty with properly formatted time objects. The diff() function calculates differences among subsequent elements of a vector. But for time (POSIX) data, it is best to use the difftime(t1, t2) function since that allows you to specify the units of the time difference (hours, days, etc.) Otherwise, strange things can happen. For example, the overall time span of the monitoring effort is:\n\nrange(diff(elk_df$Time))\n\nTime differences in secs\n[1] -256413590  293543997\n\n\nThe number of seconds is not SO helpful. But difftime is a bit more useful:\n\ndifftime(max(elk_df$Time), min(elk_df$Time), units = \"days\")\n\nTime difference of 5078.5 days\n\n\nBetter. That’s a lot of days. More than 10 years. Years is not an option with difftime, but to manipulate the output statistically, you need to convert to numeric. Thus the number of years\n\n(difftime(max(elk_df$Time), min(elk_df$Time), units = \"days\") %>% as.numeric)/365.25\n\n[1] 13.90417\n\n\nThat’s a good, long-term dataset!\nAnyways, what we really want is to figure this out for each individual. One approach is to use plyr::ddply commands. This function allows you apply functions to different groups (subsets) of a data set. Here’s an example:\n\nelk_df %>% ddply(\"ID\", plyr::summarize, \n                 start= min(Time), end = max(Time)) %>% \n  mutate(duration = difftime(end, start, units = \"days\"))\n\n\n\n\n\nID\nstart\nend\nduration\n\n\n\nGR122\n2006-03-06 16:22:41\n2007-06-08 07:00:58\n458.6099 days\n\n\nGR133\n2006-03-17 18:26:03\n2007-01-06 23:45:31\n295.2219 days\n\n\nGR159\n2005-03-27 16:33:15\n2005-11-09 13:00:50\n226.8525 days\n\n\nGR513\n2015-02-28 01:00:47\n2016-06-17 10:46:06\n475.4065 days\n\n\nOR39\n2014-02-26 01:00:42\n2014-11-23 15:02:51\n270.5848 days\n\n\nYL115_OR34\n2014-02-23 01:01:02\n2017-01-09 17:00:48\n1051.6665 days\n\n\n\n\n\nThe (near) equivalent with dplyr commands - just jumping straight to the duration:\n\nelk_df %>% group_by(ID) %>% \n  dplyr::summarize(duration = difftime(max(Time), min(Time), units = \"days\")) \n\n# A tibble: 10 × 2\n   ID          duration      \n   <fct>       <drtn>        \n 1 GR122        458.6099 days\n 2 GR133        295.2219 days\n 3 GR159        226.8525 days\n 4 GR513        475.4065 days\n 5 OR39         270.5848 days\n 6 YL115_OR34  1051.6665 days\n 7 YL121_BL250  378.1383 days\n 8 YL5          300.4165 days\n 9 YL77         250.0837 days\n10 YL89         229.8036 days\n\n\nTo get statistical summaries of these durations, you have to convert the time range object (which is a unique difftime class) into numeric. Thus:\n\nelk_df %>% group_by(ID) %>% \n  dplyr::summarize(time_range = difftime(max(Time), min(Time), units =\"days\") %>% \n                     as.numeric) %>% summary\n\n          ID      time_range    \n GR122     :1   Min.   : 226.9  \n GR133     :1   1st Qu.: 255.2  \n GR159     :1   Median : 297.8  \n GR513     :1   Mean   : 393.7  \n OR39      :1   3rd Qu.: 438.5  \n YL115_OR34:1   Max.   :1051.7  \n (Other)   :4                   \n\n\nThe median duration of monitoring is ~300 days, or a little bit less than a year. Some individual(s) have ~3 years of monitoring, and some only a few days.\nWe can visualize the monitoring duration for each individual, on a plot, by extracting the start date and the end date of the monitoring for each individual.\n\nn.summary <- elk_df %>% group_by(ID) %>% \n  summarize(start = min(Time), end = max(Time)) \n\nHere’s a version using ggplot2\n\nrequire(ggplot2)\nggplot(n.summary, aes(y = ID, xmin = start, xmax = end)) + \n    geom_linerange() \n\n\n\n\nIt may make more sense to sort the individuals not alphabetically, but by the time of release. Here’s an approach, which relies on reordering the factor levels of the ID column (an often fussy task):\n\nn.summary <- elk_df %>% group_by(ID) %>% \n  summarize(start = min(Time), end = max(Time)) %>% \n  arrange(start) %>% \n  mutate(ID = factor(ID, levels = as.character(ID)))\n  \nggplot(n.summary, aes(y = ID, xmin = start, xmax = end)) + \n    geom_linerange() \n\n\n\n\nThis is quick and easy and attractive enough. But, for the record, if you wanted to use base plotting functions (which, for many applications, can be much more easy to customize), code for a similar plot would look something like this:\n\nwith(n.summary, {\n  plot(start, ID, xlim = range(start, end), \n       type = \"n\", yaxt = \"n\", ylab = \"\", xlab = \"\")\n  segments(start, as.integer(ID), end, as.integer(ID), lwd = 2)\n  mtext(side = 2, at = 1:nrow(n.summary), ID, cex = 0.7, las = 1, line = .2)\n  })\n\n\n\n\nIn any case, on this figure each line represents the duration of the monitoring (x axis) for a given individual (y axis). While we see the beginning and end of the monitoring for each individual, we cannot see if there are any gaps in the monitoring.\nTo see if there is one or multiple gaps, we can create a vector of Date for each individual (i.e., containing only the date and not the time, to simplify the it and get only get one row per day per individual). To get the date from a time vector, we use the as.Date function. We then use the slice() command to keep only row per day for each individual. Do not forget to arrange per ID and date as all these manipulation can sometimes mess up the ordering of your data.\n\nelk_days <- elk_df %>% mutate(date = as.Date(Time)) %>% \n  group_by(ID, date) %>% \n  slice(1) %>% arrange(ID, date) \n\n\nggplot(elk_days, aes(y = ID, x = date)) + \n    geom_point(shape = 20, size = .5, alpha = .1) \n\n\n\n\nSimilar to the previous figure, each line represent the monitoring dates for a given individual. But from this figure, we can see that there are some gaps in the monitoring of some individuals.\n\n6.3.5 What is the fix rate?\nThe fix rate, or the time lag between successive locations, can be extracted by using the difftime() function on the Time column. Again, this function needs to be applied to each individual separately. Here, we are subsetting the data set per ID, and applying a function which is adding a column difftime to each subset. Note that since the vector of time difference is smaller than the vector of time, we add a missing value at the beginning of each vector, for each value to represent the difference in time to the previous location.\n\nelk_df <- elk_df %>% ddply(\"ID\", mutate, \n        dtime = c(NA, difftime(Time[-1], Time[-length(Time)], units = \"hours\")))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nTime\nsex\nLon\nLat\nX\nY\ndoy\nMonth\nYear\n\n\n\nGR122\n2006-03-06 16:22:41\nf\n-115.5828\n51.75451\n597820.5\n5734685\n65\n3\n2006\n\n\nGR122\n2006-03-06 19:01:07\nf\n-115.5830\n51.75455\n597806.6\n5734689\n65\n3\n2006\n\n\nGR122\n2006-03-06 23:01:48\nf\n-115.5865\n51.73794\n597600.9\n5732837\n65\n3\n2006\n\n\nGR122\n2006-03-07 03:01:46\nf\n-115.5774\n51.73691\n598231.4\n5732735\n66\n3\n2006\n\n\nGR122\n2006-03-07 11:01:46\nf\n-115.5828\n51.75442\n597820.7\n5734675\n66\n3\n2006\n\n\nGR122\n2006-03-07 15:01:05\nf\n-115.5828\n51.75447\n597820.6\n5734680\n66\n3\n2006\n\n\n\n\n\nWhat are the statistics (min, max, mean, median, …) of this fix rate?\n\nelk_df$dtime %>% summary\n\nLength  Class   Mode \n     0   NULL   NULL \n\n\nOn average, the fix rate is ~1 hour (median is 15 minutes). The smallest one is 1 second and the biggest is a little more than a year (10664 hours ~ 178 days). This one probably comes from an animal that has been captured and equipped once, then recaptured a year after the end of the monitoring. Understanding the sources of these gaps underscores the importance of having a relationship with those people that actually collected the data, to understand their monitoring strategy and structure of the data, and understand how to process the data depending on your research questions."
  },
  {
    "objectID": "movement_data_in_R.html#processing-data",
    "href": "movement_data_in_R.html#processing-data",
    "title": "\n6  Movement data in R\n",
    "section": "\n6.4 Processing data",
    "text": "6.4 Processing data\nThe previous section illustrated a few typical approaches to exploring a movement dataset. Processing the data - broadly speaking - implies that we will be organizing it, filtering it, or adding information to the data frame in ways that contributes in a meaningful way to some key research question. By that definition, for example, the inclusion of the individual specific fix rate above is a key bit of data processing.\nAs an example, we might investigate the following question: Is there a difference in movement rates between winter and summer, for female elk?\n\n6.4.1 Subsetting by season\nTo answer this question, we need to focus on movement data of female elk during winter and summer only. Let’s say (arbitrarily) that winter is only January and February, and summer is just July and August. We can use month() to subset the data accordingly.\n\nelk_winter_summer <- elk_df %>% mutate(Month = month(Time)) %>%\n  subset(Month %in% c(1,2,6,7))\n\nTo add a column “season”, we can use the ifelse(), function which returns different values depending on whether a given element in a vector satisfies a condition.\n\nelk_winter_summer <- elk_winter_summer %>% \n  mutate(season = ifelse(Month < 3, \"Winter\", \"Summer\"))\ntable(elk_winter_summer$season)\n\n\nSummer Winter \n 29959   2306 \n\n\nYou could also perform this operation with a vector of days of year and the useful cut() function, which transforms numeric data to ordered factors. Thus, to obtain breaks:\n\nseason.dates <- c(winter.start = yday(\"2023-01-01\"), \n                  winter.end = yday(\"2023-02-28\"),\n                  summer.start = yday(\"2023-07-01\"),\n                  summer.end = yday(\"2023-08-31\"))\nseason.dates\n\nwinter.start   winter.end summer.start   summer.end \n           1           59          182          243 \n\n\n\ncut.dates <- c(season.dates, 367)\nelk_winter_summer <- elk_df %>% \n  mutate(season = cut(yday(Time), cut.dates, labels = c(\"winter\",\"other\",\"summer\",\"other\"))) %>% \n  subset(season != \"other\") %>% droplevels\ntable(elk_winter_summer$season)\n\n\nwinter summer \n  2176  17212 \n\n\n\n6.4.2 Estimating movement rate\nTo estimate the movement rate between subsequent steps for each individual and each season, we will use what we learned in chapter Section 4.1.\n\nCreate a Z vector combining the X and Y coordinates\nCalculate the step lengths (SL)\nCalculate the time difference of the steps\nCalculate step’s movement rate\n\nAs we need to do this for each individual, year and season separately, we will use the ddply(), as before. Note, that to do this most effectively, it is nice to write our own function that makes all the computations we need. The key in this function is that the movement rate (MR) is the step length divided by the time difference, converted ti km/hour. Here’s one such function:\n\ngetMoveStats <- function(df){\n  # df - is a generic data frame that will contain X,Y and Time columns\n  Z <- df$X + 1i*df$Y\n  Time <- df$Time\n  Step <- c(NA, diff(Z)) # we add the extra NA because there is no step to the first location\n  dT <- c(NA, difftime(Time[-1], Time[-length(Time)], hours) %>% as.numeric)\n  \n  SL <- Mod(Step)/1e3 # convert to km - so the speed is in km/hour\n  MR <- SL/dT # computing the movement rate\n\n  # this is what the function returns\n  data.frame(df, Z, dT, Step, SL, MR)\n}\n\nWe took care to pick this function apart into individual pieces. And understand that it returns a new data frame with the additional columns appended to the original data frame. THe ddply command will apply this function to every individual in every season in every year. This is now very quick:\n\nelk_winter_summer <- elk_winter_summer %>% \n  plyr::ddply(c(\"ID\", \"Year\", \"season\"), getMoveStats)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nTime\nsex\nLon\nLat\nX\nY\ndoy\nMonth\nYear\nseason\nZ\ndT\nStep\nSL\nMR\n\n\n\nGR122\n2006-07-02 00:00:19\nf\n-115.5844\n51.68669\n597856.4\n5727141\n183\n7\n2006\nsummer\n597856+5727141i\nNA\nNA\nNA\nNA\n\n\nGR122\n2006-07-02 00:15:17\nf\n-115.5841\n51.68658\n597877.4\n5727129\n183\n7\n2006\nsummer\n597877+5727129i\n14.96667\n20.9744-11.830i\n0.0240806\n0.0016089\n\n\nGR122\n2006-07-02 00:30:16\nf\n-115.5836\n51.68594\n597913.3\n5727058\n183\n7\n2006\nsummer\n597913+5727058i\n14.98333\n35.9429-70.504i\n0.0791373\n0.0052817\n\n\nGR122\n2006-07-02 00:45:16\nf\n-115.5828\n51.68509\n597970.4\n5726965\n183\n7\n2006\nsummer\n597970+5726965i\n15.00000\n57.1349-93.454i\n0.1095356\n0.0073024\n\n\nGR122\n2006-07-02 01:00:48\nf\n-115.5822\n51.68451\n598013.2\n5726901\n183\n7\n2006\nsummer\n598013+5726901i\n15.53333\n42.7284-63.696i\n0.0767000\n0.0049378\n\n\nGR122\n2006-07-02 02:00:28\nf\n-115.5820\n51.68433\n598027.4\n5726881\n183\n7\n2006\nsummer\n598027+5726881i\n59.66667\n14.2142-19.749i\n0.0243324\n0.0004078\n\n\n\n\n\n\n6.4.3 Quick analysis of movement rates\nWe are ready now (finally) to answer the question: Is there a difference in movement rate between winter and summer? We can start with a quick boxplot of the movement rates against individual ID’s and season. The distributions are highly skewed and quite variable, with some animals really on the move, and some spending a lot of time not moving at all.\n\nggplot(elk_winter_summer, aes(ID, MR)) + geom_boxplot() + facet_wrap(.~season)\n\n\n\n\nSome season & individual summary stats:\n\nmr_summarystats <- elk_winter_summer %>% ddply(c(\"ID\",\"season\"), summarize,\n  min = min(MR, na.rm = TRUE), max = max(MR, na.rm = TRUE),\n  n = length(MR), NA.count = sum(is.na(MR)),\n  Zero.count = sum(MR == 0, na.rm = TRUE))\n\n\n\n\n\nID\nseason\nmin\nmax\nn\nNA.count\nZero.count\n\n\n\nGR122\nsummer\n0.0000000\n0.0827437\n3806\n1\n96\n\n\nGR133\nwinter\n0.0000000\n0.0777082\n314\n1\n10\n\n\nGR133\nsummer\n0.0000000\n0.0871294\n3782\n1\n44\n\n\nGR159\nsummer\n0.0000000\n0.1078212\n3012\n1\n21\n\n\nGR513\nwinter\n0.0000000\n0.6543712\n677\n2\n10\n\n\nGR513\nsummer\n0.0000093\n0.0271023\n711\n1\n0\n\n\nOR39\nwinter\n0.0000000\n0.5450862\n35\n1\n1\n\n\nOR39\nsummer\n0.0005555\n1.8531111\n572\n1\n0\n\n\nYL115_OR34\nwinter\n0.0000000\n0.8679974\n270\n3\n1\n\n\nYL115_OR34\nsummer\n0.0000000\n2.6096342\n1279\n2\n2\n\n\nYL121_BL250\nwinter\n0.0000000\n1.6492821\n539\n1\n1\n\n\nYL121_BL250\nsummer\n0.0011136\n2.7874279\n541\n1\n0\n\n\nYL5\nwinter\n0.0000000\n0.0761834\n251\n1\n3\n\n\nYL5\nsummer\n0.0000183\n0.0480011\n515\n1\n0\n\n\nYL77\nwinter\n0.0005551\n1.3129208\n90\n1\n0\n\n\nYL77\nsummer\n0.0011128\n2.0078617\n622\n1\n0\n\n\nYL89\nsummer\n0.0000000\n0.1279309\n2372\n1\n14\n\n\n\n\n\nNote there are lot of 0’s in the data, which is rather remarkable considering the relative imprecision of GPS data - certainly high enough to show variation at the 1 meter scale. This is an evident  red flag  … one might be inclined to simply remove those data entirely.\nWithout going into too much detail of the statistical analysis of these data, the most appropriate statistical test would use a linear mixed effects model with individuals as random effects, and look something like this:\n\nfit <- lme4::lmer(log(MR) ~ season + (1 + season|ID), \n            data = elk_winter_summer %>% subset(MR != 0 & !is.na(MR)))\nsummary(fit)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: log(MR) ~ season + (1 + season | ID)\n   Data: elk_winter_summer %>% subset(MR != 0 & !is.na(MR))\n\nREML criterion at convergence: 70408.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.2795 -0.6753  0.0264  0.7044  3.2786 \n\nRandom effects:\n Groups   Name         Variance Std.Dev. Corr \n ID       (Intercept)  3.355    1.832         \n          seasonsummer 1.001    1.000    -0.14\n Residual              2.295    1.515         \nNumber of obs: 19164, groups:  ID, 10\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept)   -4.6853     0.6123  -7.653\nseasonsummer  -0.2238     0.3738  -0.599\n\nCorrelation of Fixed Effects:\n            (Intr)\nseasonsummr -0.285\n\n\nMovement rates are slightly lower in the summer - but, perhaps surprisingly, not with meaningful statistical significance (|\\(t\\) value| < 1).\nNote that the residuals are quite well-behaved with the log-transformation.\n\nplot(fit)"
  },
  {
    "objectID": "movement_data_in_R.html#conclusions",
    "href": "movement_data_in_R.html#conclusions",
    "title": "\n6  Movement data in R\n",
    "section": "\n6.5 Conclusions",
    "text": "6.5 Conclusions"
  },
  {
    "objectID": "movement_data_in_R.html#references",
    "href": "movement_data_in_R.html#references",
    "title": "\n6  Movement data in R\n",
    "section": "\n6.6 References",
    "text": "6.6 References\n\n\n\n\nHebblewhite, Mark, Evelyn H. Merrill, Hans Martin, Jodi E. Berg, Holger Bohm, and Scott L. Eggeman. 2020. “Data from: Study \"Ya Ha Tinda Elk Project, Banff National Park, 2001-2020 (Females)\".” Movebank Data Repository. https://doi.org/10.5441/001/1.5G4H5T6C."
  },
  {
    "objectID": "likelihoods.html#why-a-chapter-on-likelihoods",
    "href": "likelihoods.html#why-a-chapter-on-likelihoods",
    "title": "\n8  Likelihood Theory\n",
    "section": "\n8.1 Why a chapter on Likelihoods?",
    "text": "8.1 Why a chapter on Likelihoods?\nThe likelihood function is the single most important, versatile, and widely-used tool for performing inference. It is the natural link between observations (i.e. data) and probabilistic models (i.e. statistical models). The overwhelming majority of statistical analyses - certainly in wildlife ecology - in which models are specified and fitted and selected and hypothesis tests are performed in the service of inferential conclusions, relies on defining and computing a likelihood function and figuring out a way to maximize it. The parameters fitted using this basic strategy - the maximum likelihood estimators (MLE’s) have some very powerful and convenient properties. For example, the standard errors around those estimates can be computed quickly and efficiently using almost the same process that maximized the likelihood. Also MLE’s are mathematically proven to have the smallest standard errors (i.e. are the most precise) of all possible estimators. Furthermore, likelihoods underlie rigorous hypothesis tests, e.g. via likelihood ratio tests, and even more flexible and extremely widely used information criterion-based model comparison and selection tools, like AIC, BIC, and variants. And as important as likelihoods are to classical inference, they are equally essential to understanding and implementing Bayesian inference.\nDespite all of these frankly astonishing properties, likelihoods are almost never taught explicitly in courses - outside of statistics departments, and even then often only in graduate school. Highly literate and skilled quantitatively minded practitioners can be hard pressed to even define a likelihood. While there is a bit of conceptual leap (or inversion) in understanding what a likelihood actually is (the equation at the top of the chapter sums it up entirely) - they are, in fact, straightforward to understand and to communicate. That, at least, is the central hypothesis of this chapter.\nMuch of the magic of likelihoods is, perhaps, obscured by the fact that they are hidden behind the black-box of statistical tools in statistical software and the limited1 set of models and model assumptions that you can fit with those tools. But if you can write your own likelihood, then you can fit models that are not nearly as straightforward as the ones in the black boxes. Given the many non-standard nuances of movement data (see chapter Chapter 3), it is very, very easy to come up with a particular piece of inference for which you will want to design your own likelihood function. We provide one example at the end of this chapter.1 In the lively, teeming, global activity of applied statistics, there is, obviously, a tremendous proliferation of models that can be fitted with “out-of-the-box” tools. Starting with linear models, through mixed additive generalized linear models, to hidden Markov models, to models with explicit spatial and temporal auto-correlation, and on and on. But the world (or at least the processes and data that we might want to model) is orders of magnitude more complex than all the black boxes put together.\n\n8.1.1 A short definition of a likelihood\nLikelihoods turn probabilities on their heads.\n\n\n\n\n\nFigure 8.1: A man on his head is the same man, and yet, an entirely different man. Like a Zen koan, we must strive to keep both truths in our minds.\n\n\n\n\n\n\n8.1.2 Not helpful!\nOk, a probability statement (or, perhaps better, a probability question) asks: \\(\\Pr({\\bf X}|\\theta)\\)? I.e. what is the probability (i.e. a score between 0 and 1 that reflects the likelihood of an occurrence) of a particular event (or observation or data, \\(\\bf X\\)) given a particular model (or parameter set, \\(\\Theta\\)). A likelihood statement asks the opposite question: \\({\\cal L}(\\Theta | {\\bf X})\\), i.e. what is the likelihood of a particular model (\\(\\Theta\\)) given a particular set of observations (\\(\\bf X\\)).\nEven at this abstract level, it should be clear the the likelihood question is the more relevant one for inference. Since we do not know any probabilistic models, except for all those completely artificial games that are played in probability class, like flipped coins, and six-sided dice, and decks of cards, and mysterious urns with different colored balls.2 For every other application, we can make observations (collect \\(\\bf X\\)) and what we want to know is what’s good model (\\(\\Theta\\)).2 The mathematical foundations probability theory are disproportionately due to the idle gambling games of French noblemen in the 17th century, who happened to be acquainted with some pretty bright bulbs.\n\n8.1.3 Please give me an example!\nOk, let’s build off a specific example. Imagine, we know the true distribution of adult human (Homo sapiens) male heights, is 175 cm (5’ 9’’), with a standard deviation of 20 cm and that the distribution is Gaussian or normal. We notate this as: \\[X \\sim {\\cal N}(\\mu_0 = 175, \\sigma_0 = 20)\\]\nThis is a probability model, i.e. it tells us that: \\[P(x < X < x+dx) = \\phi(x|\\mu, \\sigma)dx\\] where \\(f(x|\\mu, \\sigma)\\), is the density function of the normal distribution with \\(\\mu\\) and \\(\\sigma\\). As a reminder, that function looks like this: \\[\\phi(x|\\mu, \\sigma) = {1\\over \\sigma\\sqrt{2\\pi}} e^{-{1 \\over 2}\\left({x - \\mu\\over{\\sigma}}\\right)^2}\\] and it has the familiar bell shaped curve. Recall that the 95% of the probability is within 1.96 (or, just, 2) standard deviations of the mean, thus our model states that 95% of all mean are between 135 cm (4’ 5’‘) and 215 cm (7’). Maybe this is reasonable, maybe a bit wide, but we’re assuming this is the truth.\nThis probability statement means you can make some definitive probabilistic statements. For example, if you got a hold of a single 210 cm (6’ 11’’) tall person, you can say with certainty that:\n\nthe probability that a person is exactly 210 cm is exactly 0. This is a fundamental property of continuous distributions: any specific event has probability 0 because it is too easy to be very very close. Or, more formally, \\(dx\\) in the statement above is 0.\nHowever, the “per-cm” probability” that he is around 210 cm tall is given by the density function of the normal distribution at 210 cm\n\n\\[f(x = 210 \\, | \\, \\mu = 175,\\, \\sigma = 20) = 0.0043\\] i.e. the probability that a random man will be within 1 cm of 210 cm is about 0.4%. In R, this probability is given by the dnorm function.3 Note that we place the conditional notation \\(|\\), which is read as “given” or “conditional on”, and place our two parameters \\(\\mu\\) and \\(\\sigma\\) to the right. Those parameters (together with the implicit distribution) are the model \\(\\Theta\\).3 It is essential to learn about the notation for random variables in R! Every distribution has a “root” name (e.g. norm for normal, exp for exponential, binom for binomial). The probability density function (p.d.f.) is given by d****, the cumulative density function (c.d.f.) is given by p****, the quantile function by q**** and the random generation of samples from a distribution by r****. This may be confusing, but is extremely important.\n\ndnorm(210, mean = 175, sd = 20)\n\n[1] 0.004313866\n\n\nWe can compute the probability of a random male being within some range of sizes very straightforwardly as well. For example, the probability that a random male is between, say, 180 and 220 cm is computed mathematically as: \\(\\int_{180}^{220} \\phi(x | \\mu = 175, \\sigma = 20)\\,dx\\)\nand in R as:\n\npnorm(220, 175, 20) - pnorm(180, 175, 20)\n\n[1] 0.3890692\n\n\nSo 39% of men (in this model) are somewhere in that interval.\n\n8.1.4 Defining a Likelihood\nThis is all fine and well, but, in fact, we don’t know what the true distribution of human male adult heights is.4. Instead, you are - say - an alien (or profound amnesiac) who beams into (or comes to) onto a professional basketball court, and out trots a human male who is 210 cm tall. Aha! Data:4 it can be argued that there is not “true distribution*, at least not without accounting for ALL the possible multiverses. But that’s outside scope of this discussion.\n\\[ X_1 = \\{210\\}\\]\nTo “flip this on its head”“, we ask a new question: What is the likelihood that the mean and standard deviation of human heights are, in fact, 175 and 20, respectively GIVEN that we observed one male who is \\(X_1 = 175\\) feet tall?\nWe write this as: \\[ {\\cal L}(\\mu, \\sigma | x).\\]\nIt is “flipped on its head” because it as a function of the parameters given an observation, rather than as the probability of an observation given some parameters. But, by definition, the likelihood is numerically equal to the probability:\n\\[L_0 = {\\cal L}(\\mu_0, \\sigma_0 | X_1) = f(X_1|\\mu_0, \\sigma_0) = 0.0043\\]\nIn colloquial language, a “likelihood” is sort of similar to a “probability” - i.e. the statement: “Event A is likely.” seems to have similar meaning to the statement: “Event A has high probability.”\nIn statistics, the “likelihood” and the “probability” are, in fact EQUAL! But there is an inversion of what is considered “known” and “unknow. A probability tells you something about a random event given parameter values. The likelihood tells you something about parameter values given observations. In practice - statistical inference is about having the data and guessing the parameters. Thus the concept of Likelihoods is extremely useful and natural.\nA key fact to keep in mind: in contrast to probabilities, the actual raw value of the likelihood is NEVER of interest. We ONLY care about the value of a likelihood relative to likelihoods of different models. For example, we can compare the likelihood of the parameters \\(\\mu_0 = 180\\) and \\(\\sigma_0 = 20\\), GIVEN the observation \\(X_1 = 210\\), with an alternative probability model … say, \\(\\mu_1 = 210\\) cm and \\(\\sigma_1 = 0.1\\) cm. That likelihood is given by:\n\\[ L_1 = {\\cal L}(\\mu_1, \\sigma_1 \\,|\\, X_1 = 7) = f(210 \\,|\\, 210, 0.1)\\]\n\n(L1 <- dnorm(210,210,0.1))\n\n[1] 3.989423\n\n\n\\(L_1\\) is clearly, much much greater than our original likelihood, i.e. this set of parameters is much likelier than the original set of parameters. Indeed, the ratio \\(L_1 / L_0 = 92.5\\). We can make an unambiguous and strong statement: Model 1 is more than 900x more likely than the Null Model!\n\n8.1.5 Joint Likelihoods\n\n\n\nWe’re very happy with our new model (it’s 900 times better than the null model!) But are perhaps a bit uncomfortable with the small sample size. Thankfully, four more basketball players come jogging out of the locker room, we measure their heights (with our robotic alien ayes) and now have more data! Here are their heights:\n\\[X \\sim \\{190, 200, 210, 220, 230\\}\\] Let’s immediately capture this data in R:\n\nX <- c(190, 200, 210, 220, 230)\n\nAt this point, I might have reasonable suspicion that neither our null-model (M0) nor or adapted model (M1) might be appropriate for this subset of humans. Visualize these data-points against our null-model:\n\nmu0 <- 180; sigma0 <- 20\ncurve(dnorm(x, mu0, sigma0), xlim=c(120,240), ylim = c(0, 0.02))\npoints(X, dnorm(X,mu0,sigma0), pch=19, col=2)\npoints(X, dnorm(X,mu0,sigma0), type=\"h\", col=2)\n\n\n\n\nNow, we can compute the likelihood of the null parameters given all of these observations.The likelihood (a joint likelihood) is just the product of the likelihood for each of these points, because it is equal to the joint density distribution … this is because the Probability of \\(n\\) independent events is the product of the probabilities:\n\\[{\\cal L}(\\mu_0, \\sigma_0 | {\\bf X}) = \\prod_{i=1}^n f(X_i|\\mu_0, \\sigma_0) \\]\n\n(L0 <- prod(dnorm(X, mu0, sigma0)))\n\n[1] 3.263065e-12\n\n\nThis is a very small number, but again - it is meaningless without having a comparison. Let’s compute the joint likelihood of our alternative model:\n\nmu1 <- 210; sigma1 <- 0.1\n(L1 <- prod(dnorm(X, mu1, sigma1)))\n\n[1] 0\n\n\nTo machine power, the second model is MUCH LESS likely than the first model! In fact, the machine can’t multiply probabilities that small and returns a 0.\n\n\n\n\nComparison of basketball player data against three models.\n\n\n\n\nYou see that the points far away from the mean peak have extremely low probability, which brings the likelihood of this model way down."
  },
  {
    "objectID": "likelihoods.html#the-maximum-likelihood-estimator",
    "href": "likelihoods.html#the-maximum-likelihood-estimator",
    "title": "\n8  Likelihood Theory\n",
    "section": "\n8.2 The Maximum Likelihood Estimator\n",
    "text": "8.2 The Maximum Likelihood Estimator\n\nSo - how do we find the parameters that maximize the likelihood? These parameters are called the Maximum Likelihood Estimators (MLE’s), and are the “best” parameters in that they are the most precise. Remember, every estimate is a random variable, and therefore comes with some variance[^index-5]. It can be shown that MLE’s have the smallest of those possible variances, in other words the estimates have the smallest standard error. The technical notation for a MLE is:\n\\[\\{ \\widehat{\\theta_\\mathrm{mle}}\\} \\subseteq \\{ \\underset{\\theta\\in\\Theta}{\\operatorname{arg\\,max}}\\ {\\cal L}(\\theta\\,|\\,X_1,\\ldots,X_n) \\}\\] Translating this to English the MLE estimators of parameters \\(\\theta\\) are those values of \\(\\theta\\) for which the likelihood function is maximized.\nLet’s look at a range of possible values for \\(\\widehat{\\mu}\\) and \\(\\widehat{\\sigma}\\) for our basketball player example. We can do this pretty efficiently in R.\nFirst, pick some values to explore:\n\nmus <- 180:240\nsigmas <- seq(5,35,.5)\n\nNow, write a function that computes the likelihood (which, as we recall, is a function of \\(\\mu\\) and \\(\\sigma\\), and “assumes” \\(\\bf X\\))\n\nLikelihood <- function(mu, sigma)\n  prod(dnorm(X, mu, sigma))\n\nAnd compute this likelihood for all the combinations of \\(\\mu\\) and \\(\\sigma\\) above, using the mighty outer() function:\n\nL.matrix <- outer(mus, sigmas, Vectorize(Likelihood))\n\nNote (as a technical aside) that to get the Likelihood() function to work within outer(), it had to be “vectorized”. Happily, there is a function (Vectorize()) that does just that.\nWe can visualize our likelihood profile over those ranges of \\(\\mu\\) and \\(\\sigma\\) using the image() and contour() commands.\n\n\n\n\n\nClearly, there is a sharp peak in the likelihood around \\(\\widehat{\\mu} = 210\\) and somewhere just under \\(\\widehat{\\sigma} = 15\\). From our limited sets of values, we can find the values at the maximum:\n\nc(\n  mu.hat = mus[row(L.matrix)[which.max(L.matrix)]],\n  sigma.hat = sigmas[col(L.matrix)[which.max(L.matrix)]]\n)\n\n   mu.hat sigma.hat \n      210        14 \n\n\nAnd the (irrelevant) value of the likelihood is\n\nmax(L.matrix)\n\n[1] 1.465602e-09\n\n\n\n8.2.1 Numerically finding the MLE\nWe don’t need to do this search ourselves - that’s why we invented electronic computing devices. The powerhouse function for optimizing functions in R is optim(), but it takes some getting used to. The (minimal) syntax is:\n\noptim(par, fn, ...)\n\nwhere:\n\n\npar is a required vector of your initial guess for the parameters.\n\nfn(par, ...) is a function that takes as its first argument a vector of parameters par\n\nThe ... refers to other arguments passed to fn. Most importantly, this will be data!\n\nAn extremely important and confusing nuance to using optim is that is MINIMIZES. This is very natural, as the canonical optimization problem is the minimization of a so-called objective function. Therefore, the FUN have to return the negative of the likelihood, if we want to maximize the likelihood.\nSee how I make it work below, using our “naive” model (mean = 175, sd = 20) of adult male height. Note the trick of naming the parameters (p[\"mu\"] and p[\"sigma\"]) rather than indexing them by number - that’s all most always a good idea, especially as vectors get longer and longer. This will also come in handy when we interpret the output:\n\nLikelihood <- function(mu, sigma, x) prod(dnorm(x, mu, sigma))\np0 <- c(mu = 175, sigma = 20)\noptim(p0, function(p, x) -Likelihood(p[\"mu\"], p[\"sigma\"], x), x = X)\n\n$par\n       mu     sigma \n209.99981  14.14376 \n\n$value\n[1] -1.466355e-09\n\n$counts\nfunction gradient \n      63       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\nSo - lots of output, but the key numbers we’re interested in are the top two, under $par. These are: \\(\\widehat{\\mu} = 209.998\\) and \\(\\widehat{\\sigma} = 14.144\\). Good! This is consistent with our likelihood profile. The $value is very close to the (negative of the) maximum likelihood that we “hand-calculated”.\n\n8.2.2 Comparing models side by side\n\n\n\n\n\nYou can get a feel for the constraints on the MLE model. First, it is balances around the mean value. Second, if the \\(\\sigma\\) were smaller, the contribution of the outlying points would become much smaller and bring the likelihood down; if \\(\\sigma\\) were any larger, then the flattening would bring down too far the contribution of the central points.\n\n8.2.3 Comparing with Method of Moment Estimators (MME’s)\nNow … you might think that’s a whole lot of crazy to estimate a simple mean and variance. You may have guessed that the best estimate of the mean is the sample mean:\n\\[\\widehat{\\mu} = \\overline{X} = {1\\over n} \\sum_{i=1}^n X_i\\] and the standard deviation is best estimated by the sample standard deviation:\n\\[\\widehat{\\sigma^2} = s_x^2 = {1\\over n-1}\\sum (X_i - \\overline{X})^2\\] The mean estimate matches, but the sample standard deviation doesn’t quite:\n\nsd(X)\n\n[1] 15.81139\n\n\nThis suggests that the MLE of the variance - for all of its great qualities - is at least somewhat biased (remember: \\(E(s_x^2) = \\sigma^2\\)).\n Can you recognize what analytical formula gives the MLE of \\(\\sigma\\)? \n\nNote: this is a special case where the MLE is actually computable by hand, but in general it is not - and best to obtain numerically, as the examples that follow show.\n\n\n8.2.4 Log-likelihoods\nFor a combination of practical and theoretical reasons, what we actually maximize is not the likelihood but the log-likelihood. The maximum will be in the same place for both functions, since \\(\\log(x)\\) is a monotonic function, but logs are much easier to work with. Notably, the log function converts products (which either explode or - in the case of probability - collapse to very very near zero very very quickly), into tidy sums:\nLikelihood: \\[{\\cal L}(\\mu_0, \\sigma_0 | {\\bf X}) = \\prod_{i=1}^n f(X_i|\\mu_0, \\sigma_0) \\]\nLog-likelihood: \\[{\\cal l}(\\mu_0, \\sigma_0 | {\\bf X}) = \\log({\\cal L}) = \\sum_{i=1}^n \\log(f(X_i|\\mu_0, \\sigma_0)) \\]\nSums are much easier to manipulate with algebra and handle computationally than products. Also, they help turn very very very small numbers and very very very large numbers to very very very ordinary numbers.55 The number of particles in the universe is about 10^80 … its log is 184. The ratio of the mass of an electron to the size of the sun is 10^-60 … its log is -134. Even I can count to -134.\n\n\n\n\nThe log likelihood is a lot flatter, but the maximum is the same.\n\n\n\n\nBecause the log ofa probability is such an important quantity, in R there is always a default log = TRUE/FALSE option in its distribution functions. And the optim function6 much, much prefers optimizing over sums than over probabilities. Putting that together here:6 Which, to be clear, can be infuriatingly finnicky!\n\nlogLikelihood <- function(mu, sigma, x)  \n  sum(dnorm(x, mu, sigma, log = TRUE))\noptim(c(mu = 175, sigma = 20), \n      function(p, x) -logLikelihood(p[\"mu\"], p[\"sigma\"], x = X))\n\n$par\n       mu     sigma \n210.00200  14.14072 \n\n$value\n[1] 20.34049\n\n$counts\nfunction gradient \n      61       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\nThe calcualtion is nearly instant, the parameters are very close, and the maximum log-Likelihood is -20.34.\n\n8.2.5 Confidence Intervals\n\n8.2.5.1 A bit of theory\nThe peak of the (log)-likelihood surface gives you point estimates of parameters. But likelihood theory provides an additional enormously handy (asymptotically correct) result with respect to standard errors around the estimates. Specifically (in semi-precise English words): The variance around the point estimates is equal to the negative reciprocal of the second derivative of the log-likelihood at the maximum.\nWhile that might seem like a mouthful of twists (negatives! reciprocals! derivatives!), this is actually a very intuitive result. Basically: the sharper the peak of the likelihood mountain, the more negative the second derivative, the smaller the (positive) variance. In contrast, a flat peak will have a less negative second derivative, and the estimate will have a larger the variance.\nAll of the above is true in one dimension. But in almost all cases you are estimating multiple parameters. No worries, the principle and practice is the same, but the jargon (and underlying math) is a bit fancier.\n\nthe Hessian is an n-dimensional second derivative - i.e. a matrix that summarizes the sharpness of the peak.\nthe Fisher Information (\\(\\cal{I}\\)) is - specifically - the Hessian of the log-likelihood.\nthe inverse is the n-dimensional equivalent of “reciprocal”.\n\\(\\Sigma\\) is the variance-covariance matrix of the parameter estimates.\n\nWith that in mind, the (asymptotically correct) variance of a maximum likelihood estimator is given by the following compact formula:\n\\[\\Sigma(\\theta) = {\\cal I}(\\theta)^{-1}\\]\n\n8.2.5.2 Application\nSo - four steps: (1) obtain the Hessian, (2) take its inverse, and (3) take the square root of the diagonals of the matrix to get standard errors, and (4) convert to confidence intervals.\n\nThe hardest bit (numerically speaking) is to calculate the Hessian, but our workhorse optim() is happy to provide it with the hessian = TRUE argument:\n\n\nlogLikelihood <- function(mu, sigma, x) \n  sum(dnorm(x, mu, sigma, log = TRUE))\nparam.fit <- optim(c(mu = 180, sigma = 20), \n      function(p, x) -logLikelihood(p[\"mu\"], p[\"sigma\"], x),\n      x = X, hessian=TRUE)\nparam.fit$hessian\n\n                mu        sigma\nmu    2.500419e-02 1.329514e-05\nsigma 1.329514e-05 5.002096e-02\n\n\n\nIn R, the inverse of a matrix (in R) is given (somewhat unintuitively) by the solve() function. The inverse of the hessian is the variance-covariance matrix of the parameter estimates. It is worth pausing here to make sure that they are not too correlated, which is the meaning of the off-diagonal elements of this matrix:\n\n\n(Sigma <- solve(param.fit$hessian))\n\n               mu       sigma\nmu    39.99330163 -0.01062988\nsigma -0.01062988 19.99162241\n\n\n\nThe square root of the diagonal gives standard errors\n\n\n(se <- sqrt(diag(Sigma)))\n\n      mu    sigma \n6.324026 4.471199 \n\n\n\nAnd the confidence intervals are given by:\n\n\ncbind(hat = param.fit$par, \n      CI.low = param.fit$par  - 1.96*se, \n      CI.high = param.fit$par  + 1.96*se)\n\n            hat   CI.low  CI.high\nmu    209.99624 197.6012 222.3913\nsigma  14.14095   5.3774  22.9045\n\n\n\noptim: Is the engine under a great many hoods of R functions!\n\n\n\nJohn Nash is the creator (and skeptic) of optim. For a wonky history into optim’s plusses and minusses, see this insightful blog post."
  },
  {
    "objectID": "likelihoods.html#testing-hypotheses-and-model-selection-with-likelihoods",
    "href": "likelihoods.html#testing-hypotheses-and-model-selection-with-likelihoods",
    "title": "\n8  Likelihood Theory\n",
    "section": "\n8.3 Testing hypotheses and model selection with likelihoods",
    "text": "8.3 Testing hypotheses and model selection with likelihoods\n\n8.3.1 Likelihood Ratio Test:\n\nModel 0 and Model 1 are NESTED\n(i.e. Model 0 is a special case of Model 1) with \\(k_0\\) and \\(k_1\\) parameters.\nCompute MLE’s: \\(\\widehat{\\theta_0}\\) and \\(\\widehat{\\theta_1}\\)\nCompute likelihoods: \\({\\cal L_0(\\theta_0|X)}\\) and \\({\\cal L_1(\\theta_1|X)}\\)\nimportant: the data \\(X\\) must be identical!\nLikelihood Ratio Test Statistic: \\[\\Lambda = -2 \\log \\left( \\frac{L_0}{L_1}  \\right) = 2 (l_1 - l_0)\\]\nunder Null hypothesis (i.e. Model 1) has distribution \\(\\Lambda \\sim \\text{Chi-squared} (d.f. = k_1 - k_0)\\)\n\n\n8.3.1.1 An example\nWe will use home-made likelihoods to dig into the simplest and most familiar linear regression. In the simulated data below, there is considerable noise on a possible negative relationship between X and Y:\n\n\n\n\n\nWe have two competing models: \\[M0: Y_i = \\beta_0 + \\epsilon_i\\] \\[M1: Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\] where \\(\\epsilon\\) are i.i.d. Gaussian residuals.\nWe define the (negative) log-likelihood functions for this linear regression. Note that this time, we have two data objects in our likelihood: X and Y.\n\nLL0 <- function(par = c(beta0, sigma), X, Y){\n  -sum(dnorm(Y, mean = par['beta0'], sd = par['sigma'], log=TRUE) )\n}\nLL1 <- function(par = c(beta0, beta1, sigma), X, Y){\n  Y.hat <- par['beta0'] + par['beta1']*X\n  -sum(dnorm(Y, mean = Y.hat, sd = par['sigma'], log=TRUE) )\n}\n\nWe use optim to obtain estimates:\n\nfit0 <- optim(c(beta0=0, sigma=1), LL0, X = X, Y = Y)\nfit1 <- optim(c(beta0 = 0, beta1 = 0, sigma = 1), LL1, X = X, Y = Y)\n\nInspect the parameters:\n\nfit0$par\n\n    beta0     sigma \n1.2506021 0.9163669 \n\nfit1$par\n\n     beta0      beta1      sigma \n 1.1670651 -0.4251247  0.8429260 \n\n\nAnd performing the test. Recall - the likelihood ratio is the difference of the log-likelihoods, and that the log-likelihood is the negative of the value that has been optimized. The degrees of freedom is just the difference between the number of parameters in model 1 (3 parameters) and model 2 (1 parametr)\n\nLRT <- 2*(-fit1$value + fit0$value)\n1 - pchisq(LRT, df = 1)\n\n[1] 0.06770992\n\n\nSo there is a significantly negative slope. Of course, we can make that inference also by looking at the confidence intervals around that estimate:\n\nfit1 <- optim(c(beta0 = 0, beta1 = 0, sigma = 1), LL1, X = X, Y = Y, hessian = TRUE)\nse <- sqrt(diag(solve(fit1$hessian)))\ncbind(hat = fit1$par, se, \n      CI.low = fit1$par  - 1.96*se, \n      CI.high = fit1$par  + 1.96*se)\n\n             hat        se     CI.low    CI.high\nbeta0  1.0165368 0.2610965  0.5047878  1.5282859\nbeta1 -0.8858764 0.2486389 -1.3732086 -0.3985443\nsigma  1.1472530 0.1814175  0.7916746  1.5028314\n\n\nThe slope parameter’s confidence intervals are well below 0. Note that the true values I used were \\(\\beta_0 = 1, \\beta_1 = -0.8, \\sigma = 1\\), so rather excellent estimates. It’s always fun to compare these with the R function that does this the way you’d ordinarily do this:\n\nsummary(lm(Y~X))$coef\n\n              Estimate Std. Error   t value    Pr(>|t|)\n(Intercept)  1.0165030  0.2751980  3.693715 0.001661759\nX           -0.8858727  0.2620676 -3.380321 0.003333638\n\n\nThe estimates are extremely similar! But isn’t it somehow more satisfying to have written the likelihood function oneself? Like eating a home-baked cake over a store-bought cake.\n\n8.3.2 Information Criteria\nIf models are not nested (most interesting sets of models aren’t) we can’t use a likelihood ratio test. Instead, we use the very very widely applied Akaike Information Criterion (AIC), which is given by a very simple formula of the likelihood:\n\\[ AIC =  - 2 \\log(\\cal L) + 2 k\\]\nwhere \\(k\\) is the number of parameters you estimated. As a rule of thumb, the lowest AIC is best, and models that have a difference within 2 AIC are considered … preferred. Or, even, best. Not exactly “significant”, but there is no real test anymore, just a broad comparison.\n\nc(AIC0  = 2*(fit0$value + 2*2),\n  AIC1  = 2*(fit1$value + 2*3))\n\n    AIC0     AIC1 \n61.26318 74.24919 \n\n\nAlternatively, the Bayesian Information Criterion (BIC) \\[ BIC =  - 2 \\log(\\cal L) + k \\log(n) \\]\n\nc(BIC0  = 2*(fit0$value + 2 * log(length(X))),\n  BIC1  = 2*(fit1$value + 3 * log(length(X))))\n\n    BIC0     BIC1 \n65.24611 80.22359 \n\n\nWhich? Why? - a complicated debate, different underlying assumptions But generally - if you want to be more parsimonious (i.e. protect from overfitting) BIC is a better bet.\n\n\n8.3.3 In summary\nIf you have data and you can write a probability model in terms of parameters, no matter how strange or arbitrary seeming, there’s a good chance you can 1. estimate those parameters, 2. compare competing models, and 3. obtain confidence intervals. Amazing!"
  },
  {
    "objectID": "likelihoods.html#applying-likelihoods-to-the-correlated-random-walk",
    "href": "likelihoods.html#applying-likelihoods-to-the-correlated-random-walk",
    "title": "\n8  Likelihood Theory\n",
    "section": "\n8.4 Applying likelihoods to the Correlated Random Walk",
    "text": "8.4 Applying likelihoods to the Correlated Random Walk\nWe have seen that movement data often has:\n\nSkewed, Positive, step Length Distribution\nWrapped turning angle distributions, clustered around 0^o\n\n\n\n\nThis combination is known as the Correlated Random Walk (CRW) model, probably the most commonly used basic movement model in ecology (Kareiva and Shigesada (1983) coined the term, but Patlak (1953) really analyzed the heck out of it in a largely forgotten work decades earlier).\nTypical step-length model - the Weibull distribution: \\[f(x;\\alpha, \\beta) = \\frac{\\alpha}{\\beta}\\left(\\frac{x}{\\beta}\\right)^{\\alpha-1}e^{-(x/\\beta)^{k}}\\] \\(\\alpha\\) and \\(\\beta\\) are the shape and scale parameter, respectively. In R, it’s given by the dweibull() function.\n\ncurve(dweibull(x, 1, 2), xlim=c(0,6), ylim=c(0,1.2), ylab=\"density\", xlab=\"\", col=2)\ncurve(dweibull(x, 2, 2), add=TRUE, col=3)\ncurve(dweibull(x, 6, 2), add=TRUE, col=4)\nlegend(\"topright\", col=2:4, lwd=2, legend=c(1,2,6), title=\"Shape parameter\", bty=\"n\", cex=1.5)\n\n\n\n\nA common turning angle model - wrapped Cauchy distribution: \\[f(\\theta|\\mu,\\kappa)=\\frac{1}{2\\pi}\\,\\,\\frac{\\sinh\\kappa}{\\cosh\\kappa-\\cos(\\theta-\\mu)}\\] where \\(\\mu\\) is the mean angle (usually 0) and \\(\\kappa\\) is clustering parameter. This parameter, which ranges from -1 to 1, is conveniently estimated (using methods of moments) as the average cosine of the turning angles: \\(E(cos(\\theta))\\). The wrapped Cauchy distribution is in the CircStats package, under dwrpcauchy().\n\nrequire(CircStats) # NOT in base!\ncurve(dwrpcauchy(x, 0, 0), xlim=c(-pi,pi), ylim=c(0,1.5), lwd=2, ylab=\"density\", xlab=\"\", col=2)\ncurve(dwrpcauchy(x, 0, 0.5),  add=TRUE, col=3)\ncurve(dwrpcauchy(x, 0, 0.8),add=TRUE, col=4, n=1001)\nlegend(\"topright\", col=2:4, legend=c(0,0.5,0.8), title=\"Shape parameter\", bty=\"n\", cex=1.5, lwd=2)\n\n\n\n\n\n8.4.1 MLE of Weibull parameters\nHere’s some sample data:\n\nY <- cumsum(arima.sim(n=100, model=list(ar=.7)))\nZ <- X + 1i*Y\nplot(Z, type=\"o\", asp=1)\n\n\n\n\nA function that returns the likelihood as a function of the parameters and data\n\nWeibull.likelihood <- function(p, Z){ \n  S = Mod(diff(Z))\n  -sum(dweibull(S, p[\"shape\"], p[\"scale\"], log=TRUE))\n}\n\nRun the optimization:\n\n(Weibull.fit <- optim(c(shape = 1, scale = 1), Weibull.likelihood, Z=Z))\n\n$par\n   shape    scale \n2.389178 2.438174 \n\n$value\n[1] 134.0826\n\n$counts\nfunction gradient \n      67       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\nVisually assess the fit:\n\nhist(Mod(diff(Z)), freq=FALSE, col=\"grey\", breaks=10)\ncurve(dweibull(x, Weibull.fit$par[\"shape\"], Weibull.fit$par[\"scale\"]), add=TRUE, col=2, lwd=2)\n\n\n\n\nNot Bad! Using this technique to estimate \\(\\widehat{\\kappa}\\) is left as an exercise."
  },
  {
    "objectID": "likelihoods.html#now-for-something-really-fancy",
    "href": "likelihoods.html#now-for-something-really-fancy",
    "title": "\n8  Likelihood Theory\n",
    "section": "\n8.5 Now for something really fancy",
    "text": "8.5 Now for something really fancy\nNamely, a step length - turning angle dependency model! It is an unwritten assumption in the CRW literature that step lengths and turning angles are (statistically) independent. In fact, it seems quite plausible that longer step lengths would be associated with more directed movements. I have never seen anyone explicitly test that hypothesis, except in the (very common) context of separating movements into multiple states.\nSo, let use write down a model that has the property we hypothesize to be true for many animals - i.e. longer step-lengths are more directed. One model that would capture that would make the \\(\\kappa\\) coefficient a function of step length, e.g.: \\[ \\kappa_i = \\kappa_0 + (1-\\kappa_0) (1 - \\exp(\\alpha S_i))\\]\n\ns <- .5\nkappa0 <- 0.2\nfunkykappa <- function(x, kappa0=0, alpha=1) kappa0 + (1-kappa0)*(1-exp(-alpha*x))\nalphas <- c(0,0.5,1,2)\n\nplot(0,0,type=\"n\",xlim=c(0,3), ylim=c(0,1), ylab=expression(kappa), xlab = \"step length\")\nfor(i in 1:length(alphas))\n  curve(funkykappa(x, alpha = alphas[i]), add=TRUE, col=i)\nlegend(\"right\", col=1:4, legend = alphas, lty=1, title=expression(alpha), bty=\"n\")\n\n\n\n\nHere, the \\(\\kappa_0\\) is a base line clustering, and \\(\\alpha\\) is a strength of the dependence on step lengths; these are similar to intercepts and slopes.\nLet’s see how this model looks, by simulating the process and visualizing:\n\n# initailize parameter values\nkappa0 <- 0.2; alpha <- 1\nS <- rexp(1000)\n\nfunkykappa <- function(x, kappa0, alpha) kappa0 + (1-kappa0)*(1-exp(-alpha*x))\ntheta <- rwrpcauchy(1000, location = 0, rho = funkykappa(S, kappa0, alpha))\n\n# constrain theta to -pi to pi\ntheta[theta > pi] <- theta[theta > pi] - 2*pi\n# rose diagram of turning angles\nrose.diag(theta, bins=24)\n\n# plot the movement from steps and angles\nphi <- cumsum(theta)\ndZ <- complex(arg = phi, mod = S)\nZ <- cumsum(dZ)\nplot(Z, type=\"o\", asp=1, pch=16, cex=0.7, col=rgb(0,0,0,.2))\nplot(theta, S, pch=19, col=rgb(0,0,0,.2))\n\n\n\n\nThis movement looks similar to the other track, but the longer steps are more linear. The third plot illustrated the relationship between smaller turning angles and longer step-lengths, though it is quite noisy.\n\n8.5.1 Estimating the parameters\nCan we pick out the relationship? Estimate those parameters?\nThe negative log-likelihood function of this model given the data is:\n\\[{\\cal l} = \\log({\\cal L}(\\kappa_0, \\alpha | {\\bf S}, {\\bf \\theta})) = \\sum_{i=1}^n \\log(f(S_i, \\theta_i | \\kappa_0, \\alpha) \\]\nEncode in R:\n\nFunkyModel.Likelihood <- function(p, S, theta){\n  kappa <- funkykappa(S, p[\"kappa0\"], p[\"alpha\"])\n  -sum(log(dwrpcauchy(theta, 0, kappa)))\n}\n\nand run the optim() function. Remember: the optim() function takes as the first argument a vector of initial guesses for the parameter values (I arbitrarily entered 0 and 0), the function that needs to be minimized (in this case the negative log likelihood) and any data that actually enters the log-likelihood function.\n\n(funkymodel.fit <- optim(c(kappa0 = 0, alpha = 0), FunkyModel.Likelihood, \n                         S = S, theta = theta, hessian = TRUE))\n\n$par\n   kappa0     alpha \n0.1948638 0.9803800 \n\n$value\n[1] 1082.114\n\n$counts\nfunction gradient \n      51       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n          kappa0     alpha\nkappa0 1319.3033  816.3407\nalpha   816.3407 1171.1951\n\n\nThe original parameter values were 0.2 and 1 … so the maximum likelihood estimates are pretty good! Parameters and standard errors:\n\nfunkymodel.fit$par\n\n     rho0     alpha \n0.1948638 0.9803800 \n\nsqrt(diag(solve(funkymodel.fit$hessian)))\n\n      rho0      alpha \n0.03650750 0.03874715 \n\n\nPretty precise!\nHere’s a tidy function that performs the complete fit:\n\nestimateFunkyModel <- function(steplengths, turningangles, p0 = c(kappa0 = 0, alpha = 0)){\n  funkykappa <- function(x, kappa0, alpha) kappa0 + (1-kappa0)*(1-exp(-alpha*x))\n  FunkyModel.Likelihood <- function(p, S, theta){\n    kappa <- funkykappa(S, p[\"kappa0\"], p[\"alpha\"])\n    -sum(log(dwrpcauchy(theta, 0, kappa)))\n  }\n  fit <- optim(p0, FunkyModel.Likelihood, \n               S = steplengths, theta = turningangles, hessian = TRUE)\n  se <- sqrt(diag(solve(fit$hessian)))\n  data.frame(estimate = fit$par, CI.low = fit$par - 2*se, CI.high = fit$par + 2*se)\n}\n\nLet’s test it!\n\nestimateFunkyModel(steplengths = S, turningangles = theta)\n\n       estimate    CI.low   CI.high\nrho0  0.1948638 0.1218488 0.2678788\nalpha 0.9803800 0.9028857 1.0578743\n\n\nThose confidence intervals are really tight, and very significantly different from 0. This model clearly lets us know that that step lengths and turning angles are NOT independent.\n\n8.5.2 Testing on a truly random walk\nLet’s test this against a CRW where there is no relationship (\\(\\kappa_0 = 0.5, \\alpha = 0\\)):\n\nS <- rexp(1000)\ntheta <- rwrpcauchy(1000, 0, rho=funkykappa(S, .5, 0))\ntheta[theta > pi] <- theta[theta > pi] - 2*pi\nrose.diag(theta, bins=24)\nphi <- cumsum(theta); dZ <- complex(arg = phi, mod = S); Z <- cumsum(dZ)\nplot(Z, type=\"o\", asp=1, pch=16, cex=0.7, col=rgb(0,0,0,.2))\nplot(theta, S)\n\n\n\n\n\nestimateFunkyModel(steplengths = S, turningangles = theta)\n\n          estimate      CI.low    CI.high\nrho0   0.493446206  0.44719489 0.53969752\nalpha -0.009932846 -0.06916545 0.04929976\n\n\nThe \\(\\kappa\\) estimate is excellent (0.48 [95% C.I. - 0.43, 0.53]), and the \\(\\alpha\\) estimate is NOT significantly different from zero. So we confirm that the turning angles and step lengths are independent.\n\n8.5.3 Testing on some data!\nThere is an R package with a bunch of useful tools for analysis of animal movement and habitat use called adehabitatLT. This package contains some data on movement of an African buffalo (Syncerus caffer) in national park in Niger.\nTracks and distributions of step lengths and turning angles are illustrated below.\n\nrequire(amt)\ndata(amt_fisher)\nLeroy <- subset(amt_fisher, name == \"Leroy\")\nLeroy <- Leroy %>% mutate(Z = x_ + 1i * y_, \n                            dT = c(NA, difftime(t_[-1], t_[-length(t_)], units = \"mins\")), \n                            steps = c(NA, diff(Z)),\n                            steplengths = Mod(steps),\n                            turningangles = c(NA, diff(Arg(steps)))) %>% \n  na.omit %>% subset(dT < 60)\n\nExtracting the relevant variables from the movement data and plotting some relationships:\n\nplot(Leroy$Z, xlab=\"\", ylab=\"\", asp = 1, type = \"o\", cex = 0.3)\n\n# extract step lengths (in kilometer) and turning angles\nS <- Leroy$steplengths\ntheta <- Leroy$turningangles\n\nhist(S, col=\"grey\", bor=\"darkgrey\", breaks=200)\nrose.diag(na.omit(theta), bins=16)\nplot(theta, S, col=rgb(0,0,0,.2), pch=16)\n\n\n\n\nIt is hard to see visually if there is a relationship between turning angles and step lengths. But it is easy if we can fit our model with our likelihood!\n\nestimateFunkyModel(steplengths = Leroy$steplengths/1e2, \n                   turningangles = Leroy$turningangles, \n                   p0 = c(kappa0 = 0, alpha = 0))\n\n         estimate     CI.low    CI.high\nkappa0 -0.3393364 -0.4024082 -0.2762647\nalpha   0.1622015  0.1355044  0.1888986\n\n\nNote how strong the relationship is: \\(\\alpha = 0.16\\), 95% C.I. (0.135, 0.189). A very strong relationship. This is (for the record) a novel result."
  },
  {
    "objectID": "likelihoods.html#references",
    "href": "likelihoods.html#references",
    "title": "\n8  Likelihood Theory\n",
    "section": "\n8.6 References",
    "text": "8.6 References\n\n\n\n\nKareiva, P. M., and N. Shigesada. 1983. “Analyzing Insect Movement as a Correlated Random Walk.” Oecologia 56: 234–38. https://doi.org/10.1007/BF00379695.\n\n\nPatlak, C. S. 1953. “A Mathematical Contribution to the Study of Orientation of Organisms.” Bulletin of Mathematical Biophysics 15: 431–76. https://doi.org/10.1007/BF02476435."
  },
  {
    "objectID": "dependent_data.html",
    "href": "dependent_data.html",
    "title": "\n9  Dependent Data\n",
    "section": "",
    "text": "time series, autocorrelated data\n\nNB talks about time warps\n\n\nspatial correlation\ngls - nlme"
  },
  {
    "objectID": "discrete_movement_models.html#simple-random-walk",
    "href": "discrete_movement_models.html#simple-random-walk",
    "title": "\n10  Discrete time movement models\n",
    "section": "\n10.1 Simple random walk",
    "text": "10.1 Simple random walk"
  },
  {
    "objectID": "discrete_movement_models.html#correlated-random-walk",
    "href": "discrete_movement_models.html#correlated-random-walk",
    "title": "\n10  Discrete time movement models\n",
    "section": "\n10.2 Correlated random walk",
    "text": "10.2 Correlated random walk\nStep-length distributions\nTurning angle distributions"
  },
  {
    "objectID": "discrete_movement_models.html#biased-crw",
    "href": "discrete_movement_models.html#biased-crw",
    "title": "\n10  Discrete time movement models\n",
    "section": "\n10.3 Biased CRW",
    "text": "10.3 Biased CRW"
  },
  {
    "objectID": "home_ranges.html#mcp",
    "href": "home_ranges.html#mcp",
    "title": "\n12  Home ranges\n",
    "section": "\n12.1 MCP",
    "text": "12.1 MCP"
  },
  {
    "objectID": "home_ranges.html#kernel",
    "href": "home_ranges.html#kernel",
    "title": "\n12  Home ranges\n",
    "section": "\n12.2 kernel",
    "text": "12.2 kernel"
  },
  {
    "objectID": "home_ranges.html#locoh",
    "href": "home_ranges.html#locoh",
    "title": "\n12  Home ranges\n",
    "section": "\n12.3 LoCoH",
    "text": "12.3 LoCoH"
  },
  {
    "objectID": "home_ranges.html#akde",
    "href": "home_ranges.html#akde",
    "title": "\n12  Home ranges\n",
    "section": "\n12.4 akde",
    "text": "12.4 akde"
  },
  {
    "objectID": "behavioral_changes.html#change-points",
    "href": "behavioral_changes.html#change-points",
    "title": "\n13  Behavioral Changes\n",
    "section": "\n13.1 Change points",
    "text": "13.1 Change points"
  },
  {
    "objectID": "behavioral_changes.html#segmentation",
    "href": "behavioral_changes.html#segmentation",
    "title": "\n13  Behavioral Changes\n",
    "section": "\n13.2 Segmentation",
    "text": "13.2 Segmentation"
  },
  {
    "objectID": "behavioral_changes.html#state-space-models",
    "href": "behavioral_changes.html#state-space-models",
    "title": "\n13  Behavioral Changes\n",
    "section": "\n13.3 State-space models",
    "text": "13.3 State-space models"
  },
  {
    "objectID": "behavioral_changes.html#migration-dispersal",
    "href": "behavioral_changes.html#migration-dispersal",
    "title": "\n13  Behavioral Changes\n",
    "section": "\n13.4 Migration & dispersal",
    "text": "13.4 Migration & dispersal"
  },
  {
    "objectID": "hmms.html#what-are-ssms-and-hmms",
    "href": "hmms.html#what-are-ssms-and-hmms",
    "title": "\n14  Hidden Markov and State-Space Modeling\n",
    "section": "\n14.1 What are SSM’s and HMM’s?",
    "text": "14.1 What are SSM’s and HMM’s?"
  },
  {
    "objectID": "hmms.html#implementation-with-stan",
    "href": "hmms.html#implementation-with-stan",
    "title": "\n14  Hidden Markov and State-Space Modeling\n",
    "section": "\n14.2 Implementation with STAN",
    "text": "14.2 Implementation with STAN"
  },
  {
    "objectID": "time_series.html#time-series-inferno",
    "href": "time_series.html#time-series-inferno",
    "title": "9  Time Series",
    "section": "\n9.1 Time Series Inferno:",
    "text": "9.1 Time Series Inferno:\n\n\nLasciate ogni speranza (di indipendenza), voi ch’entrate!\nTranslation: “Abandon all hope (of independence), ye who enter here!”"
  },
  {
    "objectID": "time_series.html#one-slide-summary-of-linear-modeling",
    "href": "time_series.html#one-slide-summary-of-linear-modeling",
    "title": "9  Time Series",
    "section": "\n9.2 One slide summary of linear modeling",
    "text": "9.2 One slide summary of linear modeling\n\nSimple:\n\n\\[  Y_i  = {\\bf X} \\beta + \\epsilon_i  \\]\n$$ \\epsilon_i \\sim {\\cal N}(0, \\sigma^2) $$\nwhere \\({\\bf X}\\) is the linear predictor and \\(\\epsilon_i\\) are i.i.d.\nSolved with: \\(\\hat{\\beta} = ({\\bf X}'{\\bf X})^{-1}{\\bf X}'y\\).\n\n\nGeneralized:\n\\[ Y_i  \\sim \\text{Distribution}(\\mu)  \\]\n\\[ g(\\mu)   =  {\\bf X} \\beta \\]\n\n\nwhere Distribution is exponential family and \\(g(\\mu)\\) is the link function. Solved via iteratively reweighted least squares (IRLS).\n\n\nGeneralized Additive:\n\\[Y_i  \\sim \\text{Distribution}(\\mu) \\]\n\\[ g(\\mu_i)   =  {\\bf X}_i \\beta + \\sum_{j} f_j(x_{ji}) \\]\n\n\nwhere \\({\\bf X}_i\\) is the \\(i\\)th row of X and \\(f_j()\\) is some smooth function of the \\(j\\)th covariate. Solved via penalized iteratively reweighted least squares (P-IRLS).\n\nNote: In all of these cases \\(i\\) is unordered!\n\n\n\\(Y_1, Y_2, Y_3 ... Y_n\\) can be reshuffled: \\(Y_5, Y_{42}, Y_2, Y_n ... Y_3\\) with no impact on the results!\nIn other words, all of the residuals: \\(\\epsilon_i = Y_i - \\text{E}(Y_i)\\) and \\(\\epsilon_j = Y_j - \\text{E}(Y_j)\\) are independent: \\(\\text{cov}{\\epsilon_i, \\epsilon_j} = 0\\)"
  },
  {
    "objectID": "time_series.html#basic-definitions",
    "href": "time_series.html#basic-definitions",
    "title": "9  Time Series",
    "section": "\n9.3 Basic Definitions",
    "text": "9.3 Basic Definitions\n\n9.3.0.1 Stochastic process:\n\n\nAny process (data or realization of random model) that is structured by an index:\n      $$ X_{t_{n}} = f(X_{t_1}, X_{t_2}, X_{t_3} ... X_{t_{n-1}})$$ \n\n\nwhere \\(f(\\cdot)\\) is a random process. The index can be time, spatial, location on DNA chain, etc.\n\n9.3.0.2 Time-series:\n\nA stochastic process indexed by \n\n\\(i \\to t\\): \\(Y_1, Y_2, Y_3 ... Y_n\\) becomes \\(Y_{t_1}, Y_{t_2}, Y_{t_3} ... Y_{t_n}\\)\n\n\n9.3.0.3 Discrete-time time-series:\n\nData collected at regular (Annual, Monthly, Daily, Hourly, etc.)~intervals.\n\n\\(t\\) usually denoted (and ordered) \\(1, 2, 3, ... n\\).\n\n9.3.0.4 Continuous-time time-series:\n-Data collected at arbitrary intervals: \\(t_i \\in \\{T_{min}, T_{max}\\}\\)."
  },
  {
    "objectID": "time_series.html#example-of-time-series-and-questions",
    "href": "time_series.html#example-of-time-series-and-questions",
    "title": "9  Time Series",
    "section": "\n9.4 Example of time series (and questions)",
    "text": "9.4 Example of time series (and questions)\n\n9.4.0.1 Question: Has the level of Lake Huron changed over 100 years?\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Can we identify the trend / seasonal pattern / random fluctuations / make forecasts about CO2 emissions?\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: What are the dynamics and relationships within and between lynx and hare numbers?[^1] [^1]: MacLulich Fluctuations in the numbers of varying hare, 1937\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: How can we infer unobserved behavioral states from the movements of animals?\n\n\n\n\n\n\n\n\n\n\n\nNote: Multidimensional state \\(X\\), continuous time \\(T\\)\n\nQuestion: How can we quantify the time budgets and behaviors of a wolf?\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: Discrete states \\(X\\), continuous time \\(T\\)"
  },
  {
    "objectID": "time_series.html#some-objectives-of-time-series-analysis",
    "href": "time_series.html#some-objectives-of-time-series-analysis",
    "title": "9  Time Series",
    "section": "\n9.5 Some objectives of time-series analysis",
    "text": "9.5 Some objectives of time-series analysis\n\nCharacterizing a random process\n\nIdentifying trends, cycles, random structure\n\nIdentifying and estimating the stochastic model for the time series\n\n\nInference\n\nAccounting for lack of independence in linear modeling frameworks\nAvoiding FALSE INFERENCE!\n\n\n\nLearning something important the autocorrelation structure\n\nForecasting"
  },
  {
    "objectID": "time_series.html#concept-1-autocorrelation",
    "href": "time_series.html#concept-1-autocorrelation",
    "title": "9  Time Series",
    "section": "\n9.6 Concept 1: Autocorrelation",
    "text": "9.6 Concept 1: Autocorrelation\n\n\n\n\n\n\nAutocovariance function: \\(\\gamma(h) = \\text{cov}[X_t, X_{t-h}]\\)\n\nAutocorrelation function (acf): \\(\\rho(h) = \\text{cor}[X_t, X_{t+h}]\\)\n\n\n\n9.6.1 Estimates\n\nSample mean: \\[\\overline{X} = {1 \\over n}\\sum_{t=1}^n X_t\\]\n\nSample autocovariance: \\[\\widehat{\\gamma}(h) = {1 \\over n} \\sum_{t=1}^{n-|h|} (X_{t+|h|} - \\overline{X})(X_t - \\overline{X})\\]\n\nSample autocorrelation: \\[\\widehat{\\rho}(h) = {\\widehat{\\gamma}(h) \\over \\widehat{\\gamma}(0)}\\]\n\n\n9.6.2 Sample autocorrelation\n\nin R: acf(X)\n\nLake Huron\nGives an immediate sense of the significance / shape of autocorrelation\n\n\n\n\n\nWhite Noise\n\n\n\n\n\n\nNote, blue dashed line is \\(1.96 \\over \\sqrt{n}\\), because expected sample autocorrelation for white noise is \\(\\sim {\\cal N}(0,1/n)\\)\n\nLynx\nACF provides instant feel for periodic patterns.\n\n\n\n\n\nCO2\nNot useful for time-series with very strong trends!"
  },
  {
    "objectID": "time_series.html#the-autoregression-model",
    "href": "time_series.html#the-autoregression-model",
    "title": "9  Time Series",
    "section": "\n9.7 The autoregression model",
    "text": "9.7 The autoregression model\n\nFirst order autoregressive model: AR(1): \\[ X_t = \\phi_1 \\,X_{t-1} + Z_t\\] where \\(Z_t \\sim {\\cal N}(0, \\sigma^2)\\) is White Noise (time series jargon for i.i.d.).\nSecond order autoregressive: AR(2): \\[ X_t = \\phi_1 \\,X_{t-1} + \\phi_2 \\, X_{t-2} + Z_t\\]\n\\(p\\)-th order autoregressive model: AR(p): \\[ X_t = \\phi_1 \\,X_{t-1} + \\phi_2 \\, X_{t-2} + ... + \\phi_p Z_{t-p} + Z_t \\]\n\nNote: these models assume \\(\\text{E}[X] = 0\\). Relaxing this gives (for AR(1)):\n    $$ {X_t = \\phi_1 \\,(X_{t-1}-\\mu) + Z_t + \\mu} $$\n\nPause: to compute \\(\\text{E}[X]\\), \\(\\text{var}[X]\\) and \\(\\rho(h)\\) for an \\(X \\sim AR(1)\\) on the board.\n\n\n9.7.1 AR(1): Theoretical predictions\n\\[ \\rho(h) = \\phi_1^h \\]\n\ndata(LakeHuron)\nacf(LakeHuron)\ncurve(acf(LakeHuron)$acf[2]^x, add=TRUE, col=3, lwd=2)\n\n\n\n\nIf the sample acf looks exponential - probably an AR(1) model.\n\n9.7.2 Fitting Lake Huron AR(1)\nFit: \\({X_t = \\phi_1 (\\,X_{t-1} - \\mu) + Z_t + \\mu}\\)\\\n\nLakeHuron.lm <- lm(LakeHuron[-1] ~ LakeHuron[-length(LakeHuron)])\nplot(LakeHuron[-length(LakeHuron)],LakeHuron[-1])\nabline(LakeHuron.lm, col=3, lwd=2)\n\n\n\n\nResults:\n\n\\(\\bullet\\) \\(\\phi_1 = 0.8364\\); \\(\\widehat\\mu = 579\\) ft; \\(\\widehat\\sigma^2 = 0.52\\) ft\\(^2\\)\n\nNote: \\(R^2 = 0.70\\), i.e. about 70% percent of the variation observed in water levels is explained by previous years.\n\n9.7.3 Diagnostic plots\n\nplot(LakeHuron.lm, 1:2)\nacf(residuals(LakeHuron.lm))\n\n\n\n\nCheck regression assumptions: Homoscedastic, Normal, Independent - note use of acf function to test assumption of independence!\n\n9.7.4 But what about the trend?\nDecomposition with trend:\n\\[ {Y_t = m_t + X_t} \\]\nwhere:\n\n{\\(m_t\\) } is the slowly varying trend component\n{\\(X_t\\) } is a random component\n{\\(X_t\\)} can have serial autocorrelation\n\\(\\text{E}[X_t] = 0\\)\n\n\\(X_t\\) must be stationary.\n\n\nDefinition of Stationary\n\n\n\\(X_t\\) is a Stationary process if \\(\\text{E}[X_t]\\) is independent of \\(t\\)\n\n\n\\(X_t\\) is what is left over after the time-dependent part is removed\n\n\n\n9.7.5 Estimating a trend and correlation to Lake Huron\n\\[Y_{T} = \\beta_0 + \\beta_1 T + X_{T}\\]\n\\[ X_{T} = \\phi_1 X_{T-1} + Z_T \\]\n\nplot(LakeHuron, ylab=\"Level (ft above sea level)\", xlab=\"Year\", main=\"Lake Huron Water Level\", col=3, lwd=2)\ngrid()\nlines(LakeHuron, col=3, lwd=1.5)\nLH.trend <- lm(LakeHuron~time(LakeHuron))\nabline(LH.trend, lwd=2, col=\"darkred\")\n\nacf(residuals(LH.trend), main = \"ACF of trend\", lag.max=20)\nX <- residuals(LH.trend)\nX.lm <- lm(X[-1]~X[-length(X)])  \n\nsummary(X.lm)\n\n\nCall:\nlm(formula = X[-1] ~ X[-length(X)])\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.95881 -0.49932  0.00171  0.41780  1.89561 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    0.01529    0.07272    0.21    0.834    \nX[-length(X)]  0.79112    0.06590   12.00   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7161 on 95 degrees of freedom\nMultiple R-squared:  0.6027,    Adjusted R-squared:  0.5985 \nF-statistic: 144.1 on 1 and 95 DF,  p-value: < 2.2e-16\n\nacf(residuals(X.lm), main=\"ACF of residuals of X\", lag.max=20)\n\n\n\n\nResults:\n\n\n\\(\\widehat{\\beta_0} = 625\\) ft\n\n\\(\\widehat{\\beta_1} = -0.024\\) ft/year\n\\(\\widehat{\\phi_1} = 0.79\\)\n\n\\(\\widehat\\sigma^2 = 0.513\\) ft\\(^2\\)\n\n\n9.7.6 Estimating a trend and correlation to Lake Huron\n\n9.7.6.1 Version 1: Two-step\n\nLH.trend <- lm(LakeHuron ~ time(LakeHuron)) \nX <- residuals(LH.trend) \nX.lm <- lm(X[-1]~X[-length(X)])\nsummary(X.lm)$coef\n\n                Estimate Std. Error    t value     Pr(>|t|)\n(Intercept)   0.01528513 0.07272035  0.2101905 8.339691e-01\nX[-length(X)] 0.79111798 0.06590223 12.0044198 9.501693e-21\n\n\nNote the simple linear regression gives a HIGHLY significant result for time.\n\n9.7.6.2 Version 2: One step\nwith generalized least squares (gls):\n\nrequire(nlme)\nLH.gls <- gls(LakeHuron ~ time(LakeHuron), \n              correlation = corAR1(form=~1))\nsummary(LH.gls)$tTable\n\n                       Value   Std.Error   t-value      p-value\n(Intercept)     616.48869320 24.36263217 25.304683 2.944221e-44\ntime(LakeHuron)  -0.01943459  0.01266414 -1.534616 1.281674e-01\n\n\n\nCorrelation coefficient: \\(\\phi = 0.8247\\)\n\nRegression slope: \\(\\beta_1 = -0.0194\\) - (p-value: 0.13)\n\n\nWHAT HAPPENED!? The TIME effect is not significant in this model! Does this mean that there is no trend in Lake Huron levels?\n\n\n9.7.7 Generalized least squares\nWhat DOES ``generalized least squares’’ mean?\n\n9.7.7.1 Some theoretical background:\nLinear regession (“ordinary least squares”):\n\\[y = {\\bf X \\beta} + \\epsilon\\]\n\\(y\\) is \\(n \\times 1\\) response, \\({\\bf X}\\) is \\(n \\times p\\) model matrix; \\(\\beta\\) is \\(p \\times 1\\) vector of parameters, \\(\\epsilon\\) is \\(n\\times 1\\) vector of errors. Assuming \\(\\epsilon \\sim {\\cal N}(0, \\sigma^2 {\\bf I}_n)\\) (where \\({\\bf I}_n\\) is \\(n\\times n\\) identity matrix) gives {} estimator of \\(\\beta\\):\n\\[\\hat{\\beta} = ({\\bf X}'{\\bf X})^{-1}{\\bf X}'y \\]\n\\[V(\\hat{\\beta}) = \\sigma^2 ({\\bf X}'{\\bf X})^{-1}\\]\nSolving this is the equivalent of finding the \\(\\beta\\) that minimizes the Residual Sum of Squares:\n\\[ RSS(\\beta) = \\sum_{i=1}^n (y_i - {\\bf X}_i \\beta)^2\\]\nAssume more generally that \\(\\epsilon \\sim {\\cal N}(0, \\Sigma)\\) has nonzero off-diagonal entries corresponding to correlated errors. If \\(\\Sigma\\) is known, the log-likelihood for the model is maximized with:\n\\[\\hat{\\beta}_{gls} = ({\\bf X}\\Sigma^{-1}{\\bf X})^{-1} {\\bf X} \\Sigma^{-1}{\\bf y}\\]\nFor example, when \\(\\Sigma\\) is a diagonal matrix of unequal error variances (heteroskedasticity), then \\(\\hat{\\beta}\\) is the weighted-least-squares (WLS) estimator.\nIn a real application, of course, \\(\\Sigma\\) is not known, and must be estimated along with the regression coefficients \\(\\beta\\) … But there are way too many elements in \\(\\Sigma\\) - (this many: $ n (n+1) / 2 $).\n A large part of dealing with dependent data is identifying a tractable, relatively simple form for that residual variance-covariance matrix, and then solving for the coefficients. \nThis is: generalized least squares} (GLS)\n\n9.7.8 Visualizing variance-covariance matrices:\nEmpirical \\(\\Sigma\\) matrix and theoretical \\(\\Sigma\\) matrix for residuals\\\n\n# Empirical Sigma\nres <- lm(LakeHuron~time(LakeHuron))$res\nn <- length(res); V <- matrix(NA, nrow=n, ncol=n)\nfor(i in 1:n) V[n-i+1,(i:n - i)+1] <- res[i:n]\nind <- upper.tri(V); V[ind] <- t(V)[ind] \n\n# Fitted Sigma\nsigma.hat <-  LH.gls$sigma\nphi.hat <- 0.8\nrequire(lattice)\nV.hat <- outer(1:n, 1:n, function(x,y) phi.hat^abs(x-y))\n\n\nrequire(fields); image.plot(var(V)); image.plot(sigma.hat*V.hat)\n\n\n\n\n\n9.7.9 How to determine the order of an AR(p) model?\nThe base ar() function, which uses AIC.\nRaw lake Huron data:\n\nar(LakeHuron)\n\n\nCall:\nar(x = LakeHuron)\n\nCoefficients:\n      1        2  \n 1.0538  -0.2668  \n\nOrder selected 2  sigma^2 estimated as  0.5075\n\n\nResiduals of time regression:\n\nLH.lm <- lm(LakeHuron~time(LakeHuron))\nar(LH.lm$res)\n\n\nCall:\nar(x = LH.lm$res)\n\nCoefficients:\n      1        2  \n 0.9714  -0.2754  \n\nOrder selected 2  sigma^2 estimated as  0.501\n\n\nOr you can always regress by hand against BOTH orders:\n\nn <- length(LakeHuron)\nsummary(lm(LH.lm$res[3:n] ~ \n           LH.lm$res[2:(n-1)] + \n           LH.lm$res[1:(n-2)]))\n\n\nCall:\nlm(formula = LH.lm$res[3:n] ~ LH.lm$res[2:(n - 1)] + LH.lm$res[1:(n - \n    2)])\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.58428 -0.45246 -0.01622  0.40297  1.73202 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(>|t|)    \n(Intercept)          -0.007852   0.069121  -0.114  0.90980    \nLH.lm$res[2:(n - 1)]  1.002137   0.097215  10.308  < 2e-16 ***\nLH.lm$res[1:(n - 2)] -0.283798   0.099004  -2.867  0.00513 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6766 on 93 degrees of freedom\nMultiple R-squared:  0.6441,    Adjusted R-squared:  0.6365 \nF-statistic: 84.17 on 2 and 93 DF,  p-value: < 2.2e-16\n\n\nIn either case, strong evidence of a NEGATIVE second-order correlation. (note \\(\\phi_1 > 1\\)!)\n\n9.7.10 Fitting an AR(2) model with gls\n\\[Y_i = \\beta_0 + \\beta_1 T_i + \\epsilon_i\\]\n\\[\\epsilon_i = AR(2)\\]\n\nrequire(nlme)\nLH.gls2 <- gls(LakeHuron ~ time(LakeHuron), correlation = corARMA(p=2))\nsummary(LH.gls2)\n\nGeneralized least squares fit by REML\n  Model: LakeHuron ~ time(LakeHuron) \n  Data: NULL \n      AIC      BIC   logLik\n  221.028 233.8497 -105.514\n\nCorrelation Structure: ARMA(2,0)\n Formula: ~1 \n Parameter estimate(s):\n      Phi1       Phi2 \n 1.0203418 -0.2741249 \n\nCoefficients:\n                   Value Std.Error  t-value p-value\n(Intercept)     619.6442 17.491090 35.42628  0.0000\ntime(LakeHuron)  -0.0211  0.009092 -2.32216  0.0223\n\n Correlation: \n                (Intr)\ntime(LakeHuron) -1    \n\nStandardized residuals:\n        Min          Q1         Med          Q3         Max \n-2.16624672 -0.57959971  0.01070681  0.61705337  2.03975934 \n\nResidual standard error: 1.18641 \nDegrees of freedom: 98 total; 96 residual\n\n\nCorrelation coefficients: - \\(\\phi_1 = 1.02\\)\\ - \\(\\phi_2 = -0.27\\)\\\nRegression slope: \\ - \\(\\beta_1 = -0.021\\) - p-value: 0.02\\\nSo …. by taking the second order autocorrelation into account the temporal regression IS significant!?\n\n9.7.11 Comparing models …\n\nLH.gls0.1 <- gls(LakeHuron ~ 1, correlation = corAR1(form=~1), method=\"ML\")\nLH.gls0.2 <- gls(LakeHuron ~ 1, correlation = corARMA(p=2), method=\"ML\")\nLH.gls1.1 <- gls(LakeHuron ~ time(LakeHuron), correlation = corAR1(form=~1), method=\"ML\")\nLH.gls1.2 <- gls(LakeHuron ~ time(LakeHuron), correlation = corARMA(p=2), method=\"ML\")\n\nanova(LH.gls0.1, LH.gls0.2, LH.gls1.1, LH.gls1.2)\n\n          Model df      AIC      BIC    logLik   Test  L.Ratio p-value\nLH.gls0.1     1  3 219.1960 226.9509 -106.5980                        \nLH.gls0.2     2  4 215.2664 225.6063 -103.6332 1 vs 2 5.929504  0.0149\nLH.gls1.1     3  4 218.4502 228.7900 -105.2251                        \nLH.gls1.2     4  5 212.3965 225.3214 -101.1983 3 vs 4 8.053612  0.0045\n\n\nTHe lowest AIC - by a decent-ish margin - is the second order model with lag.\nNote the non-default method = \"ML\" means that we are Maximizing the Likelihood (as opposed to the default, faster REML - restricted likelihood - which fits parameters but is harder to compare models with)\n\nNewest conclusions:\n\nThe water level in Lake Huron IS dropping, and there is a high first order and significant negative second-order auto-correlation to the water level.\n\nEven for very simple time-series and questions about simple linear trends … it is easy to make false inference (both see patterns that are not there, or fail to detect patterns that are there) if you don’t take auto-correlations into account!"
  },
  {
    "objectID": "time_series.html#an-important-twist-moving-average-models",
    "href": "time_series.html#an-important-twist-moving-average-models",
    "title": "9  Time Series",
    "section": "\n9.8 An important twist: Moving average models",
    "text": "9.8 An important twist: Moving average models\n\n\nFirst-order moving average model: MA(1)\n\\[ Z_t = Z_t + \\psi Z_{t-1} \\]\n\n\ni.e. the “random shock” (aka “innovation”) depends on the previous “random shock”.\n\n\n\\(q\\)-th order moving average model: MA(\\(q\\))\n  $$\\epsilon_t = \\psi_1 \\,Z_{t-1} + \\psi_2 \\, Z_{t-2} + ... + \\psi_q Z_{t-q} + Z_t$$ \n\n\nAssessed via the {}\n\\[\\phi(k) =\\mathop{\\text{Corr}}(X_t-{\\cal P}(X_t\\mid X_{t+1},\\ldots,X_{t+k-1}) \\qquad X_{t+k}-{\\cal P}(X_{t+k}\\mid X_{t+1},\\ldots,X_{t+k-1}))\\]\nwhere: \\({\\cal P}(Y\\mid X)\\) is the best linear projection of \\(Y\\) on \\(X\\), i.e.~we’ve regressed away all of the lags and are left only with the cross-correlation of the residuals.\n\n9.8.0.1 PACF(h) of an AR(1) is 0 at \\(h>0\\)\n\n\nphi <- 0.8; nu <- rnorm(100)\nesp.AR <- c(rnorm(1),rep(NA,100))\nfor(i in 1:100) esp.AR[i+1] <- phi*esp.AR[i] + nu[i]\n\n\npacf(esp.AR, main=\"pacf\")\n\n\n\n\n\n9.8.0.2 ACF(h) of an MA(1) is 0 at \\(h>1\\)\n\n\npsi <- 0.8;  nu <- rnorm(101)\nesp.MA <- c(rnorm(1),rep(NA,100))\nfor(i in 2:101) esp.MA[i] <- nu[i] + psi*nu[i-1]\n\n\npacf(esp.MA, main=\"pacf\")\n\n\n\n\n\n9.8.1 ARMA(p,q) model\n\nY <- arima.sim(model = list(ar = c(0.9, -.5), ma = c(-2,-4)), 1000)\n\n\n\n\nA lot of the traditional wizardry of time-series analysis is looking at acf’s and pacf’s and deducing the ARMA model.\n\n9.8.2 Selecting ARMA(p,q)\nWe can just ask AIC to help us out.\n\nget.aic.table <- function(Y){\naic.table <- AIC(\n  arima(Y, c(1,0,0)), arima(Y, c(0,0,1)), arima(Y, c(1,0,1)), arima(Y, c(2,0,0)), arima(Y, c(0,0,2)), \n  arima(Y, c(1,0,2)),  arima(Y, c(2,0,1)), arima(Y, c(2,0,2)), arima(Y, c(3,0,2)), arima(Y, c(3,0,3)))\naic.table[order(aic.table$AIC),]\n}\n\n\nY <- arima.sim(model = \n  list(ar = c(0.9, -.5),\n       ma = c(-2,-4)), 1000)\nget.aic.table(Y)\n\n                     df      AIC\narima(Y, c(2, 0, 2))  6 5601.295\narima(Y, c(3, 0, 2))  7 5601.761\narima(Y, c(3, 0, 3))  8 5602.790\narima(Y, c(2, 0, 1))  5 5617.042\narima(Y, c(1, 0, 2))  5 5677.932\narima(Y, c(0, 0, 2))  4 5682.829\narima(Y, c(1, 0, 1))  4 5758.919\narima(Y, c(2, 0, 0))  4 5824.927\narima(Y, c(0, 0, 1))  3 5954.010\narima(Y, c(1, 0, 0))  3 6397.916\n\n\n\nget.aic.table(LakeHuron)\n\n                     df      AIC\narima(Y, c(1, 0, 1))  4 214.4905\narima(Y, c(2, 0, 0))  4 215.2664\narima(Y, c(1, 0, 2))  5 216.4645\narima(Y, c(2, 0, 1))  5 216.4764\narima(Y, c(2, 0, 2))  6 218.4106\narima(Y, c(1, 0, 0))  3 219.1959\narima(Y, c(3, 0, 2))  7 219.6967\narima(Y, c(3, 0, 3))  8 221.1937\narima(Y, c(0, 0, 2))  4 230.9306\narima(Y, c(0, 0, 1))  3 255.2950\n\n\n\nget.aic.table(LH.lm$res)\n\n                     df      AIC\narima(Y, c(2, 0, 0))  4 210.5032\narima(Y, c(1, 0, 1))  4 210.5176\narima(Y, c(1, 0, 2))  5 212.1225\narima(Y, c(2, 0, 1))  5 212.1784\narima(Y, c(2, 0, 2))  6 214.1224\narima(Y, c(3, 0, 2))  7 215.2280\narima(Y, c(1, 0, 0))  3 216.5835\narima(Y, c(3, 0, 3))  8 216.6927\narima(Y, c(0, 0, 2))  4 217.8177\narima(Y, c(0, 0, 1))  3 235.2032\n\n\nNote that any MA effects get (mostly) explained away by accounting for the time trend.\n\n9.8.3 Forecasting\n\nrequire(forecast)\n(LH.forecast <- forecast(Arima(LH.lm$res, order = c(2,0,0), include.mean=FALSE)))\n\n    Point Forecast      Lo 80    Hi 80      Lo 95    Hi 95\n 99    1.545023949  0.6695495 2.420498  0.2061013 2.883947\n100    0.929893256 -0.3113219 2.171108 -0.9683815 2.828168\n101    0.482673894 -0.9084677 1.873816 -1.6448936 2.610241\n102    0.213122974 -1.2274235 1.653669 -1.9900028 2.416249\n103    0.073021289 -1.3802861 1.526329 -2.1496206 2.295663\n104    0.011054189 -1.4446636 1.466772 -2.2152740 2.237382\n105   -0.010247304 -1.4662334 1.445739 -2.2369859 2.216491\n106   -0.013531751 -1.4695223 1.442459 -2.2402771 2.213214\n107   -0.010602506 -1.4666002 1.445395 -2.2373588 2.216154\n108   -0.006697958 -1.4627065 1.449311 -2.2334709 2.220075\n\nplot(LH.forecast)\n\n\n\n\nForecasting with trend: (with apologies for ugly code)\n\nplot(LakeHuron, xlim=c(1870,2012), ylim=c(570,585))\nT.new <- 1972:2000\nLH.forecast <- forecast(Arima(LH.lm$res, order = c(2,0,0), include.mean=FALSE), h=length(T.new))\nT <- as.numeric(time(LakeHuron))\n\nLH.lm <- lm(as.numeric(LakeHuron)~T)\nLH.predict <- predict(LH.lm, newdata = list(T = T.new))\n\nlines(T.new, LH.predict + LH.forecast$mean, col=2)\npolygon(c(T.new,  T.new[length(T.new):1]), c(LH.predict + LH.forecast$lower[,1], (LH.predict + LH.forecast$upper[,1])[length(T.new):1]), col=rgb(0,0,1,.2), bor=NA)\npolygon(c(T.new,  T.new[length(T.new):1]), c(LH.predict + LH.forecast$lower[,2], (LH.predict + LH.forecast$upper[,2])[length(T.new):1]), col=rgb(0,0,1,.2), bor=NA)\n\n\n\n\nOut of curiousity, I downloaded the Lake Huron water levels up to 2012 from :\n\nLH <- read.csv(\"data/miHuronAnnualData.csv\")\npar(bty=\"l\", cex.lab=1.25, las=1)\nplot(LakeHuron, xlim=c(1870,2017), ylim=c(572,585), ylab=\"Lake Huron water level (feet)\")\nT.new <- 1972:2017\nLH.forecast <- forecast(Arima(LH.lm$res, order = c(2,0,0), include.mean=FALSE), h=length(T.new))\nT <- as.numeric(time(LakeHuron))\n\nLH.lm <- lm(as.numeric(LakeHuron)~T)\nLH.predict <- predict(LH.lm, newdata = list(T = T.new))\n\nlines(T.new, LH.predict + LH.forecast$mean, col=\"darkblue\")\npolygon(c(T.new,  T.new[length(T.new):1]), c(LH.predict + LH.forecast$lower[,1], (LH.predict + LH.forecast$upper[,1])[length(T.new):1]), col=rgb(0,0,1,.2), bor=NA)\npolygon(c(T.new,  T.new[length(T.new):1]), c(LH.predict + LH.forecast$lower[,2], (LH.predict + LH.forecast$upper[,2])[length(T.new):1]), col=rgb(0,0,1,.2), bor=NA)\n\nlines(LH$year, LH$AnnAvg*3.28084, type=\"l\", col=2, lwd=2)\n\n\n\n\nConclusion? This is a time series with a weak trend and lots of variability - forecasting won’t give you much, except perhaps in the very short term.\nBut the question is important: http://research.noaa.gov/News/NewsArchive/LatestNews/TabId/684/ArtMID/1768/ArticleID/10466/NOAA-Seeks-Answers-to-Great-Lakes-Water-Level-Changes–.aspx"
  },
  {
    "objectID": "time_series.html#classical-decomposition-with-trend-and-seasonality",
    "href": "time_series.html#classical-decomposition-with-trend-and-seasonality",
    "title": "9  Time Series",
    "section": "\n9.9 Classical Decomposition, with trend and seasonality",
    "text": "9.9 Classical Decomposition, with trend and seasonality\n\\[\n{Y_t = m_t + s_t + X_t}\n\\]\nwhere:\n\n\n\\(m_t\\) is the component\n\n\\(s_t\\) is the component\n\n\\(s_{t+d} = s_t\\), where \\(d\\) is the\n\nUsually, we know the period \\(d\\) (and do not need to estimate it)\n\n\n\\(X_t\\) is a random component, that is (hopefully) stationary\n\n\n\n\n\n\n9.9.1 CO2 data from Mauna Loa\n\n\n\n\n\n\n9.9.2 Linear detrending\n\n\n\n\n\nNot very satisfactory\n\n9.9.3 “Classical” decomposition of CO2 data\n\\[Y_t = m_t + s_t + X_t}\\]\nKey bits of code (try this at home!):\n\n# time series objects have \"time\" attributes\n  t <- as.vector(time(co2))\n  co2.lm <- lm(co2 ~ poly(t, 3))\n  Y.trend <-  predict(co2.lm)\n# tapply to average away at \"cycle\" of periodic  data\n  co2.detrended <- co2 - Y.trend\n  st <- tapply(co2.detrended, cycle(co2), mean)\n  Y.seasonal <- st[cycle(co2)]\n# random component  \n  Y.random <- co2.detrended - Y.seasonal\n\n\n\n\n\n\n\n\n\\(m_t\\): fit a cubic polynomial\nobtain detrended series: \\(V_t = Y_t - m_t\\).\nSeasonal component: \\(s_{t+i} = {1\\over 12} \\sum\\limits_{d=1}^{n/d} V_{t+d+i}\\)\n\nRandom componenent: \\(X_t = AR(p)\\)\n\n\n9.9.4 One line in R: decompose()\n\n\nplot(decompose(co2))\n\n\n\n\nNote: The trend term here is non-parametric, but a moving window filter (size \\(d\\)):\n\\[ m_t = {1 \\over 2} \\sum_{i = t-d/2}^{t+d/2} Y_{i} \\]\nThis is a good smoothing technique, but not useful for prediction\n\n9.9.5 A caution about polynomial fits:\nThey look great in the region they are fitting …\n\n\n\n\n\n\n\n\nbut the better they fit, the less reliable they are far away.\n\n9.9.6 Incidentally:\nIn 2013, CO2 at Mauna Loa hit 400 ppm for the first time\n\n\n\n\n\n\n\nIn 2016, CO2went over 400 ppm permanently.\n\n\n\n\n\n\n\nHow well could we have predicted that from our time-series model?"
  },
  {
    "objectID": "time_series.html#population-cycles",
    "href": "time_series.html#population-cycles",
    "title": "9  Time Series",
    "section": "\n9.10 Population cycles",
    "text": "9.10 Population cycles\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.10.1 Cross-correlation functions\n\n9.10.1.0.1 Recall autocorrelation:\n\nAutocorrelation function: \\(\\rho(h) = {\\text{E}[(X_t - \\mu_x)(Y_{t+h} - \\mu_y)] \\over \\sigma_x^2}\\)\n\nSample autocorrelation: {\\(\\widehat{\\gamma}(h) = {1 \\over n s_x^2} \\sum_{t=1}^{n-|h|} (X_{t+|h|} - \\overline{X})(X_t - \\overline{X})\\)}\n\n9.10.1.0.2 Analogously:\n\nCross-correlation function:\n\n\\[ \\rho_{xy}(h) = {\\text{E}[(X_t - \\mu_x)(Y_{t+h} - \\mu_y)] \\over \\sigma_x \\sigma_y} \\]\n\nSample cross-correlation:\n\n\\[ \\widehat{\\gamma}(h) = {1 \\over n s_x s_y} \\sum_{t=1}^{n-|h|} (Y_{t+|h|} - \\overline{X})(X_t - \\overline{X})\\]\nCross-correlation allows identification of relationships BETWEEN time-series over different lags.\n\n9.10.2 Cross-correlation of lynx and hare\nIn R, simply: ccf(Hare, Lynx)\n\n\n\n\n\n\n\n\n\n\nWhich of these effects is more important: Less Lynx leads to more Hare? More Hare leads to more Lynx? More Lynx leads to less Hare? Less Hare leads to less Lynx? These are difficult, but not impossible, questions to parse!"
  },
  {
    "objectID": "time_series.html#some-intermediate-conclusions",
    "href": "time_series.html#some-intermediate-conclusions",
    "title": "9  Time Series",
    "section": "\n9.11 Some intermediate conclusions",
    "text": "9.11 Some intermediate conclusions\n\nIf you do not account for correlation in your data, it is easy to make false inferences…\nBUT, there are interesting things to be learned from studying the (terrifying) lack of independence.\nA general strategy is to systematically extract all the patterns or otherwise reduce the data until you have extracted the stationary piece.\nMany basic concepts from regression are useful.\nThere are many easy-to-use tools in R (and elsewhere) that, like Virgil, hold your hand as you make your journey through the inferno of dependent data.\nThere are many ways to slice a time-series! As with any analysis - you have to be persistent and dig around and be creative."
  },
  {
    "objectID": "time_series.html#in-final-conclusion",
    "href": "time_series.html#in-final-conclusion",
    "title": "9  Time Series",
    "section": "\n9.12 In final conclusion",
    "text": "9.12 In final conclusion"
  },
  {
    "objectID": "time_series.html#additional-resource",
    "href": "time_series.html#additional-resource",
    "title": "9  Time Series",
    "section": "\n9.13 Additional Resource",
    "text": "9.13 Additional Resource\nThe main R guru for time series is Rob Hyndman, author of the forecast package.\nSee the CRAN Task View: Time Series Analysis for many many packages related to time series analysis.\nAn important ones (that I use and that we’ll use in lab) is: zoo."
  }
]